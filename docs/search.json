[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "소개",
    "section": "",
    "text": "Quarto를 배우며 만든 통계 R실습 Blog입니다.\n나눔스퀘어, D2Coding 폰트를 사용했습니다.\nEmail: jsviolet@sasa.hs.kr"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JS_Land 수업자료실",
    "section": "",
    "text": "17. 정원 가꾸기\n\n\n\n\n\n\nGardening\n\n\nYouTube\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n16. 상관분석\n\n\n\n\n\n\nStatistics\n\n\n교재 9장\n\n\n\n\n\n\n\n\n\nApr 23, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n15. 로지스틱회귀분석\n\n\n\n\n\n\nStatistics\n\n\n교재 9장\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n14. 다중선형회귀분석\n\n\n\n\n\n\nStatistics\n\n\n교재 9장\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n13. 분산분석\n\n\n\n\n\n\nStatistics\n\n\n교재 9장\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n12. 카이제곱검정\n\n\n\n\n\n\nStatistics\n\n\n교재 9장\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n11. 하나의 모집단에 대한 가설검정(분산)\n\n\n\n\n\n\nStatistics\n\n\n교재 8장\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n10. 하나의 모집단에 대한 가설검정(평균)\n\n\n\n\n\n\nStatistics\n\n\n교재 8장\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n09. 모비율과 표본비율, 모비율의 추정\n\n\n\n\n\n\nStatistics\n\n\n교재 7장\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n08. 모집단과 표본, 모평균의 추정\n\n\n\n\n\n\nStatistics\n\n\n교재 7장\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n07. 감마분포\n\n\n\n\n\n\nStatistics\n\n\n교재 6장\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n06. 지수분포\n\n\n\n\n\n\nStatistics\n\n\n교재 6장\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n05. 포아송분포, 균등분포\n\n\n\n\n\n\nStatistics\n\n\n교재 5장\n\n\n교재 6장\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n04. 초기하분포, 기하분포\n\n\n\n\n\n\nStatistics\n\n\n교재 5장\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n03. 연속확률변수, 정규분포\n\n\n\n\n\n\nStatistics\n\n\n교재 4장\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n02. 이산확률변수, 이항분포\n\n\n\n\n\n\nStatistics\n\n\n교재 4장\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\n\n\n\n\n\n\n01. 기술통계(Descriptive Statistics)\n\n\n\n\n\n\nStatistics\n\n\n교재 3장\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nKim Jae Sook\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/15/index.html",
    "href": "posts/15/index.html",
    "title": "15. 로지스틱회귀분석",
    "section": "",
    "text": "로지스틱 회귀분석(logistic regression)은 독립변수의 선형 결합을 이용하여 사건의 발생 가능성을 예측하는데 사용되는 통계기법으로 설명변수(독립변수, X)와 범주형 목표변수(종속변수, Y) 간의 관계를 모형화하여 목표변수를 분석하거나 분류하는 통계적 방법론이다. 로지스틱 회귀분석을 활용한 분류(classification) 문제에서는 목표변수를 직접 예측(prediction)하는 것이 아닌 2개의 클래스(e.g., ‘성공’ or ‘실패’) 중 하나의 클래스로 예측할 때 사용된다(i.e., Binary Classification)."
  },
  {
    "objectID": "posts/15/index.html#학생들의-대학-합격-여부-예측",
    "href": "posts/15/index.html#학생들의-대학-합격-여부-예측",
    "title": "15. 로지스틱회귀분석",
    "section": "학생들의 대학 합격 여부 예측",
    "text": "학생들의 대학 합격 여부 예측\n\n학생들의 대학 합격 여부를 예측하는 로지스틱 회귀분석을 가정해보자. 여기서 종속 변수는 합격 여부(합격=1, 불합격=0)이고, 독립 변수로는 시험 점수(GRE), 학점(GPA), 그리고 추천서의 수(Recommendations)를 사용한다.\n(아래 코드에서 생성한 가상데이터가 통계적으로 유의미한 결과를 가져오지 않을 수 있음.)\n\n\n# 데이터 준비\ndata &lt;- data.frame(\n  admit = c(0, 1, 1, 0, 1, 1), # 합격 여부\n  gre = c(600, 700, 700, 500, 660, 680), # GRE 점수\n  gpa = c(3.3, 3.7, 3.9, 2.5, 3.3, 3.8), # GPA\n  recommendations = c(1, 2, 2, 1, 1, 2) # 추천서의 수\n)\n\n# 로지스틱 회귀 모델 적합\nmodel &lt;- glm(admit ~ gre + gpa + recommendations, family = binomial(link = \"logit\"), data = data)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# 결과 해석\nsummary(model)\n\n\nCall:\nglm(formula = admit ~ gre + gpa + recommendations, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     -2.086e+02  5.506e+05       0        1\ngre              7.937e-01  2.117e+03       0        1\ngpa             -9.818e+01  3.886e+05       0        1\nrecommendations  3.257e+01  1.612e+05       0        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7.6382e+00  on 5  degrees of freedom\nResidual deviance: 3.9359e-10  on 2  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 23"
  },
  {
    "objectID": "posts/15/index.html#은행에서-대출-승인-여부-예측",
    "href": "posts/15/index.html#은행에서-대출-승인-여부-예측",
    "title": "15. 로지스틱회귀분석",
    "section": "은행에서 대출 승인 여부 예측",
    "text": "은행에서 대출 승인 여부 예측\n\n은행에서 고객의 신용 데이터를 기반으로 개인 대출 승인 여부를 예측하는 경우를 들 수 있다. 이 예제에서는 고객의 신용 점수(credit score), 연 소득(annual income), 대출 요청 금액(loan amount)을 독립 변수로 사용하여 대출 승인 여부(loan approval)를 종속 변수로 예측하는 모델을 구축한다.\n(아래 코드에서 생성한 가상데이터가 통계적으로 유의미한 결과를 가져오지 않을 수 있음.)\n\n\n# 가상 데이터 생성\nset.seed(123) # 결과 재현성을 위한 시드 설정\nloan_approval &lt;- sample(0:1, 100, replace = TRUE)\ncredit_score &lt;- rnorm(100, mean = 650, sd = 100)\nannual_income &lt;- rnorm(100, mean = 50000, sd = 20000)\nloan_amount &lt;- rnorm(100, mean = 15000, sd = 5000)\ndata &lt;- data.frame(loan_approval, credit_score, annual_income, loan_amount)\n\n# 로지스틱 회귀 모델 적합\nmodel &lt;- glm(loan_approval ~ credit_score + annual_income + loan_amount, \n             family = binomial(link = \"logit\"), data = data)\n\n# 모델 요약 결과 출력\nsummary(model)\n\n\nCall:\nglm(formula = loan_approval ~ credit_score + annual_income + \n    loan_amount, family = binomial(link = \"logit\"), data = data)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    1.879e-02  1.657e+00   0.011    0.991\ncredit_score  -1.656e-04  2.119e-03  -0.078    0.938\nannual_income -1.180e-05  1.129e-05  -1.046    0.296\nloan_amount    2.527e-05  4.156e-05   0.608    0.543\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 136.66  on 99  degrees of freedom\nResidual deviance: 135.24  on 96  degrees of freedom\nAIC: 143.24\n\nNumber of Fisher Scoring iterations: 4\n\n# 새로운 데이터에 대한 예측\nnew_data &lt;- data.frame(credit_score = c(700), annual_income = c(60000), loan_amount = c(10000))\npredict(model, newdata = new_data, type = \"response\")\n\n        1 \n0.3652591"
  },
  {
    "objectID": "posts/13/index.html",
    "href": "posts/13/index.html",
    "title": "13. 분산분석",
    "section": "",
    "text": "분산분석(analysis of variance, ANOVA)은 분산을 분석하기 때문에 지어진 이름이지만 그 목적은 평균을 분석하는 것이다. 여러 집단의 평균이 같은지, 아니면 하나라도 평균이 다른 집단이 있는지 판단하려는 것이 목적이다."
  },
  {
    "objectID": "posts/13/index.html#일원분산분석",
    "href": "posts/13/index.html#일원분산분석",
    "title": "13. 분산분석",
    "section": "일원분산분석",
    "text": "일원분산분석\n\n자동차 충돌실험 결과 여러 등급의 자동차에서 인형의 파손 정도\n\n자동차 충돌실험의 결과 인형의 파손 정도가 다음과 같다.\n\n\n\n\n\n\n소형자동차\n준중형자동차\n중형자동차\n대형자동차\n\n\n\n\n\n681\n643\n469\n384\n\n\n\n728\n655\n727\n656\n\n\n\n917\n742\n525\n602\n\n\n\n898\n514\n454\n687\n\n\n\n620\n525\n459\n360\n\n\n평균\n768.8\n615.8\n526.8\n537.8\n\n\n표준편차\n132.4\n95.9\n115.5\n154.6\n\n\n\n\n이 데이터를 사용하여 인형의 파손 정도는 자동차의 크기에 관계없이 동일하다는 귀무가설을 검정하시오.\n\ny &lt;- c(681, 728, 917, 898, 620,\n       643, 655, 742, 514, 525,\n       469, 727, 525, 454, 459,\n       384, 656, 602, 687, 360) \n \nx &lt;- rep(c(\"소형\",\"준중형\",\"중형\",\"대형\"), c(5,5,5,5))\nx &lt;-factor(x)\n\naov.result=aov(y ~ x)\nsummary(aov.result)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nx            3 186825   62275   3.893  0.029 *\nResiduals   16 255923   15995                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# F통계량값인 3.8934가 기각역에 있으므로 귀무가설을 기각한다.\n# 또는 p값 0.029가 유의수준 0.05보다 작으므로 귀무가설을 기각한다.\n# 자동차의 크기에 따른 등급은 인형모형의 파손 정도에 통계적으로 유의한 차이를 보인다."
  },
  {
    "objectID": "posts/11/index.html",
    "href": "posts/11/index.html",
    "title": "11. 하나의 모집단에 대한 가설검정(분산)",
    "section": "",
    "text": "하나의 모집단에 대한 가설검정(분산) (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/11/index.html#모분산의-신뢰구간",
    "href": "posts/11/index.html#모분산의-신뢰구간",
    "title": "11. 하나의 모집단에 대한 가설검정(분산)",
    "section": "모분산의 신뢰구간",
    "text": "모분산의 신뢰구간\n\n\\(\\chi^2\\)분포가 모분산의 신뢰구간을 추정하는 데에 어떻게 사용되는지 알아보자. Y식품은 캔음료를 생산하는 공장을 운영하며, \\(350\\textrm{mL}\\) 캔음료를 가장 많이 생산하고 있다. 생산공정은 캔에 평균 \\(350\\textrm{mL}\\)의 음료가 담기도록 조절하였음에도 표기된 용량보다 적다는 소비자들의 불만이 접수되었다. 용량의 변동성을 알아보기 위해 25개의 캔이 단순확률추출되었고, 표본분산은 \\(4(\\textrm{mL}^2)\\)로 계산되었다. 그러나 25개로 이루어진 표본에서 얻은 분산이 전체 음료 용량의 모분산의 정확한 값이라고 기대할 수 없으므로, 모분산의 신뢰구간을 추정하게 된다. 모분산 \\(\\sigma^2\\)의 95% 신뢰구간을 구하시오.\n\n\nsample_variance &lt;- 4  # 표본 분산과 표본 크기\nn &lt;- 25\nalpha &lt;- 0.05  # 신뢰 수준\n\n# 카이제곱 분포의 하위 및 상위 분위수\nlower_chi_sq &lt;- qchisq(alpha / 2, n - 1)\nupper_chi_sq &lt;- qchisq(1 - alpha / 2, n - 1)\n\n# 모분산의 95% 신뢰구간 계산\nlower_variance &lt;- ((n - 1) * sample_variance) / upper_chi_sq\nupper_variance &lt;- ((n - 1) * sample_variance) / lower_chi_sq\n\n# 결과 출력\ncat(\"모분산의 95% 신뢰구간: [\", lower_variance, \",\", upper_variance, \"]\\n\")\n\n모분산의 95% 신뢰구간: [ 2.438772 , 7.741217 ]"
  },
  {
    "objectID": "posts/11/index.html#모분산에-대한-가설검정",
    "href": "posts/11/index.html#모분산에-대한-가설검정",
    "title": "11. 하나의 모집단에 대한 가설검정(분산)",
    "section": "모분산에 대한 가설검정",
    "text": "모분산에 대한 가설검정\n\n모분산에 대한 단측검정\n\nY식품은 모분산이 2.25 이하이면 현재의 생산과정을 유지하기로 하였다. 이에 귀무가설을 모분산이 2.25이고, 대립가설은 모분산이 2.25보다 크다고 세웠다. 가설검정을 하시오.\n\n\nsample_variance &lt;- 4  # 표본 분산과 표본 크기\nn &lt;- 25  # 귀무가설 하의 모분산\nsigma_squared_0 &lt;- 2.25\n\n# 카이제곱 검정 통계량 계산\nchi_sq_stat &lt;- (n - 1) * sample_variance / sigma_squared_0\n\n# p-값 계산 (단측 검정)\np_value &lt;- 1 - pchisq(chi_sq_stat, n - 1)\n\n# 결과 출력\ncat(\"검정 통계량:\", chi_sq_stat, \"\\n\")\n\n검정 통계량: 42.66667 \n\ncat(\"p-값:\", p_value, \"\\n\")\n\np-값: 0.01085479 \n\n# 결정 규칙: p-값이 유의 수준(alpha)보다 작으면 귀무가설 기각\nalpha &lt;- 0.05\nif (p_value &lt; alpha) {\n  cat(\"귀무가설 기각: 모분산은 2.25보다 크다.\\n\")\n} else {\n  cat(\"귀무가설 기각할 충분한 증거가 없음: 모분산이 2.25보다 크다고 할 수 없다.\\n\")\n}\n\n귀무가설 기각: 모분산은 2.25보다 크다.\n\n# Y식품은 캔에 음료를 주입하는 공정에 조정을 하여야 한다.\n\n\n\n모분산에 대한 양측검정\n\nH고등학교의 수학선생님들은 최근 새로운 입시에 알맞은 시험 방식을 도입하고자 한다. 적절한 변별력을 유지하기 위해 새로운 시험 방식도 종전까지 사용된 시험 방식과 같이 200이라는 시험 점수의 분산을 유지하고자 한다. H고등학교의 학생들의 시험 점수는 정규분포를 따른다고 가정하고, 새로운 시험 방식에서의 시험 점수의 분산을 평가하기 위한 가설은 \\[H_o : \\sigma^2 = 200~~vs~~H_1 : \\sigma^2 \\ne 200\\] 이 될 것이다. 만일 귀무가설을 기각하게 되면, 즉 새로운 시험 방식이 기존 시험의 변별력을 유지하지 못한다면, 기존의 방식과 비슷한 분산 수준으로 조정을 할 것이다. 새로운 시험의 변별력을 검정하기 위하여, 학생들 중 30명을 단순확률추출하여 새로운 방식의 시험을 치르게 하였으며, 10% 유의수준에서 검정을 한다. 30명이 응시한 시험 점수의 분산은 정규분포를 다르며 175이었다. 가설을 검정하시오.\n\n\n# 표본 분산과 표본 크기\nsample_variance &lt;- 175\nn &lt;- 30\n# 귀무가설 하의 모분산\nsigma_squared_0 &lt;- 200\n\n# 카이제곱 검정 통계량 계산\nchi_sq_stat &lt;- (n - 1) * sample_variance / sigma_squared_0\n\n# p-값 계산 (양측 검정)\np_value &lt;- 2 * min(pchisq(chi_sq_stat, n - 1), 1 - pchisq(chi_sq_stat, n - 1))\n\n# 결과 출력\ncat(\"검정 통계량:\", chi_sq_stat, \"\\n\")\n\n검정 통계량: 25.375 \n\ncat(\"p-값:\", p_value, \"\\n\")\n\np-값: 0.6826982 \n\n# 결정 규칙: p-값이 유의 수준(alpha)보다 작으면 귀무가설 기각\nalpha &lt;- 0.10\nif (p_value &lt; alpha) {\n  cat(\"귀무가설 기각: 모분산은 200이 아니다.\\n\")\n} else {\n  cat(\"귀무가설 기각할 충분한 증거가 없음: 모분산이 200이라고 할 수 없다.\\n\")\n}\n\n귀무가설 기각할 충분한 증거가 없음: 모분산이 200이라고 할 수 없다.\n\n\n\n단일 기계에서 생산된 플라스틱 판 두께의 분포가 정규분포를 따른다고 하자. 완성된 플라스틱 판들에 대한 두께의 분산이 2.25\\(\\textrm {mm}^2\\)이상이면, 이는 공정에 이상이 있는 것이라고 한다. 생산된 제품 중 10장의 확률표본을 추출하여 두께를 측정하였더니 다음과 같았다. \\[226, 226, 232, 227, 225, 228, 225, 228, 229, 230\\] 이 자료를 통해 모분산의 95% 신뢰구간을 구하고, \\(\\sigma^2 \\ge 2.25\\)인지 여부를 유의수준 5%로 검정하시오.\n\n\n# 샘플 데이터\nsample_data &lt;- c(226, 226, 232, 227, 225, 228, 225, 228, 229, 230)\n\n# 샘플 분산 계산\nsample_variance &lt;- var(sample_data)\nn &lt;- length(sample_data)\n\n# 모분산의 95% 신뢰구간 계산\nalpha &lt;- 0.05\nlower &lt;- qchisq(alpha / 2, n - 1)\nupper &lt;- qchisq(1 - alpha / 2, n - 1)\nconf_interval_lower &lt;- ((n - 1) * sample_variance) / upper\nconf_interval_upper &lt;- ((n - 1) * sample_variance) / lower\ncat(\"모분산의 95% 신뢰구간: [\", conf_interval_lower, \",\", conf_interval_upper, \"]\\n\")\n\n모분산의 95% 신뢰구간: [ 2.439182 , 17.18271 ]\n\n# 모분산이 2.25 이상인지 검정\nsigma_squared_0 &lt;- 2.25\nchi_sq_stat &lt;- (n - 1) * sample_variance / sigma_squared_0\np_value &lt;- pchisq(chi_sq_stat, n - 1, lower.tail = FALSE)\ncat(\"검정 통계량:\", chi_sq_stat, \"\\n\")\n\n검정 통계량: 20.62222 \n\ncat(\"p-값:\", p_value, \"\\n\")\n\np-값: 0.01443764 \n\n# 결정 규칙: p-값이 유의 수준보다 작으면 귀무가설 기각\nif (p_value &lt; alpha) {\n  cat(\"귀무가설 기각: 모분산은 2.25보다 크다.\\n\")\n} else {\n  cat(\"귀무가설 기각할 충분한 증거가 없음: 모분산이 2.25보다 크다고 할 수 없다.\\n\")\n}\n\n귀무가설 기각: 모분산은 2.25보다 크다."
  },
  {
    "objectID": "posts/09/index.html",
    "href": "posts/09/index.html",
    "title": "09. 모비율과 표본비율, 모비율의 추정",
    "section": "",
    "text": "모비율과 표본비율, 모비율의 추정 (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/09/index.html#모비율과-표본비율",
    "href": "posts/09/index.html#모비율과-표본비율",
    "title": "09. 모비율과 표본비율, 모비율의 추정",
    "section": "모비율과 표본비율",
    "text": "모비율과 표본비율\n평균이 일반적인 자료들의 대푯값이라면, 표본비율은 \\(0\\)과 \\(1\\)로 된 특수한 자료의 평균이다. 즉 비율은 평균의 개념과 같다. 따라서 모비율(한 모집단에서 특정한 속성을 가지는 비율 \\(p\\))의 구간추정은 모평균의 구간추정방법과 거의 동일하다. 모비율 \\(p\\)를 추정하기 위하여 \\(n\\)개의 표본을 추출하여 조사한 결과, ’성공’의 수가 \\(X\\)라면, 모비율 \\(p\\)의 점추정량인 표본비율(sample proportion) \\(\\bar{p}\\)은 다음과 같이 구해진다.\n\\[\\bar{p}=\\dfrac{X}{n},~X=0, 1, \\cdots, n\\]\n위의 식에서 확률변수 \\(X\\)는 이항분포를 따른다. 즉, \\(X \\sim B(n, ~p)\\)이다.\n그리고 표본의 수 \\(n\\)이 커지면(표본크기 25~30) 확률변수 \\(X\\)는 다음과 같은 평균과 분산을 갖는 정규분포를 근사적으로 따르게 된다.\n\\[X ~ \\dot{\\sim} ~ N(np, ~np(1-p))\\]\n따라서 표본비율 \\(\\bar{p}\\)은 근사적으로 다음과 같은 정규분포를 따른다.\n\\[\\bar{p} ~ \\dot\\sim ~ N \\left( p, ~\\frac{p(1-p)}{n} \\right)\\]\n다음은 이항분포를 이용할 때와 이항분포의 정규근사를 이용하여 가설검정을 할 때의 R명령어이다. 이항분포를 정규분포로 근사할때는 연속성 수정을 해서 구해야 더 정확하다. 모비율에 대한 가설검정 예제는 10. 하나의 모집단에 대한 가설검정(평균)에서 살펴보자.\n\nbinom.test()함수: 이항분포를 이용할때(이산형)\nprop.test()함수: 이항분포의 정규근사 이용 시(연속형)\n예를 들어, \\(X=20\\)이라는 이산형 값은 연속적인 수직선 상에서 \\((19.5,~ 20.5)\\)의 구간을 의미하는 것으로 수정\n연속성 수정: 이산형 확률변수의 값 \\(X\\)를 마치 연속형 확률변수의 값인 것처럼 수정함\npros.test()는 기본적으로 연속성 수정을 하여 가설검정을 수행함"
  },
  {
    "objectID": "posts/09/index.html#모평균-모비율의-구간추정",
    "href": "posts/09/index.html#모평균-모비율의-구간추정",
    "title": "09. 모비율과 표본비율, 모비율의 추정",
    "section": "모평균, 모비율의 구간추정",
    "text": "모평균, 모비율의 구간추정\n모평균과 모비율의 구간을 추정할때에는 모분산을 알고 있으면 Z값, 모분산을 몰라서 표본분산으로 대체하여 사용할 경우에는 t값을 사용한다. 만일 모집단이 정규분포를 하지 않는 경우는 표본수가 많다면 중심극한정리를 적용하여 근사적으로 Z값을 사용한다. 이때 모분산을 알고 있으면 그것을 사용하고, 모르면 표본분산을 사용한다. 정규분포를 하지 않고 표본수가 적은 경우는 그때 그때 분포 모양에 따라 계산된 값을 사용하거나 비모수통계 방법 등을 이용해야 한다."
  },
  {
    "objectID": "posts/09/index.html#모비율의-추정",
    "href": "posts/09/index.html#모비율의-추정",
    "title": "09. 모비율과 표본비율, 모비율의 추정",
    "section": "모비율의 추정",
    "text": "모비율의 추정\n\n모비율의 구간추정, 신뢰구간\n\n어떤 자동차 회사에서 미니밴의 생산을 계획하는데 소비자들의 반응을 알아보려고 표본조사를 하였다. 1,000명의 표본을 추출하여, 향후 5년 내에 미니밴의 구매의사를 조사한 결과 40명이 미니밴을 살 의사가 있는 것으로 판단되었다. 향후 5년간 미니밴을 구매하려는 고객의 비율 \\(p\\)에 대한 95% 신뢰구간을 구하시오.\n\n\nk &lt;- 40\nn &lt;- 1000\npbar &lt;-  k/n\nSE &lt;- sqrt(pbar*(1-pbar)/n)\nE &lt;- qnorm(.975)*SE \npbar + c(-E, E) \n\n[1] 0.02785455 0.05214545\n\n\n\n\n표본수의 결정\n\n앞의 예제에서 오차한계 \\(d\\)가 \\(0.2\\)라면 표본수 \\(n\\)은 얼마가 되어야 하는지 구하시오. (\\(\\alpha=0.05\\))\n\n\n# p=0.5로 생각하고 n을 구하면 다음과 같음\nzstar = qnorm(.975) \np = 0.5\nE = 0.01\nzstar^2*p*(1-p)/E^2\n\n[1] 9603.647\n\n# p=0.1로 생각하고 n을 구하면 다음과 같음\nzstar = qnorm(.975) \np = 0.1\nE = 0.01\nzstar^2*p*(1-p)/E^2\n\n[1] 3457.313\n\n\n\n앞면이 나올 확률 \\(p\\)가 알려지지 않은 동전에서 \\(p\\)를 추정하기 위하여 이 동전을 \\(n\\)번 던져서 앞면이 나올 비율 \\(\\bar{p}\\)를 계산하게 된다. 이때 \\(\\bar{p}\\)가 참값 \\(p\\)로부터 0.1 이내에 있을 확률이 99% 이상이 되도록 하기 위한 \\(n\\)의 값을 구하시오.\n\n\nzstar = qnorm(.995) \np = 0.5\nE = 0.1\nzstar^2*p*(1-p)/E^2\n\n[1] 165.8724"
  },
  {
    "objectID": "posts/07/index.html",
    "href": "posts/07/index.html",
    "title": "07. 감마분포",
    "section": "",
    "text": "감마분포 (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/07/index.html#감마분포",
    "href": "posts/07/index.html#감마분포",
    "title": "07. 감마분포",
    "section": "감마분포",
    "text": "감마분포\n\nR명령어 설명\n\n감마분포(Gamma Distribution)에 관련된 R 명령어 설명  \n\n위에 링크로 달아놓았지만, ChatGPT에게 감마분포에 관련된 r명령어를 묻고, 명령어는 알지만 더 자세한 사용법을 알고 싶을때는 프로그램에서 명령어에 커서를 두고 F1을 누르면 설명이 나옵니다.\n\ndgamma(1, shape = 2, rate = 1)  # 형상매개변수가 2이고 비율매개변수가 1일 때, 1에서의 밀도를 계산\n\n[1] 0.3678794\n\npgamma(1, shape = 2, rate = 1)  # 형상매개변수가 2이고 비율매개변수가 1일 때, 1 이하의 값이 나올 누적확률 계산\n\n[1] 0.2642411\n\nqgamma(0.5, shape = 2, rate = 1)  # 형상매개변수가 2이고 비율매개변수가 1일 때, 50%의 확률로 발생할 수 있는 값 계산\n\n[1] 1.678347\n\nrgamma(5, shape = 2, rate = 1)  # 형상매개변수가 2이고 비율매개변수가 1인 감마분포에서 5개의 무작위 값 생성\n\n[1] 2.4544590 0.1867657 1.4363106 2.7571871 1.0064694\n\n# plot함수를 이용해서 감마분포의 확률밀도함수 그리기\nshape_param &lt;- 2  # 감마 분포의 형상(shape)과 비율(rate) 매개변수 설정\nrate_param &lt;- 1\nx_values &lt;- seq(0, 10, by = 0.1)  # 확률 밀도 함수를 계산할 값의 범위\ndensities &lt;- dgamma(x_values, shape = shape_param, rate = rate_param)  # 확률밀도 계산\nplot(x_values, densities, type = \"l\", col = \"blue\",  # 확률 밀도 그래프 그리기\n     main = \"Gamma Distribution Density\", \n     xlab = \"x\", ylab = \"Density\")\n\n\n\n\n\n\n\n# 다양한 매개변수의 감마분포의 확률밀도함수 그리기\nparams &lt;- list(\n  list(shape = 2, rate = 1),\n  list(shape = 2, rate = 2),\n  list(shape = 3, rate = 2)\n)  # 서로 다른 매개변수 설정\ncolors &lt;- c(\"red\", \"blue\", \"brown\")  # 색상 설정\ncurve(dgamma(x, shape = params[[1]]$shape, rate = params[[1]]$rate), from = 0, to = 10, ylim = c(0, 1),\n      xlab = \"x\", ylab = \"Density\",\n      main = \"Gamma Distributions with Different Parameters\",\n      col = colors[1])  # 첫 번째 분포 그리기\nfor (i in 2:length(params)) {\n  curve(dgamma(x, shape = params[[i]]$shape, rate = params[[i]]$rate), from = 0, to = 10, add = TRUE, col = colors[i])\n}  # 나머지 분포 추가\nlegend(\"topright\", legend = paste(\"Shape =\", sapply(params, function(x) x$shape), \", Rate =\", sapply(params, function(x) x$rate)), col = colors, lty = 1)  # 범례 추가\n\n\n\n\n\n\n\n# 다양한 매개변수의 감마분포의 누적분포함수 그리기\nparams &lt;- list(\n  list(shape = 2, rate = 1),\n  list(shape = 2, rate = 2),\n  list(shape = 3, rate = 2)\n)  # 서로 다른 매개변수 설정\ncolors &lt;- c(\"red\", \"blue\", \"brown\")  # 색상 설정\ncurve(pgamma(x, shape = params[[1]]$shape, rate = params[[1]]$rate), from = 0, to = 10, ylim = c(0, 1),\n      xlab = \"x\", ylab = \"Cumulative Probability\",\n      main = \"Gamma Distributions CDF with Different Parameters\",\n      col = colors[1])  # 첫 번째 분포의 누적분포함수 그리기\nfor (i in 2:length(params)) {  # 나머지 분포의 누적분포함수 추가\n  curve(pgamma(x, shape = params[[i]]$shape, rate = params[[i]]$rate), from = 0, to = 10, add = TRUE, col = colors[i])\n}\n# 범례 추가\nlegend(\"bottomright\", legend = paste(\"Shape =\", sapply(params, function(x) x$shape), \", Rate =\", sapply(params, function(x) x$rate)), col = colors, lty = 1)\n\n\n\n\n\n\n\n\n\n\n감마분포 관련 문제\n\n소프트웨어 개발 프로젝트가 있다. 프로젝트는 여러 단계로 나뉘며, 각 단계의 완료 시간은 독립적으로 지수 분포를 따른다. 각 단계를 완료하는 데 걸리는 평균 시간은 5일이며, 프로젝트를 완료하는 데 필요한 전체 단계의 수는 4단계이다. 이 프로젝트가 15일 이내에 완료될 확률을 구하시오.\n\n\n# 매개변수 설정\nshape &lt;- 4            # 단계 수\nrate &lt;- 1 / 5         # 일 당 단계 완료 비율\n\n# 특정 시간(예: 15일) 이내에 프로젝트를 완료할 확률 계산\ntime &lt;- 15            # 완료 시간\nprobability &lt;- pgamma(time, shape, rate)\n\n# 확률 출력\nprobability\n\n[1] 0.3527681"
  },
  {
    "objectID": "posts/05/index.html",
    "href": "posts/05/index.html",
    "title": "05. 포아송분포, 균등분포",
    "section": "",
    "text": "포아송분포, 균등분포 (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/05/index.html#포아송분포",
    "href": "posts/05/index.html#포아송분포",
    "title": "05. 포아송분포, 균등분포",
    "section": "포아송분포",
    "text": "포아송분포\n\nR명령어 설명\n\n포아송분포(Poisson Distribution)에 관련된 R 명령어 설명\n위에 링크로 달아놓았지만, ChatGPT에게 포아송분포에 관련된 r명령어를 묻고, 명령어는 알지만 더 자세한 사용법을 알고 싶을때는 프로그램에서 명령어에 커서를 두고 F1을 누르면 설명이 나옵니다.\n\n\n# 시간당 평균 3건의 사건이 발생하는 시스템\n# 확률밀도함수: 특정 시간에 5건의 사건이 발생할 확률 계산\ndpois(5, 3)\n\n[1] 0.1008188\n\n# 누적분포함수: 특정 시간에 최대 2건의 사건이 발생할 확률 계산\nppois(2, 3)\n\n[1] 0.4231901\n\n# 분위수 함수: 90%의 확률로 발생할 수 있는 최대 사건 수\nqpois(0.90, 3)\n\n[1] 5\n\n# 랜덤 샘플링: 5개의 무작위 시간에 발생할 사건 수를 시뮬레이션\nrpois(5, 3)\n\n[1] 3 6 1 2 2\n\n# 포아송분포의 확률질량함수 그리기\nlambda &lt;- 3  # 매개변수(평균 발생 횟수) 설정\nx_values &lt;- 0:10  # 확률을 계산할 이벤트 수의 범위\nprobabilities &lt;- dpois(x_values, lambda)  # 확률 질량 함수 계산\nplot(x_values, probabilities, type = \"h\", lwd = 2, col = \"blue\",\n     main = \"Poisson Distribution\",\n     xlab = \"Number of Events\", ylab = \"Probability\")  # 확률 분포 그래프 그리기\n\n\n\n\n\n\n\n\n\n\n포아송분포 관련 문제\n\n전국 고속도로에 버려진 자동차는 매주 평균 2.2대의 포아송 분포를 따른다고 알려져 있다. 이때 다음 확률을 구하시오.\n\n\n다음 주에 버려진 차가 한 대도 없을 확률\n\n다음 주에 적어도 2대의 자동차가 버려져 있을 확률\n\n\n\nppois(0, 2.2) # (1)번 풀이\n\n[1] 0.1108032\n\n1-ppois(1, 2.2) # (2)번 풀이\n\n[1] 0.6454299\n\n\n\n삼삼 생명보험에 든 계약자 중에서 1년 동안 사망하는 사망자 수는 모수가 \\(\\lambda\\)인 포아송 분포를 따른다고 한다. \\(\\lambda =5\\)일 때 사망자 수가 적어도 3명 이상일 확률을 구하시오.\n\n\n1-ppois(2, 5)\n\n[1] 0.875348"
  },
  {
    "objectID": "posts/05/index.html#균등분포",
    "href": "posts/05/index.html#균등분포",
    "title": "05. 포아송분포, 균등분포",
    "section": "균등분포",
    "text": "균등분포\n\nR명령어 설명\n\n균등분포(Uniform Distribution)에 관련된 R 명령어 설명\n\n\ndunif(0.5, 0, 1)  # 0과 1 사이의 균등분포에서 0.5의 밀도 계산\n\n[1] 1\n\npunif(0.5, 0, 1)  # 0과 1 사이의 균등분포에서 0.5 이하의 값이 나올 누적 확률 계산\n\n[1] 0.5\n\nqunif(0.5, 0, 1)  # 0과 1 사이의 균등분포에서 중앙값(50% 분위수) 계산\n\n[1] 0.5\n\nrunif(5, 0, 1)  # 0과 1 사이에서 균등분포를 따르는 5개의 랜덤 숫자를 생성\n\n[1] 0.65771692 0.45292753 0.06555175 0.35949814 0.27536441\n\n# curve 함수를 이용한 확률밀도함수 그리기\nmin_val &lt;- 0  # 균등 분포의 최소값과 최대값 설정\nmax_val &lt;- 1\ncurve(dunif(x, min_val, max_val), from = min_val, to = max_val, # 균등분포의 확률밀도함수 그래프 그리기\n      xlab = \"x\", ylab = \"Density\", \n      main = \"Uniform Distribution Density\", col = \"blue\")\n\n\n\n\n\n\n\n# plot 함수를 이용한 확률밀도함수 그리기\nx_values &lt;- seq(min_val, max_val, by = 0.01)  # 균등분포의 확률밀도함수를 계산할 값의 범위\ndensities &lt;- dunif(x_values, min_val, max_val)  # 확률밀도 계산\nplot(x_values, densities, type = \"l\", col = \"red\",  # 확률밀도 그래프 그리기\n     main = \"Uniform Distribution Density\", \n     xlab = \"x\", ylab = \"Density\")\n\n\n\n\n\n\n\n# 균등분포의 누적분포함수 그리기\nmin_val &lt;- 0  # 균등 분포의 최소값과 최대값 설정\nmax_val &lt;- 1\ncurve(punif(x, min_val, max_val), from = min_val, to = max_val, # 균등분포의 누적분포함수 그래프 그리기\n      xlab = \"x\", ylab = \"Cumulative Probability\", \n      main = \"Uniform Distribution CDF\", col = \"purple\")\n\n\n\n\n\n\n\n\n\n\n균등분포 관련 문제\n\n신나은행 콜센터에 있는 직원은 10통의 전화에 응답하는데 걸리는 시간(단위: 분)이 \\((8, 15)\\)에서 균등분포를 한다. \\(X\\)를 1통의 전화에 응답하는데 소요되는 시간이라 할때, \\(X\\)의 평균을 구하시오.\n\n\nmin_val &lt;- 0.8  # 최솟값과 최댓값 설정\nmax_val &lt;- 1.5\nmean_val &lt;- (min_val + max_val) / 2  # 균등 분포의 평균 계산\nmean_val  # 평균 값 출력\n\n[1] 1.15"
  },
  {
    "objectID": "posts/03/index.html",
    "href": "posts/03/index.html",
    "title": "03. 연속확률변수, 정규분포",
    "section": "",
    "text": "대표적인 연속확률분포를 간단히 정리하면 다음과 같다.\n\n\n\n\n\n\n\n\n\n\n\n\n분포\n확률밀도함수\n평균(\\(\\mu\\))\n분산(\\(\\sigma^2\\))\n\n\n\n\n균등분포\\((a,~b)\\)\n\\(\\dfrac{1}{b-a},~a&lt;x&lt;b\\)\n\\(\\dfrac{a+b}{2}\\)\n\\(\\dfrac{(b-a)^2}{12}\\)\n\n\n정규분포\\((\\mu,~\\sigma^2\\))\n\\(\\dfrac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},~-\\infty&lt;x&lt;\\infty\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\n지수분포\\((\\lambda)\\)\n\\(\\lambda e^{-\\lambda x},~x&gt;0\\)\n\\(\\dfrac{1}{\\lambda}\\)\n\\(\\dfrac{1}{\\lambda^2}\\)\n\n\n감마분포\\((\\alpha,~\\lambda)\\)\n\\(\\dfrac{\\lambda^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\lambda x},~x&gt;0\\)\n\\(\\dfrac{\\alpha}{\\lambda}\\)\n\\(\\dfrac{\\alpha}{\\lambda^2}\\)\n\n\n\n\n이외에도 웨이블분포, 베타분포, 코쉬분포, 로그노말분포, 레일리분포 등이 있다.\n\n\n\n\n\n\n위험률함수 \\(\\lambda(t)=\\lambda\\), 비기억성을 가짐\n작은 \\(p\\)에서의 기하분포 \\(Geo(p)\\)의 근사분포, 포아송과정에서의 도착간격시간\n\n\n\n\n\n\\(X\\sim \\Gamma (n,~\\lambda)\\)일 때 \\(P(X \\le t) = 1- \\sum_\\limits{k=0}^{n-1} \\dfrac{(\\lambda t)^{k} e^{-\\lambda t}}{k!}\\)\n\\(\\chi ^2 (n) = \\Gamma \\left( \\dfrac{n}{2},~\\dfrac{1}{2}\\right)\\)\n포아송과정에서 \\(n\\)번째 사건이 일어날 때까지 기다린 시간\n모수 \\(r,~p\\)인 음이항분포에서 \\(p\\)가 작을 때의 근사분포\n\n\n\n\n\n\\(Z\\)가 표준정규분포이면 \\(E(Z^m )=0\\), \\(m\\)이 홀수\n\\(E(Z^{2m})=\\dfrac{(2m)!}{m!2^m},~m=0,1,2, \\cdots\\)\n\\(E(|Z|)=\\sqrt{\\dfrac{2}{\\pi}}\\)\n\\(Z^2 \\sim \\Gamma \\left( \\dfrac{1}{2},~\\dfrac{1}{2}\\right)\\)\n\\(Z_1 ,~Z_2\\)가 서로 독립이고 표준정규분포를 따르면 \\(Z_1 / Z_2 \\sim\\)표준코쉬분포\n\n\n\n\n\n\n정규분포의 공식 유도\n정규분포 확률밀도함수의 유도, 증명, 성질\n나무위키: 가우스 적분"
  },
  {
    "objectID": "posts/03/index.html#대표적인-연속확률분포",
    "href": "posts/03/index.html#대표적인-연속확률분포",
    "title": "03. 연속확률변수, 정규분포",
    "section": "",
    "text": "분포\n확률밀도함수\n평균(\\(\\mu\\))\n분산(\\(\\sigma^2\\))\n\n\n\n\n균등분포\\((a,~b)\\)\n\\(\\dfrac{1}{b-a},~a&lt;x&lt;b\\)\n\\(\\dfrac{a+b}{2}\\)\n\\(\\dfrac{(b-a)^2}{12}\\)\n\n\n정규분포\\((\\mu,~\\sigma^2\\))\n\\(\\dfrac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},~-\\infty&lt;x&lt;\\infty\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\n지수분포\\((\\lambda)\\)\n\\(\\lambda e^{-\\lambda x},~x&gt;0\\)\n\\(\\dfrac{1}{\\lambda}\\)\n\\(\\dfrac{1}{\\lambda^2}\\)\n\n\n감마분포\\((\\alpha,~\\lambda)\\)\n\\(\\dfrac{\\lambda^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\lambda x},~x&gt;0\\)\n\\(\\dfrac{\\alpha}{\\lambda}\\)\n\\(\\dfrac{\\alpha}{\\lambda^2}\\)\n\n\n\n\n이외에도 웨이블분포, 베타분포, 코쉬분포, 로그노말분포, 레일리분포 등이 있다."
  },
  {
    "objectID": "posts/03/index.html#각-분포들의-특징",
    "href": "posts/03/index.html#각-분포들의-특징",
    "title": "03. 연속확률변수, 정규분포",
    "section": "",
    "text": "위험률함수 \\(\\lambda(t)=\\lambda\\), 비기억성을 가짐\n작은 \\(p\\)에서의 기하분포 \\(Geo(p)\\)의 근사분포, 포아송과정에서의 도착간격시간\n\n\n\n\n\n\\(X\\sim \\Gamma (n,~\\lambda)\\)일 때 \\(P(X \\le t) = 1- \\sum_\\limits{k=0}^{n-1} \\dfrac{(\\lambda t)^{k} e^{-\\lambda t}}{k!}\\)\n\\(\\chi ^2 (n) = \\Gamma \\left( \\dfrac{n}{2},~\\dfrac{1}{2}\\right)\\)\n포아송과정에서 \\(n\\)번째 사건이 일어날 때까지 기다린 시간\n모수 \\(r,~p\\)인 음이항분포에서 \\(p\\)가 작을 때의 근사분포\n\n\n\n\n\n\\(Z\\)가 표준정규분포이면 \\(E(Z^m )=0\\), \\(m\\)이 홀수\n\\(E(Z^{2m})=\\dfrac{(2m)!}{m!2^m},~m=0,1,2, \\cdots\\)\n\\(E(|Z|)=\\sqrt{\\dfrac{2}{\\pi}}\\)\n\\(Z^2 \\sim \\Gamma \\left( \\dfrac{1}{2},~\\dfrac{1}{2}\\right)\\)\n\\(Z_1 ,~Z_2\\)가 서로 독립이고 표준정규분포를 따르면 \\(Z_1 / Z_2 \\sim\\)표준코쉬분포"
  },
  {
    "objectID": "posts/03/index.html#r명령어-설명",
    "href": "posts/03/index.html#r명령어-설명",
    "title": "03. 연속확률변수, 정규분포",
    "section": "R명령어 설명",
    "text": "R명령어 설명\n\n정규분포(Normal Distribution)에 관련된 R 명령어 설명  \n\n위에 링크로 달아놓았지만, ChatGPT에게 정규분포에 관련된 r명령어를 묻고, 명령어는 알지만 더 자세한 사용법을 알고 싶을때는 프로그램에서 명령어에 커서를 두고 F1을 누르면 설명이 나옵니다.\n\n# dnorm() : P(X = x) = P(x-e &lt; X &lt; x+e)\ndnorm(0, mean = 0, sd = 1)  # 평균이 0이고 표준편차가 1인 정규분포에서 0에서의 밀도 계산\n\n[1] 0.3989423\n\n# pnorm() : P(X &lt; x) = P(X &lt;= x)\npnorm(0, mean = 0, sd = 1)  # 평균이 0이고 표준편차가 1일 때, 0 이하의 값이 나올 누적확률 계산\n\n[1] 0.5\n\npnorm(0.5, mean = 0, sd = 1, lower.tail = TRUE)  # 0.5 이하의 면적을 계산하며, 이것이 기본값임\n\n[1] 0.6914625\n\npnorm(0.5, mean = 0, sd = 1, lower.tail = FALSE)  # 0.5 보다 큰 쪽의 면적을 계산함\n\n[1] 0.3085375\n\n# qnorm() : P(X &lt; x) = p ==&gt; x\nqnorm(0.5, mean = 0, sd = 1)  # 평균이 0이고 표준편차가 1일 때, 50%의 확률로 발생할 수 있는 값 계산\n\n[1] 0\n\nqnorm(0.95, mean = 0, sd = 1, lower.tail = TRUE) # 95%의 확률로 발생할 수 있는 값\n\n[1] 1.644854\n\nqnorm(0.05, mean = 0, sd = 1, lower.tail = TRUE) # 5%의 확률로 발생할 수 있는 값\n\n[1] -1.644854\n\n# rnorm() : random number from normal distribution\nrnorm(5, mean = 0, sd = 1)  # 평균이 0이고 표준편차가 1인 정규분포에서 5개의 무작위 값 생성\n\n[1]  1.0777783  0.3409585 -0.4100064  2.0192756  1.2234348\n\n\n연속확률분포에서 특정한 지점에서의 확률은 수학적으로 생각하면 선의 넓이이므로 0이다. 그런데 dnorm에서 구하는 값은 0이 나오지 않는데, 이는 아주 작은 크기의 직사각형을 만들어 면적을 계산하여 도출한다고 생각하면 된다. 직관적으로 그 점에서의 높이를 계산한다고 생각하면 된다.\n\n\n* 그림 출처: 통계교육원, R을 활용한 통계분석\n\n\n# 정규분포에서 1000개의 난수를 뽑아 표본을 만든 후, 히스토그램 그려보기\nsample = rnorm(1000, mean = 0, sd = 1)\nhist(sample)\n\n\n\n\n\n\n\n\n\n# curve함수를 이용해서 정규분포의 확률밀도함수 그리기\nmean &lt;- 0\nsd &lt;- 1\ncurve(dnorm(x, mean, sd), from = -4, to = 4, \n      xlab = \"x\", ylab = \"Density\",\n      main = \"Normal Distribution Density (mean=0, sd=1)\", col = \"blue\")\n\n\n\n\n\n\n\n# 정규분포의 누적분포함수 그리기\nmean &lt;- 0\nsd &lt;- 1\ncurve(pnorm(x, mean, sd), from = -4, to = 4, \n      xlab = \"x\", ylab = \"Cumulative Probability\",\n      main = \"Normal Distribution CDF (mean=0, sd=1)\", col = \"red\")"
  },
  {
    "objectID": "posts/03/index.html#정규분포의-확률-구하기",
    "href": "posts/03/index.html#정규분포의-확률-구하기",
    "title": "03. 연속확률변수, 정규분포",
    "section": "정규분포의 확률 구하기",
    "text": "정규분포의 확률 구하기\n\n\\(P(Z&lt;-0.155\\) 또는 \\(Z&gt;1.6)\\)를 구하시오.\n\n\n# pnorm함수는 정규분포의 확률값을 구해주는 함수이다. \n# pnorm(x, mean=a, sd=b)의 형태로 입력되며, \n# 뒤의 mean, sd 항목을 생략하면 mean=0, sd=1의 표준정규분포의 확률값을 구하는 것으로 인식하게 된다.\npnorm(-0.1555)+1-pnorm(1.6)\n\n[1] 0.4930129\n\n\n\n\\(P(-z≤Z≤z)=0.95\\)를 만족시키는 양의 실수 \\(z\\)를 구하시오.\n\n\n# qnorm함수는 확률값을 입력하였을때, 그에 해당하는 표준정규분포를 따르는 변수 Z값을 구해주는 함수이다.\nqnorm(0.025)\n\n[1] -1.959964\n\n\n\n어느 저울의 성질을 알아보기 위하여 질량이 1kg인 추를 반복적으로 잰다고 하자. 이때 측정된 질량은 평균이 1kg이고, 표준편차가 20g인 정규분포를 따른다고 한다. 이러한 측정을 계속 반복할 때, 측정된 값이 1kg으로부터 10g 이내에 있는 비율은 얼마인지 구하시오.\n\n\npnorm(1010,mean=1000,sd=20)-pnorm(990,mean=1000,sd=20)\n\n[1] 0.3829249"
  },
  {
    "objectID": "posts/03/index.html#주어진-자료의-정규성-알아보기",
    "href": "posts/03/index.html#주어진-자료의-정규성-알아보기",
    "title": "03. 연속확률변수, 정규분포",
    "section": "주어진 자료의 정규성 알아보기",
    "text": "주어진 자료의 정규성 알아보기\n\n모집단이 정규분포인지 알려져 있지 않은 경우\n\n정규분포를 따르는지 여부를 우선 파악해야 함.\n모평균을 추정하는 내용을 배울 때, 모집단이 정규분포임을 가정하고, 표본평균의 분포를 살펴보기 때문에 이때 필요함.\n\n\n\n정규분포를 따르는지 알아보는 방법\n\n히스토그램을 그려 좌우대칭인지 확인함.\n정규확률그림(normal probability plot)을 그려서 확인함. 정규확률그림이란 정규분포의 분위수와 표본자료의 분위수를 각각 가로축과 세로축으로 두고 그림을 그린 것임. 직선으로 증가하는 형태이면 정규분포를 따른다고 판단함. 히스토그램과 정규확률그림은 시각적으로 판단하여 사람에 따라 다르게 판단할 수 있음.\n통계적 가설검정을 통해 판단하는 방법이 있으며, 이 방법이 객관적이고 과학적인 방법임. 이 내용은 카이제곱검정에서 배울 것임.\n\n\n\n정규확률그림으로 확인하는 방법\n\n어떤 자료가 정규분포에 얼마나 근접한지 알고 싶으면 Q-Q(Quantile-Quantile) plot을 그려본다. Q-Q plot에서 직선은 정확한 정규분포 수식에서 나오는 값인데, 관찰값인 점들이 직선에서 크게 벗어나지 않으면 자료가 정규분포를 따른다고 할 수 있다.\nMASS 라이브러리에 있는 Cars93의 price라는 자료가 정규분포인지 확인해보자.\n\n\nlibrary(MASS)\nstr(Cars93$Price)  # 자료의 구조를 살펴보기\n\n num [1:93] 15.9 33.9 29.1 37.7 30 15.7 20.8 23.7 26.3 34.7 ...\n\nprice = Cars93$Price\nhist(price)  # 히스토그램을 그려서 좌우대칭인지 살펴보기\n\n\n\n\n\n\n\nqqnorm(price)\nqqline(price)  # qqnorm으로 그린 그림에 직선을 추가하기\n\n\n\n\n\n\n\n\n\nBodyWeight에 있는 자료의 정규확률그림을 그려보자.\n\n\nbody &lt;- read.table(\"BodyWeight.txt\", header=T)\nqqnorm(body$몸무게)\nqqline(body$몸무게, col=\"blue\")\n\n\n\n\n\n\n\n\n\n직선에 어느 정도 근접하느냐는 주관적이므로, 위 자료(100개)와 비교해보기 위해 정규분포를 이루는 난수를 발생하여 같은 그래프를 그려보자.\n\n\nx = rnorm(n=100, mean=0, sd=1) #평균이 0이고, 표준편차가 1인 표준정규분포에서 100개의 난수를 발생하여 x에 넣기\nqqnorm(x)\nqqline(x, col=\"blue\")"
  },
  {
    "objectID": "posts/01/index.html",
    "href": "posts/01/index.html",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "",
    "text": "자료의 정리 (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/01/index.html#데이터-불러오기",
    "href": "posts/01/index.html#데이터-불러오기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "데이터 불러오기",
    "text": "데이터 불러오기\n\n데이터가 들어있는 텍스트 화일을 불러오기\n\n\nclub &lt;- read.table(\"SocietyClubVillage.txt\", header=T)"
  },
  {
    "objectID": "posts/01/index.html#막대그림",
    "href": "posts/01/index.html#막대그림",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "막대그림",
    "text": "막대그림\n\n범주형 자료의 막대그래프 그리기\n\n\ntable(club)\n\n동호인마을\n미술 역술 영화 음악 \n   4    1    3    5 \n\nfreq = c(table(club), sum(table(club))) #합계를 더하기\nfreq\n\n미술 역술 영화 음악      \n   4    1    3    5   13 \n\naddmargins(table(club)) #오른쪽에 합계란 더하기\n\n동호인마을\n미술 역술 영화 음악  Sum \n   4    1    3    5   13 \n\nprop.table(table(club)) #상대도수로 나타내기(합계가 1이 됨)\n\n동호인마을\n      미술       역술       영화       음악 \n0.30769231 0.07692308 0.23076923 0.38461538 \n\nprop.table(table(club))*100 #백분율로 나타내기\n\n동호인마을\n     미술      역술      영화      음악 \n30.769231  7.692308 23.076923 38.461538 \n\nrel_freq = c(round(prop.table(table(club))*100, digit=1),sum(round(prop.table(table(club))*100, digit=1))) #소수첫째자리에서 반올림하기\n\nfreq_table_of_club = rbind(freq, rel_freq)\ncolnames(freq_table_of_club)[5] = '합계' #다섯번째 열에 이름주기\nfreq_table_of_club\n\n         미술 역술 영화 음악  합계\nfreq      4.0  1.0  3.0  5.0  13.0\nrel_freq 30.8  7.7 23.1 38.5 100.1\n\nbarplot(table(club),xlab=\"동호인 마을\", main=\"요약\")"
  },
  {
    "objectID": "posts/01/index.html#파이그림",
    "href": "posts/01/index.html#파이그림",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "파이그림",
    "text": "파이그림\n\n범주형 자료의 파이그래프 그리기\n\n\npie(table(club),xlab=\"동호인 마을\", main=\"요약\")"
  },
  {
    "objectID": "posts/01/index.html#기술통계량",
    "href": "posts/01/index.html#기술통계량",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "기술통계량",
    "text": "기술통계량\n\n데이터 불러온 후, 데이터 확인하기\n\n\nbody &lt;- read.table(\"BodyWeight.txt\", header=T)\nhead(body)\n\n  몸무게\n1   68.5\n2   77.3\n3   62.5\n4   73.0\n5   72.6\n6   61.0\n\n\n\n아래 명령어를 입력하면서 뜻 생각해보기, 명령어 안에서 줄바꿈은 shift+엔터를 쳐야 함\n\n\ndescription &lt;- function(x){\n    n=sum(is.na(x)) #결측치의 개수를 셈. FALSE==0, TRUE==1\n    min=min(x, na.rm = TRUE) # \"na.rm=TRUE\"는 결측치를 제거 후 구하라는 것임\n    x[is.na(x)] = mean(x, na.rm=T)  #결측값을 평균으로 대체하는 명령\n    mx=mean(x)\n    se=sd(x)/sqrt(length(x))\n    med=median(x)\n    mode=as.numeric(names(which.max(table(x))))\n    sx=sd(x)\n    vx=var(x) \n    kurt=mean((x-mean(x))^4)/(mean((x-mean(x))^2))^2-3 #첨도, install.packages('e1071'), library(e1071), skewness(변수명, na.rm=T)라고 해도 구할 수 있음.\n    skew=mean((x-mean(x))^3)/(mean((x-mean(x))^2))^1.5 #왜도, kurtosis(변수명, na.rm=T)\n    range=max(x)-min(x)  # range()라는 명령어를 쓰면 최소값과 최댓값을 동시 출력함\n    quantiles=quantile(x)\n    iqr=IQR(x)  # IQR = Q3 - Q1\n    cat(\"column1\",\"\\n\",\n    \"결측값의 수\", n, \"\\n\",\n    \"최솟값\", min,\"\\n\",\n    \"평균\", mx,\"\\n\",\n    \"표준오차\",se,\"\\n\",\n    \"중앙값\",med,\"\\n\", \n    \"최빈값\",mode,\"\\n\",\n    \"표준편차\",sx,\"\\n\",\n    \"분산\",vx,\"\\n\",\n    \"첨도\",kurt,\"\\n\",\n    \"왜도\",skew,\"\\n\", # 0이 나오면 평균을 중심으로 좌우대칭, 양수이면 오른쪽으로 긴꼬리임\n    \"범위\",range,\"\\n\", # 0이면 표준정규분포와 뾰족한 정도가 동일, 양수이면 표준정규분포보다 봉우리가 높고 뾰족함\n    \"사분위수\",quantiles,\"\\n\",\n    \"사분위수범위\",iqr,\"\\n\",\n    \"최솟값\",min(x),\"\\n\",\n    \"최댓값\",max(x),\"\\n\",\n    \"합\",sum(x),\"\\n\",\n    \"관측수\",length(x),\"\\n\")\n}\n\ndescription(body$몸무게)\n\ncolumn1 \n 결측값의 수 0 \n 최솟값 55.6 \n 평균 68.498 \n 표준오차 0.4974663 \n 중앙값 68.5 \n 최빈값 71.5 \n 표준편차 4.974663 \n 분산 24.74727 \n 첨도 0.1990144 \n 왜도 0.1898622 \n 범위 27.4 \n 사분위수 55.6 64.5 68.5 71.75 83 \n 사분위수범위 7.25 \n 최솟값 55.6 \n 최댓값 83 \n 합 6849.8 \n 관측수 100"
  },
  {
    "objectID": "posts/01/index.html#기술통계량-패키지-이용하기",
    "href": "posts/01/index.html#기술통계량-패키지-이용하기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "기술통계량 패키지 이용하기",
    "text": "기술통계량 패키지 이용하기\n\ndlookr 패키지가 없으면 주석에 있는 명령어로 설치하고, 패키지를 불러온 후 describe 명령어 입력하기\n\n\n# devtools::install_github(\"choonghyunryu/dlookr\")\nlibrary(dlookr)\n\nRegistered S3 methods overwritten by 'dlookr':\n  method          from  \n  plot.transform  scales\n  print.transform scales\n\n\n\nAttaching package: 'dlookr'\n\n\nThe following object is masked from 'package:base':\n\n    transform\n\ndescribe(body)\n\n# A tibble: 1 × 26\n  described_variables     n    na  mean    sd se_mean   IQR skewness kurtosis\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 몸무게                100     0  68.5  4.97   0.497  7.25    0.193    0.272\n# ℹ 17 more variables: p00 &lt;dbl&gt;, p01 &lt;dbl&gt;, p05 &lt;dbl&gt;, p10 &lt;dbl&gt;, p20 &lt;dbl&gt;,\n#   p25 &lt;dbl&gt;, p30 &lt;dbl&gt;, p40 &lt;dbl&gt;, p50 &lt;dbl&gt;, p60 &lt;dbl&gt;, p70 &lt;dbl&gt;,\n#   p75 &lt;dbl&gt;, p80 &lt;dbl&gt;, p90 &lt;dbl&gt;, p95 &lt;dbl&gt;, p99 &lt;dbl&gt;, p100 &lt;dbl&gt;"
  },
  {
    "objectID": "posts/01/index.html#도수분포표-히스토그램-그리기",
    "href": "posts/01/index.html#도수분포표-히스토그램-그리기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "도수분포표, 히스토그램 그리기",
    "text": "도수분포표, 히스토그램 그리기\n\n숫자형 자료의 도수분포표와 히스토그램 그리기\n\n\nclass &lt;- seq(55,85,5)\nfrequency &lt;- as.numeric(rep(\"\",length(class)))\nfreq.table &lt;- cbind(class,frequency)\nfor(i in 1:length(class)) freq.table[i,2] &lt;- length(body[body&gt;(class[i]-5)&body &lt;= class[i]])\n\nfreq.table #도수분포표 그리기\n\n     class frequency\n[1,]    55         0\n[2,]    60         3\n[3,]    65        24\n[4,]    70        35\n[5,]    75        31\n[6,]    80         4\n[7,]    85         3\n\nhist(body$몸무게) #히스토그램 그리기\n\n\n\n\n\n\n\nhist(body$몸무게, breaks = c(50,60,70,80,90)) #간격을 지정해서 히스토그램 그리기\n\n\n\n\n\n\n\nhist(body$몸무게, freq = FALSE, ylim = c(0, 0.08), ylab = \"relative freq\", xlab = \"Weight\", main = \"Histogram of Weight\", col = colors()[536]) #상대도수로 그리고, y축 범위 및 이름 설정하기\n\n\n\n\n\n\n\n\n\nR에서 사용할 수 있는 색\n\n\ncolors() #총 657개의 색 이름이 나옴.\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"        \n  [4] \"antiquewhite1\"        \"antiquewhite2\"        \"antiquewhite3\"       \n  [7] \"antiquewhite4\"        \"aquamarine\"           \"aquamarine1\"         \n [10] \"aquamarine2\"          \"aquamarine3\"          \"aquamarine4\"         \n [13] \"azure\"                \"azure1\"               \"azure2\"              \n [16] \"azure3\"               \"azure4\"               \"beige\"               \n [19] \"bisque\"               \"bisque1\"              \"bisque2\"             \n [22] \"bisque3\"              \"bisque4\"              \"black\"               \n [25] \"blanchedalmond\"       \"blue\"                 \"blue1\"               \n [28] \"blue2\"                \"blue3\"                \"blue4\"               \n [31] \"blueviolet\"           \"brown\"                \"brown1\"              \n [34] \"brown2\"               \"brown3\"               \"brown4\"              \n [37] \"burlywood\"            \"burlywood1\"           \"burlywood2\"          \n [40] \"burlywood3\"           \"burlywood4\"           \"cadetblue\"           \n [43] \"cadetblue1\"           \"cadetblue2\"           \"cadetblue3\"          \n [46] \"cadetblue4\"           \"chartreuse\"           \"chartreuse1\"         \n [49] \"chartreuse2\"          \"chartreuse3\"          \"chartreuse4\"         \n [52] \"chocolate\"            \"chocolate1\"           \"chocolate2\"          \n [55] \"chocolate3\"           \"chocolate4\"           \"coral\"               \n [58] \"coral1\"               \"coral2\"               \"coral3\"              \n [61] \"coral4\"               \"cornflowerblue\"       \"cornsilk\"            \n [64] \"cornsilk1\"            \"cornsilk2\"            \"cornsilk3\"           \n [67] \"cornsilk4\"            \"cyan\"                 \"cyan1\"               \n [70] \"cyan2\"                \"cyan3\"                \"cyan4\"               \n [73] \"darkblue\"             \"darkcyan\"             \"darkgoldenrod\"       \n [76] \"darkgoldenrod1\"       \"darkgoldenrod2\"       \"darkgoldenrod3\"      \n [79] \"darkgoldenrod4\"       \"darkgray\"             \"darkgreen\"           \n [82] \"darkgrey\"             \"darkkhaki\"            \"darkmagenta\"         \n [85] \"darkolivegreen\"       \"darkolivegreen1\"      \"darkolivegreen2\"     \n [88] \"darkolivegreen3\"      \"darkolivegreen4\"      \"darkorange\"          \n [91] \"darkorange1\"          \"darkorange2\"          \"darkorange3\"         \n [94] \"darkorange4\"          \"darkorchid\"           \"darkorchid1\"         \n [97] \"darkorchid2\"          \"darkorchid3\"          \"darkorchid4\"         \n[100] \"darkred\"              \"darksalmon\"           \"darkseagreen\"        \n[103] \"darkseagreen1\"        \"darkseagreen2\"        \"darkseagreen3\"       \n[106] \"darkseagreen4\"        \"darkslateblue\"        \"darkslategray\"       \n[109] \"darkslategray1\"       \"darkslategray2\"       \"darkslategray3\"      \n[112] \"darkslategray4\"       \"darkslategrey\"        \"darkturquoise\"       \n[115] \"darkviolet\"           \"deeppink\"             \"deeppink1\"           \n[118] \"deeppink2\"            \"deeppink3\"            \"deeppink4\"           \n[121] \"deepskyblue\"          \"deepskyblue1\"         \"deepskyblue2\"        \n[124] \"deepskyblue3\"         \"deepskyblue4\"         \"dimgray\"             \n[127] \"dimgrey\"              \"dodgerblue\"           \"dodgerblue1\"         \n[130] \"dodgerblue2\"          \"dodgerblue3\"          \"dodgerblue4\"         \n[133] \"firebrick\"            \"firebrick1\"           \"firebrick2\"          \n[136] \"firebrick3\"           \"firebrick4\"           \"floralwhite\"         \n[139] \"forestgreen\"          \"gainsboro\"            \"ghostwhite\"          \n[142] \"gold\"                 \"gold1\"                \"gold2\"               \n[145] \"gold3\"                \"gold4\"                \"goldenrod\"           \n[148] \"goldenrod1\"           \"goldenrod2\"           \"goldenrod3\"          \n[151] \"goldenrod4\"           \"gray\"                 \"gray0\"               \n[154] \"gray1\"                \"gray2\"                \"gray3\"               \n[157] \"gray4\"                \"gray5\"                \"gray6\"               \n[160] \"gray7\"                \"gray8\"                \"gray9\"               \n[163] \"gray10\"               \"gray11\"               \"gray12\"              \n[166] \"gray13\"               \"gray14\"               \"gray15\"              \n[169] \"gray16\"               \"gray17\"               \"gray18\"              \n[172] \"gray19\"               \"gray20\"               \"gray21\"              \n[175] \"gray22\"               \"gray23\"               \"gray24\"              \n[178] \"gray25\"               \"gray26\"               \"gray27\"              \n[181] \"gray28\"               \"gray29\"               \"gray30\"              \n[184] \"gray31\"               \"gray32\"               \"gray33\"              \n[187] \"gray34\"               \"gray35\"               \"gray36\"              \n[190] \"gray37\"               \"gray38\"               \"gray39\"              \n[193] \"gray40\"               \"gray41\"               \"gray42\"              \n[196] \"gray43\"               \"gray44\"               \"gray45\"              \n[199] \"gray46\"               \"gray47\"               \"gray48\"              \n[202] \"gray49\"               \"gray50\"               \"gray51\"              \n[205] \"gray52\"               \"gray53\"               \"gray54\"              \n[208] \"gray55\"               \"gray56\"               \"gray57\"              \n[211] \"gray58\"               \"gray59\"               \"gray60\"              \n[214] \"gray61\"               \"gray62\"               \"gray63\"              \n[217] \"gray64\"               \"gray65\"               \"gray66\"              \n[220] \"gray67\"               \"gray68\"               \"gray69\"              \n[223] \"gray70\"               \"gray71\"               \"gray72\"              \n[226] \"gray73\"               \"gray74\"               \"gray75\"              \n[229] \"gray76\"               \"gray77\"               \"gray78\"              \n[232] \"gray79\"               \"gray80\"               \"gray81\"              \n[235] \"gray82\"               \"gray83\"               \"gray84\"              \n[238] \"gray85\"               \"gray86\"               \"gray87\"              \n[241] \"gray88\"               \"gray89\"               \"gray90\"              \n[244] \"gray91\"               \"gray92\"               \"gray93\"              \n[247] \"gray94\"               \"gray95\"               \"gray96\"              \n[250] \"gray97\"               \"gray98\"               \"gray99\"              \n[253] \"gray100\"              \"green\"                \"green1\"              \n[256] \"green2\"               \"green3\"               \"green4\"              \n[259] \"greenyellow\"          \"grey\"                 \"grey0\"               \n[262] \"grey1\"                \"grey2\"                \"grey3\"               \n[265] \"grey4\"                \"grey5\"                \"grey6\"               \n[268] \"grey7\"                \"grey8\"                \"grey9\"               \n[271] \"grey10\"               \"grey11\"               \"grey12\"              \n[274] \"grey13\"               \"grey14\"               \"grey15\"              \n[277] \"grey16\"               \"grey17\"               \"grey18\"              \n[280] \"grey19\"               \"grey20\"               \"grey21\"              \n[283] \"grey22\"               \"grey23\"               \"grey24\"              \n[286] \"grey25\"               \"grey26\"               \"grey27\"              \n[289] \"grey28\"               \"grey29\"               \"grey30\"              \n[292] \"grey31\"               \"grey32\"               \"grey33\"              \n[295] \"grey34\"               \"grey35\"               \"grey36\"              \n[298] \"grey37\"               \"grey38\"               \"grey39\"              \n[301] \"grey40\"               \"grey41\"               \"grey42\"              \n[304] \"grey43\"               \"grey44\"               \"grey45\"              \n[307] \"grey46\"               \"grey47\"               \"grey48\"              \n[310] \"grey49\"               \"grey50\"               \"grey51\"              \n[313] \"grey52\"               \"grey53\"               \"grey54\"              \n[316] \"grey55\"               \"grey56\"               \"grey57\"              \n[319] \"grey58\"               \"grey59\"               \"grey60\"              \n[322] \"grey61\"               \"grey62\"               \"grey63\"              \n[325] \"grey64\"               \"grey65\"               \"grey66\"              \n[328] \"grey67\"               \"grey68\"               \"grey69\"              \n[331] \"grey70\"               \"grey71\"               \"grey72\"              \n[334] \"grey73\"               \"grey74\"               \"grey75\"              \n[337] \"grey76\"               \"grey77\"               \"grey78\"              \n[340] \"grey79\"               \"grey80\"               \"grey81\"              \n[343] \"grey82\"               \"grey83\"               \"grey84\"              \n[346] \"grey85\"               \"grey86\"               \"grey87\"              \n[349] \"grey88\"               \"grey89\"               \"grey90\"              \n[352] \"grey91\"               \"grey92\"               \"grey93\"              \n[355] \"grey94\"               \"grey95\"               \"grey96\"              \n[358] \"grey97\"               \"grey98\"               \"grey99\"              \n[361] \"grey100\"              \"honeydew\"             \"honeydew1\"           \n[364] \"honeydew2\"            \"honeydew3\"            \"honeydew4\"           \n[367] \"hotpink\"              \"hotpink1\"             \"hotpink2\"            \n[370] \"hotpink3\"             \"hotpink4\"             \"indianred\"           \n[373] \"indianred1\"           \"indianred2\"           \"indianred3\"          \n[376] \"indianred4\"           \"ivory\"                \"ivory1\"              \n[379] \"ivory2\"               \"ivory3\"               \"ivory4\"              \n[382] \"khaki\"                \"khaki1\"               \"khaki2\"              \n[385] \"khaki3\"               \"khaki4\"               \"lavender\"            \n[388] \"lavenderblush\"        \"lavenderblush1\"       \"lavenderblush2\"      \n[391] \"lavenderblush3\"       \"lavenderblush4\"       \"lawngreen\"           \n[394] \"lemonchiffon\"         \"lemonchiffon1\"        \"lemonchiffon2\"       \n[397] \"lemonchiffon3\"        \"lemonchiffon4\"        \"lightblue\"           \n[400] \"lightblue1\"           \"lightblue2\"           \"lightblue3\"          \n[403] \"lightblue4\"           \"lightcoral\"           \"lightcyan\"           \n[406] \"lightcyan1\"           \"lightcyan2\"           \"lightcyan3\"          \n[409] \"lightcyan4\"           \"lightgoldenrod\"       \"lightgoldenrod1\"     \n[412] \"lightgoldenrod2\"      \"lightgoldenrod3\"      \"lightgoldenrod4\"     \n[415] \"lightgoldenrodyellow\" \"lightgray\"            \"lightgreen\"          \n[418] \"lightgrey\"            \"lightpink\"            \"lightpink1\"          \n[421] \"lightpink2\"           \"lightpink3\"           \"lightpink4\"          \n[424] \"lightsalmon\"          \"lightsalmon1\"         \"lightsalmon2\"        \n[427] \"lightsalmon3\"         \"lightsalmon4\"         \"lightseagreen\"       \n[430] \"lightskyblue\"         \"lightskyblue1\"        \"lightskyblue2\"       \n[433] \"lightskyblue3\"        \"lightskyblue4\"        \"lightslateblue\"      \n[436] \"lightslategray\"       \"lightslategrey\"       \"lightsteelblue\"      \n[439] \"lightsteelblue1\"      \"lightsteelblue2\"      \"lightsteelblue3\"     \n[442] \"lightsteelblue4\"      \"lightyellow\"          \"lightyellow1\"        \n[445] \"lightyellow2\"         \"lightyellow3\"         \"lightyellow4\"        \n[448] \"limegreen\"            \"linen\"                \"magenta\"             \n[451] \"magenta1\"             \"magenta2\"             \"magenta3\"            \n[454] \"magenta4\"             \"maroon\"               \"maroon1\"             \n[457] \"maroon2\"              \"maroon3\"              \"maroon4\"             \n[460] \"mediumaquamarine\"     \"mediumblue\"           \"mediumorchid\"        \n[463] \"mediumorchid1\"        \"mediumorchid2\"        \"mediumorchid3\"       \n[466] \"mediumorchid4\"        \"mediumpurple\"         \"mediumpurple1\"       \n[469] \"mediumpurple2\"        \"mediumpurple3\"        \"mediumpurple4\"       \n[472] \"mediumseagreen\"       \"mediumslateblue\"      \"mediumspringgreen\"   \n[475] \"mediumturquoise\"      \"mediumvioletred\"      \"midnightblue\"        \n[478] \"mintcream\"            \"mistyrose\"            \"mistyrose1\"          \n[481] \"mistyrose2\"           \"mistyrose3\"           \"mistyrose4\"          \n[484] \"moccasin\"             \"navajowhite\"          \"navajowhite1\"        \n[487] \"navajowhite2\"         \"navajowhite3\"         \"navajowhite4\"        \n[490] \"navy\"                 \"navyblue\"             \"oldlace\"             \n[493] \"olivedrab\"            \"olivedrab1\"           \"olivedrab2\"          \n[496] \"olivedrab3\"           \"olivedrab4\"           \"orange\"              \n[499] \"orange1\"              \"orange2\"              \"orange3\"             \n[502] \"orange4\"              \"orangered\"            \"orangered1\"          \n[505] \"orangered2\"           \"orangered3\"           \"orangered4\"          \n[508] \"orchid\"               \"orchid1\"              \"orchid2\"             \n[511] \"orchid3\"              \"orchid4\"              \"palegoldenrod\"       \n[514] \"palegreen\"            \"palegreen1\"           \"palegreen2\"          \n[517] \"palegreen3\"           \"palegreen4\"           \"paleturquoise\"       \n[520] \"paleturquoise1\"       \"paleturquoise2\"       \"paleturquoise3\"      \n[523] \"paleturquoise4\"       \"palevioletred\"        \"palevioletred1\"      \n[526] \"palevioletred2\"       \"palevioletred3\"       \"palevioletred4\"      \n[529] \"papayawhip\"           \"peachpuff\"            \"peachpuff1\"          \n[532] \"peachpuff2\"           \"peachpuff3\"           \"peachpuff4\"          \n[535] \"peru\"                 \"pink\"                 \"pink1\"               \n[538] \"pink2\"                \"pink3\"                \"pink4\"               \n[541] \"plum\"                 \"plum1\"                \"plum2\"               \n[544] \"plum3\"                \"plum4\"                \"powderblue\"          \n[547] \"purple\"               \"purple1\"              \"purple2\"             \n[550] \"purple3\"              \"purple4\"              \"red\"                 \n[553] \"red1\"                 \"red2\"                 \"red3\"                \n[556] \"red4\"                 \"rosybrown\"            \"rosybrown1\"          \n[559] \"rosybrown2\"           \"rosybrown3\"           \"rosybrown4\"          \n[562] \"royalblue\"            \"royalblue1\"           \"royalblue2\"          \n[565] \"royalblue3\"           \"royalblue4\"           \"saddlebrown\"         \n[568] \"salmon\"               \"salmon1\"              \"salmon2\"             \n[571] \"salmon3\"              \"salmon4\"              \"sandybrown\"          \n[574] \"seagreen\"             \"seagreen1\"            \"seagreen2\"           \n[577] \"seagreen3\"            \"seagreen4\"            \"seashell\"            \n[580] \"seashell1\"            \"seashell2\"            \"seashell3\"           \n[583] \"seashell4\"            \"sienna\"               \"sienna1\"             \n[586] \"sienna2\"              \"sienna3\"              \"sienna4\"             \n[589] \"skyblue\"              \"skyblue1\"             \"skyblue2\"            \n[592] \"skyblue3\"             \"skyblue4\"             \"slateblue\"           \n[595] \"slateblue1\"           \"slateblue2\"           \"slateblue3\"          \n[598] \"slateblue4\"           \"slategray\"            \"slategray1\"          \n[601] \"slategray2\"           \"slategray3\"           \"slategray4\"          \n[604] \"slategrey\"            \"snow\"                 \"snow1\"               \n[607] \"snow2\"                \"snow3\"                \"snow4\"               \n[610] \"springgreen\"          \"springgreen1\"         \"springgreen2\"        \n[613] \"springgreen3\"         \"springgreen4\"         \"steelblue\"           \n[616] \"steelblue1\"           \"steelblue2\"           \"steelblue3\"          \n[619] \"steelblue4\"           \"tan\"                  \"tan1\"                \n[622] \"tan2\"                 \"tan3\"                 \"tan4\"                \n[625] \"thistle\"              \"thistle1\"             \"thistle2\"            \n[628] \"thistle3\"             \"thistle4\"             \"tomato\"              \n[631] \"tomato1\"              \"tomato2\"              \"tomato3\"             \n[634] \"tomato4\"              \"turquoise\"            \"turquoise1\"          \n[637] \"turquoise2\"           \"turquoise3\"           \"turquoise4\"          \n[640] \"violet\"               \"violetred\"            \"violetred1\"          \n[643] \"violetred2\"           \"violetred3\"           \"violetred4\"          \n[646] \"wheat\"                \"wheat1\"               \"wheat2\"              \n[649] \"wheat3\"               \"wheat4\"               \"whitesmoke\"          \n[652] \"yellow\"               \"yellow1\"              \"yellow2\"             \n[655] \"yellow3\"              \"yellow4\"              \"yellowgreen\"         \n\n\n\n숫자형 자료의 도수분포표를 그리는 다른 방법\n\n\nmin(body$몸무게, na.rm = TRUE)\n\n[1] 55.6\n\nmax(body$몸무게, na.rm = TRUE)\n\n[1] 83\n\nWeight_cat = cut(body$몸무게, breaks = c(55, 60, 65, 70, 75, 80, 85)) #수치형 자료를 범주형 자료로 바꿈\nWeight_cat\n\n  [1] (65,70] (75,80] (60,65] (70,75] (70,75] (60,65] (65,70] (65,70] (70,75]\n [10] (70,75] (65,70] (60,65] (70,75] (70,75] (70,75] (60,65] (70,75] (70,75]\n [19] (70,75] (65,70] (75,80] (65,70] (65,70] (60,65] (80,85] (70,75] (65,70]\n [28] (65,70] (60,65] (70,75] (65,70] (70,75] (65,70] (60,65] (70,75] (60,65]\n [37] (70,75] (70,75] (70,75] (65,70] (75,80] (65,70] (65,70] (60,65] (70,75]\n [46] (65,70] (70,75] (60,65] (60,65] (55,60] (65,70] (65,70] (60,65] (65,70]\n [55] (60,65] (70,75] (70,75] (65,70] (70,75] (65,70] (65,70] (65,70] (65,70]\n [64] (60,65] (70,75] (65,70] (60,65] (60,65] (60,65] (70,75] (70,75] (65,70]\n [73] (70,75] (65,70] (65,70] (70,75] (75,80] (60,65] (60,65] (65,70] (60,65]\n [82] (65,70] (60,65] (65,70] (65,70] (65,70] (80,85] (65,70] (55,60] (70,75]\n [91] (70,75] (80,85] (70,75] (60,65] (60,65] (65,70] (60,65] (55,60] (70,75]\n[100] (65,70]\nLevels: (55,60] (60,65] (65,70] (70,75] (75,80] (80,85]\n\ntable(Weight_cat)\n\nWeight_cat\n(55,60] (60,65] (65,70] (70,75] (75,80] (80,85] \n      3      24      35      31       4       3"
  },
  {
    "objectID": "posts/01/index.html#줄기-잎-그림-그리기",
    "href": "posts/01/index.html#줄기-잎-그림-그리기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "줄기-잎 그림 그리기",
    "text": "줄기-잎 그림 그리기\n\n숫자형 자료의 줄기-잎 그림 그리기\n\n\nstem(body$몸무게)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  5 | 679\n  6 | 111133333344444444444\n  6 | 5555666666667777777888889999999999\n  7 | 00000111111222222222233333334444\n  7 | 5556667\n  8 | 113"
  },
  {
    "objectID": "posts/01/index.html#summary함수-이용하기",
    "href": "posts/01/index.html#summary함수-이용하기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "Summary()함수 이용하기",
    "text": "Summary()함수 이용하기\n\n데이터 불러온 후, 데이터 확인하기\n\n\n# 내장된 데이터를 불러오기\ntrees\n\n   Girth Height Volume\n1    8.3     70   10.3\n2    8.6     65   10.3\n3    8.8     63   10.2\n4   10.5     72   16.4\n5   10.7     81   18.8\n6   10.8     83   19.7\n7   11.0     66   15.6\n8   11.0     75   18.2\n9   11.1     80   22.6\n10  11.2     75   19.9\n11  11.3     79   24.2\n12  11.4     76   21.0\n13  11.4     76   21.4\n14  11.7     69   21.3\n15  12.0     75   19.1\n16  12.9     74   22.2\n17  12.9     85   33.8\n18  13.3     86   27.4\n19  13.7     71   25.7\n20  13.8     64   24.9\n21  14.0     78   34.5\n22  14.2     80   31.7\n23  14.5     74   36.3\n24  16.0     72   38.3\n25  16.3     77   42.6\n26  17.3     81   55.4\n27  17.5     82   55.7\n28  17.9     80   58.3\n29  18.0     80   51.5\n30  18.0     80   51.0\n31  20.6     87   77.0\n\n\n\nsummary(), quantile() 함수를 이용하여 기술통계량 살펴보기\n\n\nsummary(trees) #최솟값, 1사분위수, 중앙값, 평균, 3사분위수, 최댓값, 결측치 개수(결측치를 제거하고 계산함)\n\n     Girth           Height       Volume     \n Min.   : 8.30   Min.   :63   Min.   :10.20  \n 1st Qu.:11.05   1st Qu.:72   1st Qu.:19.40  \n Median :12.90   Median :76   Median :24.20  \n Mean   :13.25   Mean   :76   Mean   :30.17  \n 3rd Qu.:15.25   3rd Qu.:80   3rd Qu.:37.30  \n Max.   :20.60   Max.   :87   Max.   :77.00  \n\nquantile(trees$Volume, na.rm=T) #결측치가 없어야 사용할 수 있음. 최솟값, 사분위수, 중앙값, 최댓값 계산함\n\n  0%  25%  50%  75% 100% \n10.2 19.4 24.2 37.3 77.0 \n\n\n\nboxplot() 함수로 데이터 살펴보기\n\n\nboxplot(trees) #수염 위 아래의 점은 이상값(사분위범위를 1.5배 이상 벗어난 데이터)을 뜻함, 수평으로 그리려면 horizontal=T 를 넣기\n\n\n\n\n\n\n\n\n\npairs()을 사용하여 산포도를 통해 변수들의 연관성을 한눈에 살펴보기\n\n\npairs(trees)"
  },
  {
    "objectID": "posts/01/index.html#웹사이트-데이터-불러오기",
    "href": "posts/01/index.html#웹사이트-데이터-불러오기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "웹사이트 데이터 불러오기",
    "text": "웹사이트 데이터 불러오기\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks dlookr::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(httr)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nsigungu_url &lt;- \"https://ko.wikipedia.org/wiki/대한민국의_기초자치단체_목록\"\n\nsgg_html &lt;- read_html(sigungu_url)\n\nsgg_raw &lt;- sgg_html |&gt; \n  html_nodes(\"table\") |&gt;\n  html_table() %&gt;%\n  .[[1]]\n\nsgg_raw |&gt; \n  janitor::clean_names(ascii = FALSE)\n\n# A tibble: 226 × 5\n   이름   광역자치단체   형태  면적_km2 인구_명\n   &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  \n 1 가평군 경기도         군    843.71   62,814 \n 2 강남구 서울특별시     구    39.5     542,341\n 3 강동구 서울특별시     구    24.59    426,178\n 4 강릉시 강원특별자치도 시    1,040.34 212,858\n 5 강북구 서울특별시     구    23.6     318,268\n 6 강서구 부산광역시     구    181.48   125,309\n 7 강서구 서울특별시     구    41.44    596,602\n 8 강진군 전라남도       군    500.95   35,886 \n 9 강화군 인천광역시     군    411.43   68,940 \n10 거제시 경상남도       시    402.64   249,490\n# ℹ 216 more rows"
  },
  {
    "objectID": "posts/01/index.html#광역자치단체별로-평균-인구수를-구하기",
    "href": "posts/01/index.html#광역자치단체별로-평균-인구수를-구하기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "광역자치단체별로 평균 인구수를 구하기",
    "text": "광역자치단체별로 평균 인구수를 구하기\n\nssg_tbl &lt;- sgg_raw |&gt;\n  janitor::clean_names(ascii = FALSE) %&gt;%  \n  mutate(인구_명 = parse_number(인구_명)) |&gt;\n  group_by(광역자치단체) |&gt;\n  summarise(평균인구수 = mean(인구_명, na.rm = TRUE)) |&gt;\n  arrange(desc(평균인구수))"
  },
  {
    "objectID": "posts/01/index.html#범주형-데이터를-예쁜-그래프로-그리기",
    "href": "posts/01/index.html#범주형-데이터를-예쁜-그래프로-그리기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "범주형 데이터를 예쁜 그래프로 그리기",
    "text": "범주형 데이터를 예쁜 그래프로 그리기\n\nssg_gg &lt;- ssg_tbl %&gt;% \n  # 요기부터 그래프\n  ggplot(aes(x = fct_reorder(광역자치단체, 평균인구수),  y = 평균인구수)) +\n    geom_col(width = 0.1) +\n    geom_point(size = 3, color = \"red\") +\n    coord_flip() +\n    scale_y_continuous(labels = scales::comma) +\n    labs(\n      title = \"광역자치단체별 평균 인구수\",\n      subtitle = \"2024년 1월 기준\",\n      x = \"\",\n      y = \"평균 인구수\",\n      caption = \"출처: 위키백과\"\n    ) +\n    theme_minimal()\n\nssg_gg\n\n\n\n\n\n\n\nfs::dir_create(\"images\")\n\nggsave(\"images/ssg_gg.png\", width = 10, height = 6, dpi = 300)"
  },
  {
    "objectID": "posts/01/index.html#범주형-데이터를-예쁜-표로-그리기",
    "href": "posts/01/index.html#범주형-데이터를-예쁜-표로-그리기",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "범주형 데이터를 예쁜 표로 그리기",
    "text": "범주형 데이터를 예쁜 표로 그리기\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\n\nssg_gt &lt;- ssg_tbl %&gt;% \n  # 표 그릴꺼야\n  gt() %&gt;% \n    gt::tab_header(\n      title = \"광역자치단체별 평균 인구수\",\n      subtitle = \"2024년 1월 기준\"\n    ) %&gt;% \n    gt::cols_align(column = 광역자치단체, align = \"center\") %&gt;% \n    gt::fmt_integer(columns = 평균인구수) %&gt;% \n    gtExtras::gt_theme_excel()\n\nssg_gt\n\n\n\n\n\n\n\n광역자치단체별 평균 인구수\n\n\n2024년 1월 기준\n\n\n광역자치단체\n평균인구수\n\n\n\n\n경기도\n422,732\n\n\n서울특별시\n390,826\n\n\n대전광역시\n297,521\n\n\n인천광역시\n295,606\n\n\n광주광역시\n291,842\n\n\n대구광역시\n275,767\n\n\n울산광역시\n230,747\n\n\n부산광역시\n214,764\n\n\n경상남도\n187,279\n\n\n충청북도\n145,352\n\n\n충청남도\n141,691\n\n\n전북특별자치도\n134,608\n\n\n경상북도\n120,356\n\n\n강원특별자치도\n85,580\n\n\n전라남도\n85,266\n\n\n전라북도\n82,321\n\n\n\n\n\n\n ssg_gt %&gt;% \n  gtsave(\"images/ssg_gt.png\")\n\nRegistered S3 method overwritten by 'webshot2':\n  method        from   \n  print.webshot webshot\n\n\n\n\n\n광역자치단체 평균 인구수"
  },
  {
    "objectID": "posts/01/index.html#고급-쿼토",
    "href": "posts/01/index.html#고급-쿼토",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "고급 쿼토",
    "text": "고급 쿼토\n\n그래프 이미지표 이미지"
  },
  {
    "objectID": "link.html",
    "href": "link.html",
    "title": "Link",
    "section": "",
    "text": "￭ RTools https://cran.r-project.org/bin/windows/Rtools/\n - 설치위치를 C:\\Program Files\\Rtools 에서 C:\\Rtools 로 변경, 그 외에는 모두 그대로 설치\n￭ R 4.3.2 https://cran.yu.ac.kr/\n - 경로에서 공백과 한글은 오류 발생 가능하므로 수정하기\n - 설치위치를 C:\\Program Files\\R\\R-4.3.2 에서 C:\\R-4.3.2 로 변경\n￭ RStudio IDE(lastest) https://posit.co/downloads/\n - 설치위치를 C:\\Program Files\\RStudio 에서 C:\\RStudio 로 변경, 그 외에는 모두 그대로\n - 이 프로그램을 주로 사용, 작업표시줄 고정\n￭ D2Coding 폰트 https://github.com/naver/d2codingfont/releases/tag/VER1.3.2\n￭ 쿼토(Quarto) (V1.4 이상) https://quarto.org/docs/download/\n - Quarto는 데이터 과학, 학술 연구, 교육, 기술 문서 작성을 위한 오픈 소스 문서 생성 도구이고, 이는 R, Python, Julia, 다른 프로그래밍 언어의 코드를 포함한 문서를 만들 수 있게 해주며, 다양한 출력 형식(HTML, PDF, Microsoft Word, 슬라이드쇼 등)을 지원함."
  },
  {
    "objectID": "link.html#프로그램",
    "href": "link.html#프로그램",
    "title": "Link",
    "section": "",
    "text": "￭ RTools https://cran.r-project.org/bin/windows/Rtools/\n - 설치위치를 C:\\Program Files\\Rtools 에서 C:\\Rtools 로 변경, 그 외에는 모두 그대로 설치\n￭ R 4.3.2 https://cran.yu.ac.kr/\n - 경로에서 공백과 한글은 오류 발생 가능하므로 수정하기\n - 설치위치를 C:\\Program Files\\R\\R-4.3.2 에서 C:\\R-4.3.2 로 변경\n￭ RStudio IDE(lastest) https://posit.co/downloads/\n - 설치위치를 C:\\Program Files\\RStudio 에서 C:\\RStudio 로 변경, 그 외에는 모두 그대로\n - 이 프로그램을 주로 사용, 작업표시줄 고정\n￭ D2Coding 폰트 https://github.com/naver/d2codingfont/releases/tag/VER1.3.2\n￭ 쿼토(Quarto) (V1.4 이상) https://quarto.org/docs/download/\n - Quarto는 데이터 과학, 학술 연구, 교육, 기술 문서 작성을 위한 오픈 소스 문서 생성 도구이고, 이는 R, Python, Julia, 다른 프로그래밍 언어의 코드를 포함한 문서를 만들 수 있게 해주며, 다양한 출력 형식(HTML, PDF, Microsoft Word, 슬라이드쇼 등)을 지원함."
  },
  {
    "objectID": "link.html#유용한-사이트",
    "href": "link.html#유용한-사이트",
    "title": "Link",
    "section": "유용한 사이트",
    "text": "유용한 사이트\n￭ Posit Cloud(R설치 안하고 Web에서 R사용하는 곳) https://posit.cloud/\n￭ 데이터 사이언스를 위한 R프로그래밍 https://greendaygh.github.io/Rprog2021/\n￭ 국가통계포털 https://kosis.kr\n￭ 통계지리정보서비스 https://sgis.kostat.go.kr\n￭ 챗GPT 디지털 글쓰기 https://r2bit.com/gpt-writing/\n￭ Quarto Pub https://quartopub.com/\n￭ Github.com https://github.com/\n￭ ChatGPT https://chat.openai.com/\n￭ 한국R 사용자회 https://r2bit.com/\n￭ 네이버 글꼴 모음 https://hangeul.naver.com/font"
  },
  {
    "objectID": "link.html#참고문헌",
    "href": "link.html#참고문헌",
    "title": "Link",
    "section": "참고문헌",
    "text": "참고문헌\n￭ Excel, SPSS, R로 배우는 통계학 입문(강상욱 외 13명) \n￭ R을 이용한 누구나 하는 통계분석(안재형) \n￭ 기초확률론(이외숙)"
  },
  {
    "objectID": "posts/02/index.html",
    "href": "posts/02/index.html",
    "title": "02. 이산확률변수, 이항분포",
    "section": "",
    "text": "대표적인 이산확률분포를 간단히 정리하면 다음과 같다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n분포\n\\(X\\)\n모수\n확률질량함수\n평균(\\(\\mu\\))\n분산(\\(\\sigma^2\\))\n\n\n\n\n베르누이 분포\n한번 시행에서 성공하면 \\(1\\), 실패면 \\(0\\)\n\\(p\\)\n\n\\(p\\)\n\\(pq\\)\n\n\n이항분포\n\\(n\\)번 시행했을 때 성공 횟수\n\\(n,~p\\)\n\\(\\begin{pmatrix}n \\\\x\\end{pmatrix}p^{x}(1-p)^{n-x}\\)\n\\(np\\)\n\\(npq\\)\n\n\n기하분포\n처음 성공할 때까지의 시행 횟수\n\\(p\\)\n\\(p(1-p)^{x-1}\\)\n\\(\\dfrac{1}{p}\\)\n\\(\\dfrac{1-p}{p^2}\\)\n\n\n음이항분포\n\\(r\\)번째 성공할때까지의 총 시행 횟수\n\\(r,~p\\)\n\\(\\begin{pmatrix}n-1 \\\\r-1\\end{pmatrix}p^{r}(1-p)^{n-r}\\)\n\\(\\dfrac{r}{p}\\)\n\\(\\dfrac{r(1-p)}{p^2}\\)\n\n\n포아송 분포\n이항분포의 근사분포(\\(\\lambda=np\\), \\(n \\to \\infty\\))\n\\(\\lambda\\)\n\\(e^{-\\lambda}\\dfrac{\\lambda^{x}}{x!}\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n분포\n\\(X\\)\n모수\n확률질량함수\n평균(\\(\\mu\\))\n분산(\\(\\sigma^2\\))\n\n\n\n\n초기하분포\n\\(n\\)개 중 들어있는 \\(Type1\\)의 개수\n\\(N,~r,~n\\)\n\\(\\frac{\\begin{pmatrix}r \\\\x\\end{pmatrix}\\begin{pmatrix}N-r \\\\n-x\\end{pmatrix}}{\\begin{pmatrix}N \\\\n\\end{pmatrix}}\\)\n\\(n \\dfrac{r}{N}\\)\n\\(n \\dfrac{r}{N} \\left(1-\\dfrac{r}{N}\\right)\\left(\\dfrac{N-n}{N-1}\\right)\\)\n\n\n\n\n\n\n\n\n\n\n\n최빈값: \\([(n+1)p]\\)\n포아송분포로의 근사: \\(P(X=k)\\approx \\frac{e^{-\\lambda} \\lambda^k}{k!}\\), \\((\\lambda=np,~p\\approx 0)\\)\n정규분포로의 근사: \\(n\\)이 충분히 클때, \\(\\Phi\\left(\\frac{b+0.5-\\mu}{\\sigma}\\right)-\\Phi\\left(\\frac{a-0.5-\\mu}{\\sigma}\\right)\\), \\(\\Phi\\)는 표준정규분포의 누적분포함수\n\\(n\\)개의 서로 독립인 베르누이 분포를 따르는 확률변수들의 합의 분포\n\n\n\n\n\n꼬리확률(tail probability): \\(P(X&gt;n)=(1-p)^n\\)\n비기억성(memoryless property)을 가짐: \\(P(X&gt;n+k~|~X&gt;n)=P(X&gt;k)\\)\n\n\n\n\n\n\\(r\\)개의 서로 독립인 기하분포를 하는 확률분포들의 합의 분포\n\n\n\n\n\n최빈값: \\([\\lambda]\\)\n정규분포로의 근사: 모수 \\(\\lambda\\)가 클때\n\n\n\n\n\n포아송분포로의 근사: \\(p=\\frac{r}{N}\\)은 충분히 작고 \\(r\\)과 \\(N\\)은 클때\n이항분포로의 근사: \\(n\\ll N\\) 그리고 \\(r\\)과 \\(N-r\\)은 클때"
  },
  {
    "objectID": "posts/02/index.html#대표적인-이산확률분포",
    "href": "posts/02/index.html#대표적인-이산확률분포",
    "title": "02. 이산확률변수, 이항분포",
    "section": "",
    "text": "분포\n\\(X\\)\n모수\n확률질량함수\n평균(\\(\\mu\\))\n분산(\\(\\sigma^2\\))\n\n\n\n\n베르누이 분포\n한번 시행에서 성공하면 \\(1\\), 실패면 \\(0\\)\n\\(p\\)\n\n\\(p\\)\n\\(pq\\)\n\n\n이항분포\n\\(n\\)번 시행했을 때 성공 횟수\n\\(n,~p\\)\n\\(\\begin{pmatrix}n \\\\x\\end{pmatrix}p^{x}(1-p)^{n-x}\\)\n\\(np\\)\n\\(npq\\)\n\n\n기하분포\n처음 성공할 때까지의 시행 횟수\n\\(p\\)\n\\(p(1-p)^{x-1}\\)\n\\(\\dfrac{1}{p}\\)\n\\(\\dfrac{1-p}{p^2}\\)\n\n\n음이항분포\n\\(r\\)번째 성공할때까지의 총 시행 횟수\n\\(r,~p\\)\n\\(\\begin{pmatrix}n-1 \\\\r-1\\end{pmatrix}p^{r}(1-p)^{n-r}\\)\n\\(\\dfrac{r}{p}\\)\n\\(\\dfrac{r(1-p)}{p^2}\\)\n\n\n포아송 분포\n이항분포의 근사분포(\\(\\lambda=np\\), \\(n \\to \\infty\\))\n\\(\\lambda\\)\n\\(e^{-\\lambda}\\dfrac{\\lambda^{x}}{x!}\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n분포\n\\(X\\)\n모수\n확률질량함수\n평균(\\(\\mu\\))\n분산(\\(\\sigma^2\\))\n\n\n\n\n초기하분포\n\\(n\\)개 중 들어있는 \\(Type1\\)의 개수\n\\(N,~r,~n\\)\n\\(\\frac{\\begin{pmatrix}r \\\\x\\end{pmatrix}\\begin{pmatrix}N-r \\\\n-x\\end{pmatrix}}{\\begin{pmatrix}N \\\\n\\end{pmatrix}}\\)\n\\(n \\dfrac{r}{N}\\)\n\\(n \\dfrac{r}{N} \\left(1-\\dfrac{r}{N}\\right)\\left(\\dfrac{N-n}{N-1}\\right)\\)"
  },
  {
    "objectID": "posts/02/index.html#각-분포들의-특징",
    "href": "posts/02/index.html#각-분포들의-특징",
    "title": "02. 이산확률변수, 이항분포",
    "section": "",
    "text": "최빈값: \\([(n+1)p]\\)\n포아송분포로의 근사: \\(P(X=k)\\approx \\frac{e^{-\\lambda} \\lambda^k}{k!}\\), \\((\\lambda=np,~p\\approx 0)\\)\n정규분포로의 근사: \\(n\\)이 충분히 클때, \\(\\Phi\\left(\\frac{b+0.5-\\mu}{\\sigma}\\right)-\\Phi\\left(\\frac{a-0.5-\\mu}{\\sigma}\\right)\\), \\(\\Phi\\)는 표준정규분포의 누적분포함수\n\\(n\\)개의 서로 독립인 베르누이 분포를 따르는 확률변수들의 합의 분포\n\n\n\n\n\n꼬리확률(tail probability): \\(P(X&gt;n)=(1-p)^n\\)\n비기억성(memoryless property)을 가짐: \\(P(X&gt;n+k~|~X&gt;n)=P(X&gt;k)\\)\n\n\n\n\n\n\\(r\\)개의 서로 독립인 기하분포를 하는 확률분포들의 합의 분포\n\n\n\n\n\n최빈값: \\([\\lambda]\\)\n정규분포로의 근사: 모수 \\(\\lambda\\)가 클때\n\n\n\n\n\n포아송분포로의 근사: \\(p=\\frac{r}{N}\\)은 충분히 작고 \\(r\\)과 \\(N\\)은 클때\n이항분포로의 근사: \\(n\\ll N\\) 그리고 \\(r\\)과 \\(N-r\\)은 클때"
  },
  {
    "objectID": "posts/02/index.html#이산확률변수의-분포",
    "href": "posts/02/index.html#이산확률변수의-분포",
    "title": "02. 이산확률변수, 이항분포",
    "section": "이산확률변수의 분포",
    "text": "이산확률변수의 분포\n\n이산확률변수의 분포에서 확률질량함수, 누적확률분포함수, 평균, 분산 표준편차를 구하기\n\n[어느 대리점의 자동차 판매 대수], X=하루 동안 판매한 자동차의 대수\nX의 값 / 도수\n0 / 54\n1 / 117\n2 / 72\n3 / 42\n4 / 12\n5 / 3\n합계 / 300\n\n\n\nx1 &lt;-   rbind( 0, 1, 2, 3, 4, 5)\nfreq &lt;- rbind(54, 117, 72, 42, 12, 3)\ntotal &lt;- function(x,y){\n  fx=y/sum(y)\n  cdf=cumsum(fx)\n  prob=sum(fx)\n  xfx=x*fx\n  mean=sum(xfx)\n  x2fx=x^2*fx\n  mix=(x-1.5)^2*fx\n  variance=sum(mix)\n  s.d=sqrt(variance)\n  cat( \" &lt;x&gt;               0    1    2    3    4    5 \", \"\\n\",\n  \"&lt;f(x)&gt;          \",fx,\"\\n\",\n  \"&lt;F(x)&gt;          \",cdf,\"\\n\",\n  \"&lt;x*f(x)&gt;        \",xfx,\"\\n\",\n  \"&lt;x^2*f(x)&gt;      \",x2fx,\"\\n\",\n  \"&lt;(x-1.5)^2*f(x)&gt;\",mix,\"\\n\",\"\\n\",\n  \"&lt;sum(f(x))&gt;     \",prob,\"\\n\",\n  \"&lt;E(x)&gt;          \",mean,\"\\n\",\n  \"&lt;V(x)&gt;          \",variance,\"\\n\",\n  \"&lt;S(x)&gt;          \",s.d,\"\\n\")\n}\n\ntotal(x1, freq)\n\n &lt;x&gt;               0    1    2    3    4    5  \n &lt;f(x)&gt;           0.18 0.39 0.24 0.14 0.04 0.01 \n &lt;F(x)&gt;           0.18 0.57 0.81 0.95 0.99 1 \n &lt;x*f(x)&gt;         0 0.39 0.48 0.42 0.16 0.05 \n &lt;x^2*f(x)&gt;       0 0.39 0.96 1.26 0.64 0.25 \n &lt;(x-1.5)^2*f(x)&gt; 0.405 0.0975 0.06 0.315 0.25 0.1225 \n \n &lt;sum(f(x))&gt;      1 \n &lt;E(x)&gt;           1.5 \n &lt;V(x)&gt;           1.25 \n &lt;S(x)&gt;           1.118034"
  },
  {
    "objectID": "posts/02/index.html#r명령어-설명",
    "href": "posts/02/index.html#r명령어-설명",
    "title": "02. 이산확률변수, 이항분포",
    "section": "R명령어 설명",
    "text": "R명령어 설명\n\n이항분포(Binomial Distribution)에 관련된 R 명령어 설명  \n\n위에 링크로 달아놓았지만, ChatGPT에게 이항분포에 관련된 r명령어를 묻고, 명령어는 알지만 더 자세한 사용법을 알고 싶을때는 프로그램에서 명령어에 커서를 두고 F1을 누르면 설명이 나옵니다.\n\nR 에서 제공하는 확률분포\n▶ 이산분포\n   ● binom : 이항분포\n   ● hyper : 초기하분포\n   ● pois  : 포와송분포\n   ● geom : 기하분포\n   ● nbinom : 음이항분포\n   ● multinom : 다항분포\n▶ 연속분포\n  ● unif : Uniform Distribution(균등분포)\n  ● norm : 정규분포\n  ● exp : 지수분포(Exponential Distribution)\n  ● t : t분포\n  ● f : f분포\n  ● chisq : 카이분포\n  ● 기타 : gamma, beta, cauchy, lnorm, weibull 등\n▶ 접두사: 위 함수앞에 아래의 접두사를 붙여 사용한다.\n  ● d : probability density(mass) function - 확률질량함수\n  ● p : cumulative distribution function - 누적함수\n  ● q : quantile function ( p ≤ P (X  ≤ x ) 를 만족하는 최소 x )\n  ● r : random number generator\n\n\n이항분포에 관련된 R명령어\n\ndbinom(3, 10, 0.3)  # 10번의 시행에서 성공 확률이 0.3인 경우에 3번 성공할 확률\n\n[1] 0.2668279\n\npbinom(3, 10, 0.3)  # 10번의 시행에서 성공 확률이 0.3인 경우에 3번 이하로 성공할 확률\n\n[1] 0.6496107\n\nqbinom(0.25, 10, 0.3)  # 10번의 시행에서 성공 확률이 0.3인 이항분포에서 하위 25%에 해당하는 성공 횟수\n\n[1] 2\n\nrbinom(5, 10, 0.3)  # 10번의 시행에서 성공 확률이 0.3인 이항분포를 따르는 난수 5개 생성\n\n[1] 6 2 2 1 1\n\n# 이항분포의 확률질량함수 그리기\nsize &lt;- 10  # 시행 횟수와 성공 확률 설정\nprob &lt;- 0.5\n\nx &lt;- 0:size  # 가능한 모든 성공 횟수에 대한 확률 계산\ny &lt;- dbinom(x, size, prob)\nbarplot(y, names.arg=x, xlab=\"성공 횟수\", ylab=\"확률\", main=\"이항 분포 (n=10, p=0.5)\")  # 막대 그래프로 그리기\n\n\n\n\n\n\n\n# 이항분포의 누적분포함수 그리기\ncumulative_probs &lt;- pbinom(x, size, prob)  # 누적 확률 계산\n# 계단 형태의 그래프로 그리기\nplot(x, cumulative_probs, type=\"s\", xlab=\"성공 횟수\", ylab=\"누적 확률\", main=\"이항 분포의 누적 분포 함수 (n=10, p=0.5)\")\n\n\n\n\n\n\n\n\n\n\nggplot 패키지를 사용해서 그래프 그리기\n\n# install.packages(\"ggplot\")  # ggplot페키지가 설치가 안된 경우 왼쪽 주석 지우고 명령어 실행\n\n\n# x를 정의 합니다 \nx &lt;- c(0:10) \nx_prob &lt;- x / 10 \n\n# 이항분포를 계산하는 데이터프레임을 만듭니다.\nlibrary(tidyverse) \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf &lt;- tibble(\n  x = x,\n  x_prob = x / 10, \n  # 반복횟수 10회, 성공확률 0.5인 이항분포에 대한 각각의 확률값을 구합니다. \n  dbinom = dbinom(x, 10, 0.5), \n  pbinom = pbinom(x, 10, 0.5),  \n  qbinom = qbinom(x_prob, 10, 0.5), \n  rbinom = rbinom(x, 10, 0.5)\n)\n\n# 데이터를 확인 합니다. \ndf \n\n# A tibble: 11 × 6\n       x x_prob   dbinom   pbinom qbinom rbinom\n   &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n 1     0    0   0.000977 0.000977      0      6\n 2     1    0.1 0.00977  0.0107        3      4\n 3     2    0.2 0.0439   0.0547        4      5\n 4     3    0.3 0.117    0.172         4      4\n 5     4    0.4 0.205    0.377         5      4\n 6     5    0.5 0.246    0.623         5      7\n 7     6    0.6 0.205    0.828         5      4\n 8     7    0.7 0.117    0.945         6      4\n 9     8    0.8 0.0439   0.989         6      1\n10     9    0.9 0.00977  0.999         7      8\n11    10    1   0.000977 1            10      5\n\n\n\n## 확률질량함수 pmf 그래프 그리기 : y = dbinom 결과값 이용\ndf %&gt;% \n  ggplot(aes(x = x, y = dbinom)) + \n  geom_col() + \n  scale_x_continuous(breaks = c(0:10), labels = c(0:10)) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n## 누적확률질량함수 cdf 그래프 그리기 : y = pbinom 결과값 \ndf %&gt;% \n  ggplot(aes(x = x, y = pbinom)) + \n  geom_col() + \n  scale_x_continuous(breaks = c(0:10), labels = c(0:10)) +\n  theme_classic()"
  },
  {
    "objectID": "posts/02/index.html#이항분포-관련-문제",
    "href": "posts/02/index.html#이항분포-관련-문제",
    "title": "02. 이산확률변수, 이항분포",
    "section": "이항분포 관련 문제",
    "text": "이항분포 관련 문제\n\n확률변수 \\(X\\)가 이항분포 \\(B(10, 1/3)\\)을 따를 때, \\(P(X≤2)\\)와 \\(P(X&gt;3)\\)를 구하시오.\n\n\npbinom(2, 10, 0.3333)\n\n[1] 0.2992194\n\n1-pbinom(3, 10, 0.3333)\n\n[1] 0.4406446\n\n\n\n어떤 병이 새로운 치료법으로 치료될 확률이 20%라고 한다. 15명의 환자에게 이 치료법을 적용하였을때 4명 이상 7명 이하가 치료될 확률을 구하시오.\n\n\npbinom(7, 15, 0.2) - pbinom(3, 15, 0.2)\n\n[1] 0.3475981\n\n\n\n확률변수 \\(X\\)가 이항분포 \\(B(n,0.15)\\)를 따를 때, \\(P(X=1)&gt;P(X=0)\\)을 만족시키는 자연수 \\(n\\)의 최솟값을 구하시오.\n\n\nn &lt;- 1\nwhile(dbinom(1,n,0.15) &lt;= dbinom(0,n,0.15)){\n  n &lt;- n+1\n}\nn\n\n[1] 6"
  },
  {
    "objectID": "posts/04/index.html",
    "href": "posts/04/index.html",
    "title": "04. 초기하분포, 기하분포",
    "section": "",
    "text": "초기하분포, 기하분포 (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/04/index.html#초기하분포",
    "href": "posts/04/index.html#초기하분포",
    "title": "04. 초기하분포, 기하분포",
    "section": "초기하분포",
    "text": "초기하분포\n\nR명령어 설명\n\n초기하분포(Hypergeometric Distribution)에 관련된 R 명령어 설명\n위에 링크로 달아놓았지만, ChatGPT에게 초기하분포에 관련된 r명령어를 묻고, 명령어는 알지만 더 자세한 사용법을 알고 싶을때는 프로그램에서 명령어에 커서를 두고 F1을 누르면 설명이 나옵니다.\n\n\n# 성공적인 요소 20개, 실패적인 요소 80개, 총 15개의 요소를 뽑는 경우\nm &lt;- 20\nn &lt;- 80\nk &lt;- 15\n\n# 확률밀도함수: 5개의 성공적인 요소를 뽑을 확률\ndhyper(5, m, n, k)\n\n[1] 0.1007633\n\n# 누적분포함수: 5개 이하의 성공적인 요소를 뽑을 누적 확률\nphyper(5, m, n, k)\n\n[1] 0.9538525\n\n# 분위수 함수: 누적 확률 0.5에 해당하는 성공적인 요소의 수\nqhyper(0.5, m, n, k)\n\n[1] 3\n\n# 랜덤 샘플링: 위 조건을 갖는 초기하분포에서 10개의 샘플 생성\nrhyper(10, m, n, k)\n\n [1] 3 1 3 4 4 1 5 3 4 3\n\n# 초기하분포의 확률질량함수 그리기\nm &lt;- 20  # 성공 가능한 항목의 수\nn &lt;- 30  # 실패 가능한 항목의 수\nk &lt;- 10  # 추출되는 항목의 총 수\nx_values &lt;- 0:min(k, m)  # 가능한 성공 횟수의 범위\nprobabilities &lt;- dhyper(x_values, m, n, k)  # 확률 질량 함수 계산\nplot(x_values, probabilities, type = \"h\", lwd = 2, col = \"blue\",\n     main = \"Hypergeometric Distribution\",\n     xlab = \"Number of Successes\", ylab = \"Probability\")  # 확률 분포 그래프 그리기\n\n\n\n\n\n\n\n# 확률질량 또는 매개변수 N, n, k가 있는 초기하분포와 관련된 분포함수를 그리기\nlibrary(LearningStats)\n\nWarning: package 'LearningStats' was built under R version 4.3.3\n\nN=20;n=12;k=5\nplotHyper(N,n,k,type=\"d\")\n\n\n\n\n\n\n\n\nProbability mass and distribution function associated with a H(N,n,k)\n \n\n\n x P(H(N,n,k)=x) P(H(N,n,k)&lt;=x)\n 0       0.00361        0.00361\n 1       0.05418        0.05779\n 2       0.23839        0.29618\n 3       0.39732        0.69350\n 4       0.25542        0.94892\n 5       0.05108        1.00000\n\nplotHyper(N,n,k,type=\"p\",col=\"pink\")\n\n\n\n\n\n\n\n\nProbability mass and distribution function associated with a H(N,n,k)\n \n\n\n x P(H(N,n,k)=x) P(H(N,n,k)&lt;=x)\n 0       0.00361        0.00361\n 1       0.05418        0.05779\n 2       0.23839        0.29618\n 3       0.39732        0.69350\n 4       0.25542        0.94892\n 5       0.05108        1.00000\n\nplotHyper(N,n,k)\n\n\n\n\n\n\n\n\nProbability mass and distribution function associated with a H(N,n,k)\n \n\n\n x P(H(N,n,k)=x) P(H(N,n,k)&lt;=x)\n 0       0.00361        0.00361\n 1       0.05418        0.05779\n 2       0.23839        0.29618\n 3       0.39732        0.69350\n 4       0.25542        0.94892\n 5       0.05108        1.00000\n\n\n\n\nggplot 패키지를 사용해서 그래프 그리기\n\nggplot 패키지 사용법1\nggplot 패키지 사용법2\nggplot 패키지 사용법3\nggplot2 패키지로 히스토그램 그리기\n흰 공 25개, 검은 공 35개가 있는 주머니에서 랜덤하게 공을 10개를 뽑는 초기하분포를 만족하는 난수 100개를 x_random 열에 저장한 이터프레임을 생성하시오.\n\n\nlibrary(tidyverse)   \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 동일한 결과를 얻기 위해 난수 생성 씨드를 정합니다. \nset.seed(1234) \n\n# rhyper로 난수를 생성합니다: K = 25, N-K = 35, n = 10\ndf &lt;- tibble(\n  x_random = rhyper(100, 25, 35, 10)\n)\n\n# 6줄을 출력해 봅니다. \nhead(df)\n\n# A tibble: 6 × 1\n  x_random\n     &lt;int&gt;\n1        2\n2        5\n3        5\n4        5\n5        6\n6        5\n\n# 히스토그램으로 난수 생성이 어떻게 분포 되었는지 살펴보기\n\n# install.packages(\"ggplot\")  # ggplot페키지가 설치가 안된 경우 왼쪽 주석 지우고 명령어 실행\n\ndf %&gt;% \n  ggplot(aes(x = x_random)) +\n  geom_bar(binwidth = 40) + \n  scale_x_continuous(breaks = c(0:10), labels = c(0:10), limits = c(0, 10)) +\n  theme_classic()\n\nWarning in geom_bar(binwidth = 40): Ignoring unknown parameters: `binwidth`\n\n\n\n\n\n\n\n\n# 확률질량함수 pmf 그래프 : 위와 동일한 조건에서 0~10까지 흰 공이 뽑힌 모집단 확률 그래프 \n\n# 흰 공 25개, 검은 공 35개가 있는 주머니에서 랜덤하게 공을 10개를 뽑는 초기하분포에서 흰공이 0~10개까지 나올 확률을 계산하여 그래프로 나타내기 \n\n# 확률을 계산 : 일반 확률은 dhyper 함수를 사용하고, 누적 확률은 phyper를 사용함 \n# 여기서 x는 10개 표본 중 흰공의 수\n\ndf_hyper &lt;- tibble(\n  x = c(0:10), \n  dhyper = dhyper(x, 25, 35, 10),\n  phyper = phyper(x, 25, 35, 10) \n)\n\n# 누적확률질량 함수 cdf 그래프 : y = phyper 값을 이용\n\ndf_hyper %&gt;% \n  ggplot(aes(x = x, y = phyper)) + \n  geom_col() +\n  scale_x_continuous(breaks = c(0:10), labels = c(0:10)) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n초기하분포 관련 문제\n\n4개의 불량품이 섞여있는 20개의 부품이 들어있는 상자로부터 3개의 부품을 꺼내었을 때의 불량품의 수를 \\(X\\)라 하자. \\(E(X)\\)와 \\(Var(x)\\)를 구하여라.\n\n\n# 파라미터 설정\nM &lt;- 20  # 전체 요소의 수\nn &lt;- 4   # 성공 요소의 수\nN &lt;- 3   # 시행 횟수\n\n# 확률 질량 함수 계산\nk &lt;- seq(max(0, N - (M-n)), min(n, N))\npmf &lt;- dhyper(k, n, M - n, N)\n\n# 그래프 그리기\nplot(k, pmf, type=\"h\", lwd=2, col=\"blue\",\n     xlab=\"Number of Successes\", ylab=\"Probability\",\n     main=\"Hypergeometric Distribution PMF\")\n\n\n\n\n\n\n\n# 평균과 분산 계산\nmean &lt;- N * n / M\nvariance &lt;- N * n * (M - n) * (M - N) / (M^2 * (M - 1))\n\n# 결과 출력\nmean\n\n[1] 0.6\n\nvariance\n\n[1] 0.4294737\n\n\n\n\n초기하분포의 이항분포로의 근사(시행횟수 \\(n\\)이 \\(0.05N\\) 이하(전체수 \\(N\\)))\n\n10개의 불량품이 섞여있는 2000개의 부품이 들어있는 상자로부터 5개의 부품을 꺼내었을 때의 불량품의 수를 \\(X\\)라 하자. \\(E(X)\\)와 \\(Var(x)\\)를 구하여라.(그래프의 모양, 평균과 분산의 값을 비교하기)\n\n\n# 초기하분포의 모수 설정\nM &lt;- 2000  # 전체 요소의 수\nn &lt;- 10   # 성공 요소의 수\nN &lt;- 5   # 시행 횟수\n\n# 확률 질량 함수 계산\nk &lt;- seq(max(0, N - (M-n)), min(n, N))\npmf &lt;- dhyper(k, n, M - n, N)\n\n# 그래프 그리기\nplot(k, pmf, type=\"h\", lwd=2, col=\"blue\",\n     xlab=\"Number of Successes\", ylab=\"Probability\",\n     main=\"Hypergeometric Distribution PMF\")\n\n\n\n\n\n\n\n# 초기하분포의 평균과 분산 계산\nmean &lt;- N * n / M\nvariance &lt;- N * n * (M - n) * (M - N) / (M^2 * (M - 1))\n\n# 결과 출력\nmean\n\n[1] 0.025\n\nvariance\n\n[1] 0.02482523\n\n\n\n# 이항분포의 모수 설정\nM &lt;- 2000  # 전체 요소의 수\nn &lt;- 10   # 성공 요소의 수\nN &lt;- 5   # 시행 횟수\n\n# 이항분포의 확률질량함수 그리기\nsize &lt;- 5  # 시행 횟수와 성공 확률 설정\nprob &lt;- 0.005\n\nx &lt;- 0:size  # 가능한 모든 성공 횟수에 대한 확률 계산\ny &lt;- dbinom(x, size, prob)\nbarplot(y, names.arg=x, xlab=\"성공 횟수\", ylab=\"확률\", main=\"이항 분포 (n=5, p=0.005)\")  # 막대 그래프로 그리기\n\n\n\n\n\n\n\n# 이항분포의 평균과 분산 계산\nmean &lt;- N * n / M\nvariance &lt;- N * n * (M - n)/ (M^2)\n\n# 결과 출력\nmean\n\n[1] 0.025\n\nvariance\n\n[1] 0.024875"
  },
  {
    "objectID": "posts/04/index.html#기하분포",
    "href": "posts/04/index.html#기하분포",
    "title": "04. 초기하분포, 기하분포",
    "section": "기하분포",
    "text": "기하분포\n\nR명령어 설명\n\n기하분포(Geometric Distribution)에 관련된 R 명령어 설명\n\n\n# 성공 확률이 0.2인 기하분포를 만들기 위해 모수 정의\nprob &lt;- 0.2\n\n# 확률밀도함수: 3번 실패 후 4번째 처음 성공할 확률\ndgeom(3, prob)\n\n[1] 0.1024\n\n# 누적분포함수: 4번의 사건발생에서 첫번째 ~ 네번째에서 처음 성공한 확률을 모두 더함\npgeom(3, prob)\n\n[1] 0.5904\n\n# 분위수 함수, 누적분포함수의 역함수: 누적 확률 0.5이 될때까지의 실패 횟수\nqgeom(0.5, prob)\n\n[1] 3\n\n# 임의추출: 성공 확률이 0.2인 기하분포로부터 5개 샘플 생성\nrgeom(5, prob)\n\n[1] 12  5  2  5 13\n\n# 확률질량함수\npar(mar=c(5.1, 4.1, 4.1, 7),xpd=TRUE)\nplot(0,type=\"n\", xlim=c(0,100),ylim=c(0,1),ann=FALSE)\np=c(0.1,0.3,0.5,0.7,0.9)\nfor (i in 1:length(p)){\n  x=0:100\n  y=dgeom(x,p[i])\n  points(x,y,type=\"l\",col=rainbow(length(p))[i])\n}\ntitle(main=\"PMF of geom\",xlab=\"x\",ylab=\"p(x)\",cex.main=2,cex.lab=1.2)\nbox(\"outer\",col=\"gray\")\nlegend(\"topright\",inset=c(-0.15,0), legend=paste(\"p=\",p),col=rainbow(length(p)),lty=1)\n\n\n\n\n\n\n\n# 누적분포함수\npar(mar=c(5.1, 4.1, 4.1, 7),xpd=TRUE)\nplot(0,type=\"n\", xlim=c(0,100),ylim=c(0,1),ann=FALSE)\np=c(0.1,0.3,0.5,0.7,0.9)\nfor (i in 1:length(p)){\n  x=0:100\n  y=pgeom(x,p[i])\n  points(x,y,type=\"l\",col=rainbow(length(p))[i])\n  }\n\ntitle(main=\"CMF of binom\",xlab=\"x\",ylab=\"p(x)\",cex.main=2,cex.lab=1.2)\nbox(\"outer\",col=\"gray\")\nlegend(\"topright\",inset=c(-0.15,0),legend=paste(\"p=\",p),col=rainbow(length(p)),lty=1)\n\n\n\n\n\n\n\n\n\n\n기하분포 관련 문제\n\n로또복권에 당첨될 확률(3등 이상)은 0.028이다. 매주 복권을 1장씩 산다.\n\n\n1년 이내에 당첨될 확률은 얼마인가?\n\n당첨될 확률이 90% 이상 되기 위해서는 얼마동안 복권을 사야 할까?\n\n작년 한 해 동안 당첨되지 못하였다. 올해에는 당첨될 확률은?\n\n\n\noptions(digits=5) # 값을 소수점 5자리까지 출력\nprob &lt;- 0.028\npgeom(51, prob) # (1) 누적분포함수 이용, 52회 실행하며 첫 번째 당첨이 발생하기 전에 51번 이하로 실패할 누적확률\n\n[1] 0.77163\n\nqgeom(0.1, prob) # (2) 이 함수의 결과는 오답이며, qgeom은 첫 번째 성공이 발생하기 전까지의 실패 확률이 90% 이하가 되는 지점을 찾는 것을 의미하며, 적어도 한 번 성공할 확률이 90% 이상이 되기 위한 시도 횟수를 찾는 것과는 다른 접근이며, 문제를 해결하기 위해서는 while 루프 방식의 계산이 적합함\n\n[1] 3\n\n# ChatGPT가 알려준 (2)번 풀이에 대한 코드\nwin_probability &lt;- 0.028\ntarget_probability &lt;- 0.90\nn &lt;- 0  # 주 수 카운터\ncurrent_probability &lt;- 0\nwhile (current_probability &lt; target_probability) {\n  n &lt;- n + 1  # 90% 이상 당첨 확률이 될 때까지 반복\n  current_probability &lt;- 1 - (1 - win_probability)^n\n}\nn  # 필요한 주 수 출력\n\n[1] 82\n\npgeom(51, prob) # (3) 기하분포의 무기억성의 성질에 의해 (1)과 같은 확률임\n\n[1] 0.77163"
  },
  {
    "objectID": "posts/06/index.html",
    "href": "posts/06/index.html",
    "title": "06. 지수분포",
    "section": "",
    "text": "지수분포 (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/06/index.html#지수분포",
    "href": "posts/06/index.html#지수분포",
    "title": "06. 지수분포",
    "section": "지수분포",
    "text": "지수분포\n\nR명령어 설명\n\n지수분포(Exponential Distribution)에 관련된 R 명령어 설명   \n\n\n위에 링크로 달아놓았지만, ChatGPT에게 지수분포에 관련된 r명령어를 묻고, 명령어는 알지만 더 자세한 사용법을 알고 싶을때는 프로그램에서 명령어에 커서를 두고 F1을 누르면 설명이 나옵니다.\n\ndexp(1, 0.2)  # 비율이 0.2일 때, 1단위시간 내에 사건이 발생할 밀도 계산\n\n[1] 0.1637462\n\npexp(1, 0.2)  # 비율이 0.2일 때, 1단위시간 이내에 사건이 발생할 누적 확률 계산\n\n[1] 0.1812692\n\nqexp(0.5, 0.2)  # 비율이 0.2일 때, 50%의 확률로 발생할 수 있는 사건의 시간 계산\n\n[1] 3.465736\n\nrexp(5, 0.2)  # 비율이 0.2인 지수 분포에서 5개의 무작위 사건 시간 생성\n\n[1] 11.374880  3.239535  8.815600  5.755029  3.175466\n\n# curve함수를 이용해서 지수분포의 확률밀도함수 그리기\nrate &lt;- 0.5  # 지수 분포의 비율(λ) 설정\ncurve(dexp(x, rate), from = 0, to = 10, # 지수 분포의 확률 밀도 함수 그래프 그리기\n      xlab = \"x\", ylab = \"Density\", \n      main = \"Exponential Distribution Density\", col = \"blue\")\n\n\n\n\n\n\n\n# plot함수를 이용해서 지수분포의 확률밀도함수 그리기\nx_values &lt;- seq(0, 10, by = 0.1)  # 확률 밀도 함수를 계산할 값의 범위\ndensities &lt;- dexp(x_values, rate)  # 확률 밀도 계산\nplot(x_values, densities, type = \"l\", col = \"red\",  # 확률 밀도 그래프 그리기\n     main = \"Exponential Distribution Density\", \n     xlab = \"x\", ylab = \"Density\")\n\n\n\n\n\n\n\n# curve함수를 이용해서 지수분포의 누적분포함수 그리기\nrate &lt;- 0.5  # 지수 분포의 비율(λ) 설정\ncurve(pexp(x, rate), from = 0, to = 10,  # 지수 분포의 누적 분포 함수 그래프 그리기\n      xlab = \"x\", ylab = \"Cumulative Probability\", \n      main = \"Exponential Distribution CDF\", col = \"purple\")\n\n\n\n\n\n\n\n# 세 개의 모수(0.5, 1, 1.5)에 대한 지수분포의 확률밀도함수 그리기\nrates &lt;- c(0.5, 1, 1.5)  # 세 개의 모수 설정\ncolors &lt;- c(\"red\", \"blue\", \"brown\")  # 색상 설정\ncurve(dexp(x, rates[1]), from = 0, to = 10, ylim = c(0, 1.5),  # 첫 번째 분포 그리기\n      xlab = \"x\", ylab = \"Density\",\n      main = \"Exponential Distributions with Different Rates\",\n      col = colors[1])\nfor (i in 2:length(rates)) {  # 나머지 분포 추가\n  curve(dexp(x, rates[i]), from = 0, to = 10, add = TRUE, col = colors[i])\n}\nlegend(\"topright\", legend = paste(\"Rate =\", rates), col = colors, lty = 1)# 범례 추가\n\n\n\n\n\n\n\n\n\n\n지수분포 관련 문제\n\n어떤 화학물질에서 \\(\\alpha\\)입자가 10초당 평균 5개씩 포아송 과정을 따라 발생한다고 하자. 이때 첫번째 입자가 발생될 때까지 걸리는 시간 \\(X\\)가 5초 이상일 확률을 구하시오.\n\n\n1-pexp(5, 0.5)\n\n[1] 0.082085"
  },
  {
    "objectID": "posts/08/index.html",
    "href": "posts/08/index.html",
    "title": "08. 모집단과 표본, 모평균의 추정",
    "section": "",
    "text": "모집단과 표본, 모평균의 추정 (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/08/index.html#모집단과-표본",
    "href": "posts/08/index.html#모집단과-표본",
    "title": "08. 모집단과 표본, 모평균의 추정",
    "section": "모집단과 표본",
    "text": "모집단과 표본\n\n표본평균의 분포\n\n히스토그램을 그려 표본수의 증가에 따른 변화를 관찰하기\n\n\n# 난수발생에 따라 평균값이 xlim=c(0.3, 1.7)을 벗어나거나, \n# 도수가 40보다 많아 ylim=c(0, 40)의 범위를 벗어나는 경우가 있을 수 있다.\n\npar(mfrow=c(2,2))\n\ncolumn_1 &lt;- c()\nfor (i in 1:100) column_1[i] &lt;- mean(runif(5,0,2)) # 균등분포에서 난수발생하기\nhist(column_1, border=\"white\", col=\"blue\", main=\"mean of 5 samples\",\n     xlim=c(0.3,1.7), ylim=c(0,40), xlab=\"Class\", ylab=\"Frequency\")\n\ncolumn_2 &lt;- c()\nfor (i in 1:100) column_2[i] &lt;- mean(runif(10,0,2))\nhist(column_2, border=\"white\", col=\"blue\", main=\"mean of 10 samples\",\n     xlim=c(0.3,1.7), ylim=c(0,40), xlab=\"Class\", ylab=\"Frequency\")\n\ncolumn_3 &lt;- c()\nfor (i in 1:100) column_3[i] &lt;- mean(runif(30,0,2))\nhist(column_3, border=\"white\", col=\"blue\", main=\"mean of 30 samples\",\n     xlim=c(0.3,1.7), ylim=c(0,40), xlab=\"Class\", ylab=\"Frequency\")\n\ncolumn_4 &lt;- c()\nfor (i in 1:100) column_4[i] &lt;- mean(runif(100,0,2))\nhist(column_4, border=\"white\", col=\"blue\", main=\"mean of 100 samples\",\n     xlim=c(0.3,1.7), ylim=c(0,40), xlab=\"Class\", ylab=\"Frequency\")\n\n\n\n\n\n\n\n\n\n표본수의 증가에 따른 기술통계량의 변화를 관찰하기\n\n중심극한정리에 의하면 표본평균 \\(\\bar{X}\\)의 분포는 이론적으로 평균이 \\(\\mu=\\frac{0+2}{2}=1\\), 분산이 \\(\\frac{\\sigma^2}{n}=\\frac{(2-0)^2}{12}\\times\\frac{1}{5}=\\frac{1}{15}=0.06667\\)이다. 계산된 요약 통계량을 보면 이론값과 근사적으로 같다는 것을 확인할 수 있다. 최솟값은 점점 커지면서 1로 가고 있고 최댓값은 점점 작아지면서 1로 가고 있기에 대수의 법칙이 작동하고 있음을 알 수 있다.\n\nstat_1 &lt;- c(mean(column_1), var(column_1), median(column_1), \n            min(column_1), max(column_1))\nstat_2 &lt;- c(mean(column_2), var(column_2), median(column_2), \n            min(column_2), max(column_2))\nstat_3 &lt;- c(mean(column_3), var(column_3), median(column_3), \n            min(column_3), max(column_3))\nstat_4 &lt;- c(mean(column_4), var(column_4), median(column_4), \n            min(column_4), max(column_4))\n\nSimStat &lt;- cbind(stat_1, stat_2, stat_3, stat_4) \nrownames(SimStat) &lt;- c(\"mean\", \"var\", \"median\", \"min\", \"max\")\ncolnames(SimStat) &lt;- c(\"n=5\", \"n=10\", \"n=30\", \"n=100\") \n\nSimStat\n\n              n=5       n=10       n=30       n=100\nmean   1.04403650 0.99194277 0.99585782 0.993736371\nvar    0.04754242 0.02959242 0.01132904 0.002992766\nmedian 1.04478925 0.98403686 0.98856144 0.990584588\nmin    0.43296607 0.50810527 0.71512453 0.877360070\nmax    1.51308187 1.39665089 1.28565056 1.141150215"
  },
  {
    "objectID": "posts/08/index.html#모평균의-추정",
    "href": "posts/08/index.html#모평균의-추정",
    "title": "08. 모집단과 표본, 모평균의 추정",
    "section": "모평균의 추정",
    "text": "모평균의 추정\n\n모평균의 구간추정, 신뢰구간\n\n다음은 베어링의 평균 직경을 알아보고자 9개의 베어링 표본을 대상으로 조사한 것이다. 3.4, 3.3, 4.2, 4.4, 3.7, 4.5, 4.6, 3.8, 4.1 평균 직경에 대한 95% 신뢰구간을 구하시오.\n\n\nx &lt;- c(3.4, 3.3, 4.2, 4.4, 3.7, 4.5, 4.6, 3.8, 4.1)\n\nlibrary(TeachingDemos)\n\nWarning: package 'TeachingDemos' was built under R version 4.3.3\n\nz.test(x, sd=0.4, 9)  # 과거로부터 σ=0.4로 알려졌을때 신뢰구간\n\n\n    One Sample z-test\n\ndata:  x\nz = -37.5, n = 9.00000, Std. Dev. = 0.40000, Std. Dev. of the sample\nmean = 0.13333, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 9\n95 percent confidence interval:\n 3.738671 4.261329\nsample estimates:\nmean of x \n        4 \n\nt.test(x)  # 모표준편차를 모를때 표본표준편차를 이용한 t분포로 구한 신뢰구간\n\n\n    One Sample t-test\n\ndata:  x\nt = 25.298, df = 8, p-value = 6.384e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3.635389 4.364611\nsample estimates:\nmean of x \n        4 \n\n\n\n\n표본수의 결정\n\n앞의 예제에서 오차한계 \\(d\\)가 \\(0.2\\)라면 표본수 \\(n\\)은 얼마가 되어야 하는지 구하시오. (\\(\\alpha=0.05\\))\n\n\n# 오차한계 d=0.2일때때\nzstar = qnorm(.975) # σ=0.4로 알려졌을때 n의 개수\nsigma = 0.4 \nE = 0.2 \nzstar^2 * sigma^2/ E^2\n\n[1] 15.36584\n\nzstar = qt(.975, 15) # 모표준편차를 모를때, 자유도를 15로 사용할때의 n의 개수\nsigma = 0.4 \nE = 0.2 \nzstar^2 * sigma^2/ E^2  \n\n[1] 18.17231\n\nzstar = qt(.975, 17) # 모표준편차를 모를때, 자유도를 17로 사용할때의 n의 개수 \nsigma = 0.4 \nE = 0.2 \nzstar^2 * sigma^2/ E^2    \n\n[1] 17.80529\n\n# 따라서 n=18이면 적절하다."
  },
  {
    "objectID": "posts/10/index.html",
    "href": "posts/10/index.html",
    "title": "10. 하나의 모집단에 대한 가설검정(평균)",
    "section": "",
    "text": "하나의 모집단에 대한 가설검정(평균) (sasa메일로 로그인해야 내용을 볼 수 있습니다.)"
  },
  {
    "objectID": "posts/10/index.html#모평균-mu에-대한-검정",
    "href": "posts/10/index.html#모평균-mu에-대한-검정",
    "title": "10. 하나의 모집단에 대한 가설검정(평균)",
    "section": "모평균 \\(\\mu\\)에 대한 검정",
    "text": "모평균 \\(\\mu\\)에 대한 검정\n\n\\(\\sigma\\)를 아는 경우\n\n어떤 이들은 한국 청소년들의 하루 평균 TV 시청 시간이 3시간이라 주장하고, 다른 이들은 공부하느라 시청 시간이 3시간이 안될 것이라 주장하고 있다. 어느 편이 맞는지 알아보기 위하여 단순임의 추출한 100명을 조사한 결과 평균이 2.75시간이었다. TV 시청 시간은 정규분포를 하며 분산은 과거 조사에서 1로 알려져 있다.\n\n\n# 가설검정을 위한 변수 설정(양측검정)\nmu0 &lt;- 3 # 귀무 가설의 평균\nxbar &lt;- 2.75 # 표본의 평균\nsigma &lt;- 1 # 모집단의 분산 (표준편차)\nn &lt;- 100 # 표본의 크기\n\nz &lt;- (xbar - mu0) / (sigma / sqrt(n))  # z-검정 수행\np_value &lt;- 2 * pnorm(-abs(z))  # p-값 계산\nz\n\n[1] -2.5\n\np_value  # 결과 출력\n\n[1] 0.01241933\n\n# p값이 σ=0.05보다 작으므로 귀무가설을 기각하게 되어 \n# 청소년들의 하루 평균 TV 시청 시간이 3시간이라고 할 수 없다.\n\n\n\n\\(\\sigma\\)를 모르는 경우\n\n건강한 성인의 콜레스테롤 수치는 220 이하라고 한다. 콜레스테롤이 높은 사람은 심장마비가 발생할 가능성이 더 높은 것으로 알려져 있다. 이를 확인하기 위해 30명의 심장마비 환자를 조사하여 평균 콜레스테롤이 231이며, 표준편차가 20인 것을 알아냈다. 심장마비 환자들의 평균 콜레스테롤은 정상인의 평균 콜레스테롤보다 높은 수치라고 결론내릴 수 있는가?\n\n\nt_test &lt;- function(alpha, mu0, xbar, s, n){\n    se = s/sqrt(n)\n    t = (xbar-mu0)/se\n    cr = qt(1-alpha, df=n-1)\n    p_value = pt(t, df= n-1, lower.tail=F)\n    if(p_value &lt; alpha) result = \"Reject H0\" else result = \"Accept H0\"\n    \n    print(paste(\"Hypothesis testing for H0: mu = \",mu0,\"or Ha: mu &gt; \",mu0))\n    print(c(paste(\"t Statistics is \", round(t,4), \n          \"and Critical Region is \", round(cr, 4)), paste(\"p-value =\", round(p_value,4))))\n    print(paste(result, \"at significance level\", alpha))\n}\n\nt_test(0.05, 220, 231, 20, 30)  \n\n[1] \"Hypothesis testing for H0: mu =  220 or Ha: mu &gt;  220\"\n[1] \"t Statistics is  3.0125 and Critical Region is  1.6991\"\n[2] \"p-value = 0.0027\"                                      \n[1] \"Reject H0 at significance level 0.05\"\n\n# p값이 σ=0.05보다 작으므로 귀무가설을 기각하게 되어 \n# 심장마비 환자들의 평균 콜레스테롤은 정상의 평균 콜레스테롤보다 높다고 결론을 내린다.\n\n\n\nMASS 라이브러리에 있는 Cars93의 연료탱크용량의 모평균 추정하기(t검정)\n\nlibrary(MASS)\nfuel = Cars93$Fuel.tank.capacity\n\n## mu of fuel is 16.\n# H0: mu = 16\n# H1: mu is not 16 (양측검정)\n\n## normality test\nhist(fuel)  # 좌우대칭인지 대략적으로 살펴보기\n\n\n\n\n\n\n\nshapiro.test(fuel)  # alpha=0.05 &gt; p ==&gt; H0 reject인데, p=0.287이므로 정규분포를 따름\n\n\n    Shapiro-Wilk normality test\n\ndata:  fuel\nW = 0.98341, p-value = 0.287\n\n## 1. hypothesis H0 : mu=16  H1 : mu is not 16\n## 2. alpha = 0/05\n## 3. test statistic = barX ~ N()\n## 4. p\n## 5. alpha &gt; p ==&gt; H0 reject\n\nt.test(fuel, mu=16, alternative = 'two.sided')  # p=0.5372 &gt; 0.5이므로 H0를 기각할 수 없고, 즉 연료탱크 용량의 평균은 16이라 할 수 있음\n\n\n    One Sample t-test\n\ndata:  fuel\nt = 1.9541, df = 92, p-value = 0.05372\nalternative hypothesis: true mean is not equal to 16\n95 percent confidence interval:\n 15.98914 17.33989\nsample estimates:\nmean of x \n 16.66452 \n\nt.test(fuel, mu=16, alternative = 'greater')  # p=0.02686 &lt; 0.5이므로 H0를 기각하고, 즉 연료탱크 용량의 평균은 16보다 큼(단측검정이 양측검정보다 귀무가설을 기각하기 쉬움)\n\n\n    One Sample t-test\n\ndata:  fuel\nt = 1.9541, df = 92, p-value = 0.02686\nalternative hypothesis: true mean is greater than 16\n95 percent confidence interval:\n 16.09949      Inf\nsample estimates:\nmean of x \n 16.66452 \n\nt.test(fuel, mu=16, alternative = 'less')  # p=0.9731 &gt; 0.5이므로 H0를 채택한다. 즉 연료탱크 용량의 평균은 16보다 작지 않음\n\n\n    One Sample t-test\n\ndata:  fuel\nt = 1.9541, df = 92, p-value = 0.9731\nalternative hypothesis: true mean is less than 16\n95 percent confidence interval:\n     -Inf 17.22955\nsample estimates:\nmean of x \n 16.66452 \n\n\n\n\nMASS 라이브러리에 있는 Cars93의 가격의 모평균 추정하기(비모수적 방법)\n\nlibrary(MASS)\nprice = Cars93$Price\n\n## mu of fuel is 16.\n# H0: mu = 16\n# H1: mu is not 16 (양측검정)\n\n## normality test\nhist(price)  # 좌우대칭인지 대략적으로 살펴보기\n\n\n\n\n\n\n\nshapiro.test(price)  # alpha &gt; p ==&gt; H0 reject인데, p=0.0000004235이므로 정규분포를 따른다고 할 수 없음\n\n\n    Shapiro-Wilk normality test\n\ndata:  price\nW = 0.88051, p-value = 4.235e-07\n\n## Cars93$Price, nor normal dist~, n&gt;30이면 중심극한정리에 의해 표본평균의 정규성을 활용하여 검정\n\n## n&lt;30(소표본) ==&gt; non-parametric test of one sample(비모수적 방법)\n# 1. hypothesis H0 : mu=19  H1 : mu is not 19\n# 2. alpha = 0/05\n\nwilcox.test(price, mu=19, alternative = 'two.sided') # 윌콕슨의 부호순위검정, alpha=0.05 &gt; p ==&gt; H0 reject인데, p=0.4701이므로 price의 평균은 19라 할 수 있음\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  price\nV = 1953, p-value = 0.4701\nalternative hypothesis: true location is not equal to 19\n\nwilcox.test(price, mu=19, alternative = 'greater') # p=0.7662이므로 price의 평균은 19보다 크지 않음\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  price\nV = 1953, p-value = 0.7662\nalternative hypothesis: true location is greater than 19\n\nwilcox.test(price, mu=19, alternative = 'less') # p=0.235이므로 price의 평균은 19보다 작지 않음\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  price\nV = 1953, p-value = 0.235\nalternative hypothesis: true location is less than 19"
  },
  {
    "objectID": "posts/10/index.html#모비율-p에-대한-검정",
    "href": "posts/10/index.html#모비율-p에-대한-검정",
    "title": "10. 하나의 모집단에 대한 가설검정(평균)",
    "section": "모비율 \\(p\\)에 대한 검정",
    "text": "모비율 \\(p\\)에 대한 검정\n\n경기도 어떤 지역에 새로운 지역 신문을 발행하기 위해서는 구독률이 12%를 넘어야 광고주를 안정적으로 확보할 수 있다고 한다. 새로운 지역 신문의 발행을 고심하는 한 신문사에서는 400명의 주민을 대상으로 만약 신문이 발행되면 구독을 할 것인지를 물어서 58명이 구독할 것이라는 답을 얻었다. 새로운 지역 신문의 구독률이 12%를 초과할 것이라고 결론내릴 수 있는가?(유의수준 5% 사용)\n\n\nprop_test &lt;- function(alpha, p, N, n){\n    phat = n/N\n    se = sqrt((p*(1-p))/N)\n    z = (phat - p) / se\n    cr = qnorm(1-alpha)\n    p_value = pnorm(z, lower.tail=F)\n    if(p_value &lt; alpha) result = \"Reject H0\" else result = \"Accept H0\"\n    \n    print(paste(\"Hypothesis testing for H0: mu = \",p,\"or Ha: mu &gt; \",p))\n    print(c(paste(\"p_hat is \", round(phat,5), \"/ Z Statistics is \", round(z,4),\n           \"and Critical Region is \", round(cr, 4)), paste(\"p-value =\", round(p_value,4))))\n    print(paste(result, \"at significance level\", alpha))\n}\n\nprop_test(0.05, 0.12, 400,58)\n\n[1] \"Hypothesis testing for H0: mu =  0.12 or Ha: mu &gt;  0.12\"\n[1] \"p_hat is  0.145 / Z Statistics is  1.5386 and Critical Region is  1.6449\"\n[2] \"p-value = 0.0619\"                                                        \n[1] \"Accept H0 at significance level 0.05\"\n\n# p값이 유의수준 σ=0.05를 초과하므로 귀무가설을 기각하지 못한다.\n# 새로운 지역 신문은 구독률이 12%를 초과할 것이라 결론내릴 수 없다.\n\n\nK후보가 소속된 정당에서 K후보 지지율이 0.3인지를 알아보고자 한다. 실제 표본을 1000명을 뽑았을 때, 250명이 K후보를 지지했다고 하자.\n\n\n# Ho: p=p0,  p=0.3 ?  n= 1000,  X= 250,  approval rating of mr.K =0.3\n## alpha = 0.05\n## 1) binom.test()\nn=1000\nx=250\np0=0.3\nbinom.test(x, n, p=p0, alternative = 'two.sided') # HO reject... p0 is not 0.3 (0.3보다 더 작다고 할 수 있다.)\n\n\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 250, number of trials = 1000, p-value = 0.0004876\nalternative hypothesis: true probability of success is not equal to 0.3\n95 percent confidence interval:\n 0.2234304 0.2780500\nsample estimates:\nprobability of success \n                  0.25 \n\n\n\n## 2) approximate normal distribution\nprop.test(x, n, p=p0, alternative = 'two.sided', correct = T) # correct=T(연속성 수정을 함)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability p0\nX-squared = 11.668, df = 1, p-value = 0.0006359\nalternative hypothesis: true p is not equal to 0.3\n95 percent confidence interval:\n 0.2236728 0.2782761\nsample estimates:\n   p \n0.25 \n\nprop.test(x, n, p=p0, alternative = 'two.sided', correct = F) # correct=F\n\n\n    1-sample proportions test without continuity correction\n\ndata:  x out of n, null probability p0\nX-squared = 11.905, df = 1, p-value = 0.0005599\nalternative hypothesis: true p is not equal to 0.3\n95 percent confidence interval:\n 0.2241531 0.2777603\nsample estimates:\n   p \n0.25 \n\n\n\n과거 추석때 귀향률이 20%다. 최근 500명 중 귀향하려는 사람이 79일때 귀향률이 과거보다 줄어들었는지를 알아보고자 한다.\n\n\n# p0=0.2,  n=500, x=79  H0: p = 0.2  H1 : p&lt;0.2\n## 1) binomial distribution\nbinom.test(79, 500, 0.2, alternative = 'less')\n\n\n    Exact binomial test\n\ndata:  79 and 500\nnumber of successes = 79, number of trials = 500, p-value = 0.009506\nalternative hypothesis: true probability of success is less than 0.2\n95 percent confidence interval:\n 0.0000000 0.1873274\nsample estimates:\nprobability of success \n                 0.158 \n\n## 2) normal distribution with continuity correction\nprop.test(79, 500, 0.2, alternative = 'less', correct=T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  79 out of 500, null probability 0.2\nX-squared = 5.2531, df = 1, p-value = 0.01095\nalternative hypothesis: true p is less than 0.2\n95 percent confidence interval:\n 0.0000000 0.1877246\nsample estimates:\n    p \n0.158 \n\n\n\n‘Cars93’ 데이터셋에서 변속기 사용 비율이 0.75인지 검정하기\n\n\nlibrary(MASS)\nstr(Cars93)\n\n'data.frame':   93 obs. of  27 variables:\n $ Manufacturer      : Factor w/ 32 levels \"Acura\",\"Audi\",..: 1 1 2 2 3 4 4 4 4 5 ...\n $ Model             : Factor w/ 93 levels \"100\",\"190E\",\"240\",..: 49 56 9 1 6 24 54 74 73 35 ...\n $ Type              : Factor w/ 6 levels \"Compact\",\"Large\",..: 4 3 1 3 3 3 2 2 3 2 ...\n $ Min.Price         : num  12.9 29.2 25.9 30.8 23.7 14.2 19.9 22.6 26.3 33 ...\n $ Price             : num  15.9 33.9 29.1 37.7 30 15.7 20.8 23.7 26.3 34.7 ...\n $ Max.Price         : num  18.8 38.7 32.3 44.6 36.2 17.3 21.7 24.9 26.3 36.3 ...\n $ MPG.city          : int  25 18 20 19 22 22 19 16 19 16 ...\n $ MPG.highway       : int  31 25 26 26 30 31 28 25 27 25 ...\n $ AirBags           : Factor w/ 3 levels \"Driver & Passenger\",..: 3 1 2 1 2 2 2 2 2 2 ...\n $ DriveTrain        : Factor w/ 3 levels \"4WD\",\"Front\",..: 2 2 2 2 3 2 2 3 2 2 ...\n $ Cylinders         : Factor w/ 6 levels \"3\",\"4\",\"5\",\"6\",..: 2 4 4 4 2 2 4 4 4 5 ...\n $ EngineSize        : num  1.8 3.2 2.8 2.8 3.5 2.2 3.8 5.7 3.8 4.9 ...\n $ Horsepower        : int  140 200 172 172 208 110 170 180 170 200 ...\n $ RPM               : int  6300 5500 5500 5500 5700 5200 4800 4000 4800 4100 ...\n $ Rev.per.mile      : int  2890 2335 2280 2535 2545 2565 1570 1320 1690 1510 ...\n $ Man.trans.avail   : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 1 1 1 1 ...\n $ Fuel.tank.capacity: num  13.2 18 16.9 21.1 21.1 16.4 18 23 18.8 18 ...\n $ Passengers        : int  5 5 5 6 4 6 6 6 5 6 ...\n $ Length            : int  177 195 180 193 186 189 200 216 198 206 ...\n $ Wheelbase         : int  102 115 102 106 109 105 111 116 108 114 ...\n $ Width             : int  68 71 67 70 69 69 74 78 73 73 ...\n $ Turn.circle       : int  37 38 37 37 39 41 42 45 41 43 ...\n $ Rear.seat.room    : num  26.5 30 28 31 27 28 30.5 30.5 26.5 35 ...\n $ Luggage.room      : int  11 15 14 17 13 16 17 21 14 18 ...\n $ Weight            : int  2705 3560 3375 3405 3640 2880 3470 4105 3495 3620 ...\n $ Origin            : Factor w/ 2 levels \"USA\",\"non-USA\": 2 2 2 2 2 1 1 1 1 1 ...\n $ Make              : Factor w/ 93 levels \"Acura Integra\",..: 1 2 4 3 5 6 7 9 8 10 ...\n\n## Man.trans.avail \n\ntable(Cars93$Man.trans.avail) #표본수가 크니 prop.test 이용\n\n\n No Yes \n 32  61 \n\n## HO : p = 0.75  proportion of Man.trans.avail =0.75\n## H1 : p is not 0.75\nprop.test(x=61, n=93, p=0.75, alternative = 'two.sided', correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  61 out of 93, null probability 0.75\nX-squared = 3.9032, df = 1, p-value = 0.04819\nalternative hypothesis: true p is not equal to 0.75\n95 percent confidence interval:\n 0.5494157 0.7493692\nsample estimates:\n       p \n0.655914 \n\n## H1 : p &lt; 0.75\nprop.test(x=61, n=93, p=0.75, alternative = 'less', correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  61 out of 93, null probability 0.75\nX-squared = 3.9032, df = 1, p-value = 0.0241\nalternative hypothesis: true p is less than 0.75\n95 percent confidence interval:\n 0.0000000 0.7364348\nsample estimates:\n       p \n0.655914 \n\n## H1 : p &gt; 0.75\nprop.test(x=61, n=93, p=0.75, alternative = 'greater', correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  61 out of 93, null probability 0.75\nX-squared = 3.9032, df = 1, p-value = 0.9759\nalternative hypothesis: true p is greater than 0.75\n95 percent confidence interval:\n 0.5660022 1.0000000\nsample estimates:\n       p \n0.655914"
  },
  {
    "objectID": "posts/12/index.html",
    "href": "posts/12/index.html",
    "title": "12. 카이제곱검정",
    "section": "",
    "text": "날씨가 맑고, 흐리고, 눈 오고, 비 오고 등으로 조사되고, 직업이 전문직, 사무직, 노무직 등으로 분류되는 등 숫자가 아닌 범주형 형태로 얻어지는 자료를 분석하는 기본적인 도구로 카이제곱통계를 이용한다. 표본으로 얻어진 범주의 분포들이 주어진 분포에 부합하는지 알아보려는 적합도 검정, 범주형 변수들끼리의 독립성 여부를 알아보려는 독립성 검정, 두 개 이상의 집단에서의 분포가 같은지 알아보려는 동질성 검정을 할 수 있다."
  },
  {
    "objectID": "posts/12/index.html#적합도-검정",
    "href": "posts/12/index.html#적합도-검정",
    "title": "12. 카이제곱검정",
    "section": "적합도 검정",
    "text": "적합도 검정\n\n특정 분포를 따르는지 알아보는 방법을 적합도 검정(Goodness of fit test)이라고 한다.\n정규분포 적합도 검정으로는 콜모그로프-스미르노프 검정, 샤피로 윌크 검정, 앤더슨 달링 검정이 주로 사용된다. 세 가지 방법은 결과가 거의 비슷하게 나온다.\n\n\nMASS 라이브러리에 있는 Cars93의 price라는 자료가 정규분포인지 위 3가지 방법으로 확인해보자.\n\nKolmogorov-smirnov test (KS test)\n\n\nlibrary(MASS)\nprice = Cars93$Price\n\n# alpha = 0.05 &gt; p ==&gt; H0 reject\nks.test(price, \"pnorm\")  # 귀무가설은 '변수 X가 정규분포를 따른다'이고, 대립가설은 '정규분포를 따르지 않는다'이다.\n\nWarning in ks.test.default(price, \"pnorm\"): ties should not be present for the\nKolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  price\nD = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n# 검정 결과, p &lt; 2.2e-16 = 0.00000000000000022 이므로 귀무가설을 기각한다.\n\n\nShapiro-wilk test\n\n\n# alpha = 0.05 &gt; p ==&gt; H0 reject\nshapiro.test(price)  # 귀무가설은 '변수 X가 정규분포를 따른다'이고, 대립가설은 '정규분포를 따르지 않는다'이다.\n\n\n    Shapiro-Wilk normality test\n\ndata:  price\nW = 0.88051, p-value = 4.235e-07\n\n# 검정 결과, p = 4.235e-07 = 0.0000004235 이므로 귀무가설을 기각한다.\n\n\nAnderson darling test\n\n\n# install.packages('nortest')  # nortest 라이브러리가 없으면 설치하기\nlibrary(nortest)\nad.test(price)\n\n\n    Anderson-Darling normality test\n\ndata:  price\nA = 2.6704, p-value = 8.797e-07\n\n# alpha = 0.05 &gt; p ==&gt; H0 reject\n# 검정 결과, p = 8.797e-07 = 0.0000008797 이므로 귀무가설을 기각한다.\n\n\n\nMASS 라이브러리의 Cars93의 price라는 자료가 다른 분포(t, F, chisq)를 따르는지 확인해보자.\n\nKolmogorov-smirnov test (KS test)\n\n\nks.test(price, 'pnorm')  # 정규분포를 따르는지 검정하기\n\nWarning in ks.test.default(price, \"pnorm\"): ties should not be present for the\nKolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  price\nD = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\nks.test(price, 'pt', df=30)  # 자유도가 30인 t분포를 따르는지 검정하기\n\nWarning in ks.test.default(price, \"pt\", df = 30): ties should not be present\nfor the Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  price\nD = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\nks.test(price, 'pf', df1=30, df2=40)  # 자유도가 30, 40인 F분포를 따르는지 검정하기\n\nWarning in ks.test.default(price, \"pf\", df1 = 30, df2 = 40): ties should not be\npresent for the Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  price\nD = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\nks.test(price, 'pchisq', df=30)  # 자유도가 30인 카이제곱분포를 따르는지 검정하기\n\nWarning in ks.test.default(price, \"pchisq\", df = 30): ties should not be\npresent for the Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  price\nD = 0.60067, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "posts/12/index.html#독립성-검정",
    "href": "posts/12/index.html#독립성-검정",
    "title": "12. 카이제곱검정",
    "section": "독립성 검정",
    "text": "독립성 검정\n혈액형과 성별, 학년과 학점 등 두 개의 범주형 변수에 대해서 먼저 두 변수 간의 관련성이 주된 관심이 된다. 즉 두 변수가 독립인지 검정하게 되는데, 이를 독립성 검정이라고 한다.\n\n전공과 학점 분할표\n\n130명의 대학원생을 대상으로 학부학점과 대학원전공을 조사한 결과가 다음 표와 같다. 대학원전공과 학부학점 간의 관련이 있는지 검정하시오. \n\n\ny &lt;- matrix(c(30, 13, 16,\n               8, 16,  7,\n              12, 11, 17), nrow=3, ncol=3, byrow=T)\ndimnames(y) &lt;- list(c(\"3.5-4.5\",\"2.5-3.5\",\"1.5-2.5\"), c(\"사회과학\",\"자연과학\",\"공학\"))\ny\n\n        사회과학 자연과학 공학\n3.5-4.5       30       13   16\n2.5-3.5        8       16    7\n1.5-2.5       12       11   17\n\nchisq.test(y, correct=F)\n\n\n    Pearson's Chi-squared test\n\ndata:  y\nX-squared = 13.088, df = 4, p-value = 0.01085\n\n# 학부성적과 대학원전공이 독립이라는 가정을 기각한다.\n\nx &lt;- chisq.test(y, correct=F)  # 독립된 가정에서의 기댓값 출력\nx$expected\n\n        사회과학  자연과학      공학\n3.5-4.5 22.69231 18.153846 18.153846\n2.5-3.5 11.92308  9.538462  9.538462\n1.5-2.5 15.38462 12.307692 12.307692\n\nx$residuals^2  # 독립된 가정에서의 카이제곱값\n\n         사회과학  자연과학      공학\n3.5-4.5 2.3533246 1.4631682 0.2555411\n2.5-3.5 1.2908189 4.3771712 0.6755583\n1.5-2.5 0.7446154 0.1389423 1.7889423"
  },
  {
    "objectID": "posts/12/index.html#동질성-검정",
    "href": "posts/12/index.html#동질성-검정",
    "title": "12. 카이제곱검정",
    "section": "동질성 검정",
    "text": "동질성 검정\n\nHomogeneity Test 동질성 검정\n\n남녀별 대학생의 전공자수를 조사하였다. 남학생은 45명 중 독문학, 불문학을 각각 30, 15명이 전공하고 있었고, 125명의 여학생은 각각 50, 75명이 전공하고 있었다. 남녀별 전공의 분포가 같은지 알아보고자 한다. 즉 남학생 집단에서의 전공에 대한 이항분포의 두 범주에 대한 확률과 여학생 집단에서의 전공비율이 각각 같은지 검정하시오.\n\n\nhomo &lt;- matrix(c(30, 15, 50, 75), nrow=2, ncol=2, byrow=T)\ndimnames(homo) &lt;- list(c(\"남자\",\"여자\"), c(\"독문학\",\"불문학\"))\nn.marginal &lt;- rowSums(homo)\np.obs &lt;- homo / n.marginal\np.exp &lt;- colSums(homo)/350\nhomo.exp &lt;- n.marginal%o%p.exp\nhomo.exp\n\n       독문학   불문학\n남자 10.28571 11.57143\n여자 28.57143 32.14286\n\nhomo.chisq &lt;- (homo - homo.exp)^2 / homo.exp\nhomo.chisq\n\n       독문학    불문학\n남자 37.78571  1.015873\n여자 16.07143 57.142857\n\nrowSums(homo.chisq)\n\n    남자     여자 \n38.80159 73.21429 \n\n(xx &lt;- chisq.test(homo))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  homo\nX-squared = 8.4044, df = 1, p-value = 0.003743\n\nxx$expected\n\n       독문학   불문학\n남자 21.17647 23.82353\n여자 58.82353 66.17647\n\nxx$residuals^2  # 독립이라는 가정 하에 기댓값\n\n       독문학   불문학\n남자 3.676471 3.267974\n여자 1.323529 1.176471\n\nsum(xx$residuals^2)  # 독립이라는 가정 하에 카이제곱값\n\n[1] 9.444444\n\n# p값이 유의 수준(0.05)보다 낮으므로, 두 집단의 분포가 같다는 가정을 기각한다.\n\n\nR에서 이러한 검정을 수행하기 위해 prop.test 함수를 사용할 수 있고, 이 함수는 두 또는 그 이상의 비율을 비교하는 데 사용된다.\n\n\n# 남학생과 여학생의 독문학과 불문학 전공자 수\nmale_students &lt;- c(30, 15)   # 남학생 독문학 30명, 불문학 15명\nfemale_students &lt;- c(50, 75) # 여학생 독문학 50명, 불문학 75명\n\n# prop.test를 사용하여 두 집단 간의 비율 차이 검정\nprop.test(x = c(sum(male_students), sum(female_students)), \n          n = c(sum(male_students) + sum(female_students), sum(female_students) + sum(male_students)), \n          alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(sum(male_students), sum(female_students)) out of c(sum(male_students) + sum(female_students), sum(female_students) + sum(male_students))\nX-squared = 73.424, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.5702594 -0.3709171\nsample estimates:\n   prop 1    prop 2 \n0.2647059 0.7352941 \n\n# p값이 유의 수준(0.05)보다 낮으므로, 두 집단의 분포가 같다는 가정을 기각한다."
  },
  {
    "objectID": "posts/14/index.html",
    "href": "posts/14/index.html",
    "title": "14. 다중선형회귀분석",
    "section": "",
    "text": "국민총생산에 영향을 주는 것이 이자율 이외에 수출, 수입, R&D비율, 실업률 등 여러가지일 수 있다. 몸무게를 설명할 수 있는 변수가 키뿐 아니라 하루 칼로리 섭취량과 운동량도 영향을 줄 것이라고 생각한다면 키, 칼로리 섭취량과 운동량을 동시에 고려하여 선형회귀분석을 시도할 수 있다. 이와 같이 둘 이상의 설명변수를 고려하는 것을 다중선형회귀분석이라고 한다."
  },
  {
    "objectID": "posts/14/index.html#다중선형회귀분석",
    "href": "posts/14/index.html#다중선형회귀분석",
    "title": "14. 다중선형회귀분석",
    "section": "다중선형회귀분석",
    "text": "다중선형회귀분석\n\n광고비 및 설비투자와 매출액 연관성 비교 자료\n\n다음 표는 광고비와 설비투자가 매출액에 어떤 영향을 미치는지 보여주고 있다.\n\n\n\n\n\n광고비\n5\n6\n7\n8\n9\n11\n12\n13\n14\n15\n\n\n설비투자\n2\n1.9\n4\n5.6\n6.1\n6.2\n7\n7.2\n8\n9\n\n\n매출액\n16\n19\n18\n20\n24\n26\n30\n32\n31\n34\n\n\n\n\n매출액(\\(Y\\))에 대한 광고비(\\(X_{1}\\))와 설비투자(\\(X_{2}\\))의 다중선형회귀모형을 구하시오.\n\nx =matrix(c(5,6,7,8,9,11,12,13,14,15,2,1.9,4,5.6,6.1,6.2,7,7.2,8,9,16,19,18,20,24,26,30,32,31,34), nrow=10, ncol=3)\ncolnames(x)=c(\"광고비\",\"설비투자\",\"매출액\")\nx=data.frame(x)\nattach(x)\nx\n\n   광고비 설비투자 매출액\n1       5      2.0     16\n2       6      1.9     19\n3       7      4.0     18\n4       8      5.6     20\n5       9      6.1     24\n6      11      6.2     26\n7      12      7.0     30\n8      13      7.2     32\n9      14      8.0     31\n10     15      9.0     34\n\nfit &lt;- lm(매출액 ~ 광고비 + 설비투자)\nfit\n\n\nCall:\nlm(formula = 매출액 ~ 광고비 + 설비투자)\n\nCoefficients:\n(Intercept)       광고비     설비투자  \n      6.099        2.257       -0.643  \n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: 매출액\n          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \n광고비     1 370.95  370.95 232.5153 1.256e-06 ***\n설비투자   1   1.89    1.89   1.1828    0.3128    \nResiduals  7  11.17    1.60                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(fit)\n\n\nCall:\nlm(formula = 매출액 ~ 광고비 + 설비투자)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5477 -0.8391 -0.1286  1.0417  1.5139 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.0985     1.3618   4.478 0.002872 ** \n광고비        2.2567     0.4048   5.575 0.000838 ***\n설비투자     -0.6430     0.5913  -1.088 0.312805    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 7 degrees of freedom\nMultiple R-squared:  0.9709,    Adjusted R-squared:  0.9626 \nF-statistic: 116.8 on 2 and 7 DF,  p-value: 4.195e-06\n\nconfint(fit)\n\n                2.5 %    97.5 %\n(Intercept)  2.878488 9.3186072\n광고비       1.299484 3.2138731\n설비투자    -2.041147 0.7550655\n\nplot(fit$fitted, fit$residuals, xlab=\"예측치 매출액\", ylab=\"잔차\", main=\"잔차그림\")\nabline(0,0)"
  },
  {
    "objectID": "webr.html",
    "href": "webr.html",
    "title": "Web-R",
    "section": "",
    "text": "mtcars는 내장된 데이터입니다. 지금 상태에서 Run Code를 click해 보세요.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "webr.html#쿼토-안에-r",
    "href": "webr.html#쿼토-안에-r",
    "title": "Web-R",
    "section": "",
    "text": "mtcars는 내장된 데이터입니다. 지금 상태에서 Run Code를 click해 보세요.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "webr.html#히스토그램",
    "href": "webr.html#히스토그램",
    "title": "Web-R",
    "section": "히스토그램",
    "text": "히스토그램\n아래 명령어 실행 후 “mtcars”를 입력하여 실행해보세요. 내장된 데이터를 살펴볼 수 있습니다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "webr.html#실습",
    "href": "webr.html#실습",
    "title": "Web-R",
    "section": "실습",
    "text": "실습\n중량과 배기량(disp)를 이용하여 연비를 예측하는 다중회귀모형을 구축하시오.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "posts/16/index.html",
    "href": "posts/16/index.html",
    "title": "16. 상관분석",
    "section": "",
    "text": "상관분석(correlation analysis)은 내용작성 중."
  },
  {
    "objectID": "posts/01/index.html#자료의-분포-모양-살펴보기왜도-첨도",
    "href": "posts/01/index.html#자료의-분포-모양-살펴보기왜도-첨도",
    "title": "01. 기술통계(Descriptive Statistics)",
    "section": "자료의 분포 모양 살펴보기(왜도, 첨도)",
    "text": "자료의 분포 모양 살펴보기(왜도, 첨도)\n\n자료를 만들어서 왜도, 첨도의 값과 비교하여 자료의 분포를 살펴보자.\n\n\nbase = c(1,2,2,3,3,3,4,4,4,4,5,5,5,6,6,7) # 좌우대칭인 자료를 하나 만듦\nhist(base, breaks = (-1:8)) # 범위를 -1부터 8까지 하는 히스토그램을 그림\n\n\n\n\n\n\n\n# install.packages('e1071') # e1071 패키지가 없으면 옆에 주석을 없애고 실행하기\nlibrary(e1071)\n\nWarning: package 'e1071' was built under R version 4.3.3\n\n\n\nAttaching package: 'e1071'\n\n\nThe following objects are masked from 'package:dlookr':\n\n    kurtosis, skewness\n\nskewness(base, na.rm=T)\n\n[1] 0\n\n# skew to the left\nhist(log(base)) # log함수로 왼쪽에 꼬리 즉 이상값이 있고 오른쪽에 자료가 몰려있는 상태의 자료 만들기\n\n\n\n\n\n\n\nskewness((log(base))) # 왼쪽으로 길게 뻗은 경우에는 음수가 나옴을 확인할 수 있음\n\n[1] -0.9539476\n\n# skew to the right\nhist(base**2) # base를 제곱하여 오른쪽으로 치우친 자료를 만듦\n\n\n\n\n\n\n\nskewness((base**2)) # 오른쪽으로 길게 뻗은 경우에는 양수가 나옴을 확인할 수 있음\n\n[1] 0.6713173\n\n# kurtosis\nkurtosis(base) # 음수가 나오면 표준정규분포보다 봉오리가 낮다는 뜻임, 좌우꼬리가 두껍다는 뜻임\n\n[1] -0.9609375\n\nhist(base+(base-4)**3)\n\n\n\n\n\n\n\nkurtosis(base+(base-4)**3) # 양수가 나오면 표준정규분포보다 봉오리가 높다는 뜻임\n\n[1] 1.719828"
  },
  {
    "objectID": "webr.knit.html",
    "href": "webr.knit.html",
    "title": "Web-R",
    "section": "",
    "text": "mtcars는 내장된 데이터입니다. 지금 상태에서 Run Code를 click해 보세요.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "webr.knit.html#쿼토-안에-r",
    "href": "webr.knit.html#쿼토-안에-r",
    "title": "Web-R",
    "section": "",
    "text": "mtcars는 내장된 데이터입니다. 지금 상태에서 Run Code를 click해 보세요.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "webr.knit.html#히스토그램",
    "href": "webr.knit.html#히스토그램",
    "title": "Web-R",
    "section": "히스토그램",
    "text": "히스토그램\n아래 명령어 실행 후 “mtcars”를 입력하여 실행해보세요. 내장된 데이터를 살펴볼 수 있습니다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "webr.knit.html#실습",
    "href": "webr.knit.html#실습",
    "title": "Web-R",
    "section": "실습",
    "text": "실습\n중량과 배기량(disp)를 이용하여 연비를 예측하는 다중회귀모형을 구축하시오.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "posts/08/index.html#t분포",
    "href": "posts/08/index.html#t분포",
    "title": "08. 모집단과 표본, 모평균의 추정",
    "section": "t분포",
    "text": "t분포\n\n표준정규분포 \\(N(0,1)\\)을 따르는 확률변수를 \\(Z\\)라 하자. 독립적이면서 자유도가 \\(v\\)인 카이제곱 분포를 따르는 확률변수를 \\(V\\)라고 할 때, \\(T=\\dfrac{Z}{\\sqrt{V/v}}\\)의 분포를 자유도가 \\(v\\)인 \\(t\\)분포라고 한다. 이때 \\(T \\sim t(v)\\)라고 표시한다.\n\\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_n\\) \\(\\sim N(\\mu, \\sigma^2)\\), \\(\\bar{X} \\sim N\\left(\\mu , \\dfrac{\\sigma^2}{n}\\right)\\) 일때, 표준정규분포로 변환이 가능하며, \\(\\dfrac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\)이다.\n모분산 \\(\\sigma^2\\)을 모를 때, 표본분산 \\(S^2 = \\dfrac{\\sum\\limits_{i=1}^{n}(X_i - \\bar{x})^2}{n-1}\\)으로 모분산의 추정치로 사용한다.\n즉, \\(\\sigma^2\\)을 \\(S^2\\)으로 추정하면, 자유도가 \\(n-1\\)인 \\(t\\)분포를 따르게 되며, 이를 표현하면 \\(\\dfrac{\\bar{X}-\\mu}{S/\\sqrt{n}} \\sim t(n-1)\\)이다. (\\(S\\)의 자유도가 \\(n-1\\)이고, 이것으로부터 \\(t\\)분포가 정의되기 때문에 자유도가 \\(n-1\\)이다.)\n\\(t\\)분포를 따르는 이러한 변환을 스튜던트화(Studentized)라고 하며, \\(t\\)분포를 스튜던트 \\(t\\)분포라고도 한다.\n\n\ndt(0, df=30)  # x~t(30)일때, P(x=0)의 값 계산\n\n[1] 0.3956322\n\ndnorm(0, mean=0, sd=1)  #정규분포 Z~N(0,1)의 값과 비교해보면, t분포에서의 값이 약간 작고 따라서 t분포의 양 끝이 조금 두꺼움을 알 수 있다\n\n[1] 0.3989423\n\npt(0, df=30)  # x~t(30)일때, P(x&lt;0)의 값 계산하면 좌우대칭이라서 0.5임을 알 수 있다\n\n[1] 0.5\n\npt(1, df=30, lower.tail = T)  # P(x&lt;1)의 값 계산\n\n[1] 0.8373457\n\npt(1, df=30, lower.tail = F)  # P(x&gt;1)의 값 계산\n\n[1] 0.1626543\n\npt(1, df=30, lower.tail = T) + pt(1, df=30, lower.tail = F)  # 합이 1임을 확인할 수 있다\n\n[1] 1\n\nqt(0.05, df=30, lower.tail=T)  # 확률이 0.05인 x값\n\n[1] -1.697261\n\nqt(0.025, df=30, lower.tail=T)  # 확률이 0.025인 x값\n\n[1] -2.042272\n\nqt(0.05, df=30, lower.tail=F)  # P(x&gt;x) = 0.05인 x값\n\n[1] 1.697261\n\nqt(0.025, df=30, lower.tail=F)  # P(x&gt;x) = 0.025인 x값\n\n[1] 2.042272\n\nsample = rt(1000, df=30)\nhist(sample)"
  },
  {
    "objectID": "posts/11/index.html#카이제곱-분포chi2분포",
    "href": "posts/11/index.html#카이제곱-분포chi2분포",
    "title": "11. 하나의 모집단에 대한 가설검정(분산)",
    "section": "카이제곱 분포(\\(\\chi^2\\)분포)",
    "text": "카이제곱 분포(\\(\\chi^2\\)분포)\n\nR에서 제공되는 카이제곱 분포와 관련된 함수\ndchisq() : \\(P(X=x)\\)\npchisq() : \\(P(X&lt;x)\\) 또는 \\(P(X \\leq x)\\)\nqchisq() : \\(P(X&lt;x)=p\\)가 되는 \\(x\\)값\nrchisq() : 주어진 카이제곱에서 난수를 뽑는 방법\n\n\n# chi_squares distribution\ndchisq(1, df=4) # 자유도가 4인 분포에서 1에서의 확률\n\n[1] 0.1516327\n\npchisq(1, df=4, lower.tail = TRUE) # 자유도가 4인 분포에서 1보다 작을 확률(옵션 안줄때의 기본값)\n\n[1] 0.09020401\n\npchisq(1, df=4, lower.tail = FALSE) # 자유도가 4인 분포에서 1보다 클 확률\n\n[1] 0.909796\n\nqchisq(0.05, df=4, lower.tail = TRUE) # 자유도가 4인 분포에서 확률이 0.05일때의 x값\n\n[1] 0.710723\n\nqchisq(0.05, df=4, lower.tail = FALSE) # 자유도가 4인 분포에서 오른쪽 면적이 0.05일때의 x값\n\n[1] 9.487729\n\nsample = rchisq(1000, df=4) # 자유도가 4인 카이제곱분포에서 1000개의 수를 임의로 뽑기\nhist(sample) # 히스토그램 그리기"
  },
  {
    "objectID": "posts/08/index.html#표본-추출",
    "href": "posts/08/index.html#표본-추출",
    "title": "08. 모집단과 표본, 모평균의 추정",
    "section": "표본 추출",
    "text": "표본 추출\n\n복원추출(Sampling with replacement)\n\nset.seed(12345)  # 이 명령어는 난수생성기의 초기값을 설정하며, 아무 양수를 넣고 실행 후 아래 명령어를 실행하면, 매번 같은 수가 나옴\nsample(x=1:10, size=5, replace=T)  # 1부터 10의 수에서 크기가 5인 표본을 복원추출로 뽑음\n\n[1]  3 10  8 10  8\n\n\n\n\n비복원추출(Sampling without replacement)\n\nset.seed(123)  # 이 명령어를 실행한 후 아래 명령어를 실행하면, 매번 같은 수가 나옴\nsample(x=1:10, size=5, replace=F)  # 1부터 10의 수에서 크기가 5인 표본을 비복원추출로 뽑음\n\n[1]  3 10  2  8  6\n\n\n\n\n표본추출예제\n\n\\(1\\)부터 \\(45\\)까지 자연수 중 서로 다른 수 \\(6\\)개를 추출하시오.\n\n\nsample(1:45, 6, replace=F) \n\n[1] 43 37 14 25 26 27\n\n\n\n등이 나올 확률이 \\(0.4\\)인 윷짝 \\(4\\)개를 한 번 던지는 모의실험을 하시오.\n\n\nsample(c(1:0), 4, replace=T, prob=c(0.4, 0.6)) # 등을 1, 배를 0으로 간주함 \n\n[1] 0 0 0 1\n\n\n\n표준정규분포를 따르는 모집단에서 크기가 \\(10\\)인 표본을 임의로 추출하시오.\n\n\nrnorm(10, 0, 1)\n\n [1]  1.2240818  0.3598138  0.4007715  0.1106827 -0.5558411  1.7869131\n [7]  0.4978505 -1.9666172  0.7013559 -0.4727914\n\n\n\n한 개의 주사위를 \\(1000\\)번 던질 때, \\(1\\)의 눈이 나오는 횟수를 구하는 모의실험을 하시오.\n\n\ncount &lt;- 0\nn &lt;- 1\nwhile(n&lt;=1000){\n  s &lt;- sample(1:6, 1)\n  if(s==1){count &lt;- count+1; n &lt;- n+1}\n  else{n &lt;- n+1}\n}\ncount\n\n[1] 170"
  },
  {
    "objectID": "posts/13/index.html#f-분포x-sim-fdf1-df2",
    "href": "posts/13/index.html#f-분포x-sim-fdf1-df2",
    "title": "13. 분산분석",
    "section": "F 분포(\\(X \\sim F(df1, df2)\\))",
    "text": "F 분포(\\(X \\sim F(df1, df2)\\))\n\n\\(V_1\\), \\(V_2\\)가 서로 독립이고, 자유도가 각각 \\(d_1\\)과 \\(d_2\\)인 카이제곱(\\(\\chi^2\\)) 분포를 따를 때, \\(F= \\dfrac{V_1 / d_1}{V_2 /d_2}\\)의 분포를 자유도가 \\((d_1 , d_2 )\\)인 \\(F\\)분포라 한다.\nF분포는 분자의 자유도 \\(d_1\\)과 분모의 자유도 \\(d_2\\) 두 개의 자유도에 의해 분포가 유일하게 결정된다. 아래 그림과 같이 분자와 분모의 자유도가 모두 커지면 좌우대칭인 분포를 따르는 형태가 나온다.\n\n \n\n* 그림 출처: 통계교육원, R을 활용한 통계분석\n\n\n\nR에서 제공되는 F분포와 관련된 함수\ndf() : \\(P(X=x)\\)\npf() : \\(P(X&lt;x)\\) 또는 \\(P(X \\leq x)\\)\nqf() : \\(P(X&lt;x)=p\\)가 되는 \\(x\\)값\nrf() : 주어진 카이제곱에서 난수를 뽑는 방법\n\n\n# chi_squares distribution\ndf(1, df1=5, df2=2) # 분자의 자유도가 5, 분모의 자유도가 2인 F분포에서 X=1에서의 확률\n\n[1] 0.3080008\n\npf(1, df1=5, df2=2, lower.tail = T) # 분자의 자유도가 5, 분모의 자유도가 2인 F분포에서 X가 1보다 작을 확률\n\n[1] 0.4312012\n\npf(1, df1=5, df2=2, lower.tail = F) # 분자의 자유도가 5, 분모의 자유도가 2인 F분포에서 X가 1보다 클 확률\n\n[1] 0.5687988\n\nqf(0.05, df1=5, df2=2, lower.tail = T) # 분자의 자유도가 5, 분모의 자유도가 2인 F분포에서 확률이 0.05일때의 x값\n\n[1] 0.1728269\n\nqf(0.95, df1=5, df2=2, lower.tail = T) # 분자의 자유도가 5, 분모의 자유도가 2인 F분포에서 확률이 0.95일때의 x값\n\n[1] 19.29641\n\nsample = rf(1000, df1=5, df2=2) # 자유도가 4인 카이제곱분포에서 1000개의 수를 임의로 뽑기\nhist(sample, breaks = c(-1:round(max(sample))+1), xlim=c(-1,15)) # 히스토그램 그리기"
  },
  {
    "objectID": "posts/12/index.html#중심극한정리central-limit-theory",
    "href": "posts/12/index.html#중심극한정리central-limit-theory",
    "title": "12. 카이제곱검정",
    "section": "중심극한정리(Central limit theory)",
    "text": "중심극한정리(Central limit theory)\n중심극한정리란 변수 \\(X\\)가 정규분포를 따르지는 않지만, 만약 모집단으로부터 충분한 크기의 표본을 뽑았다면(25~30개 이상) 통계량의 분포인 표본분포가 정규분포에 근사한다는 이론이다. 따라서 표본을 충분히 크게 뽑는다면 모집단이 정규분포를 따르지 않아도 중심극한정리에 의해 표본분포가 정규분포를 따른다."
  },
  {
    "objectID": "posts/12/index.html#카이제곱검정을-활용한-적합도-검정",
    "href": "posts/12/index.html#카이제곱검정을-활용한-적합도-검정",
    "title": "12. 카이제곱검정",
    "section": "카이제곱검정을 활용한 적합도 검정",
    "text": "카이제곱검정을 활용한 적합도 검정\n코로나 접종을 받은 사람의 비율이 20대, 30대, 40대 연령에서 동일한지 \\(\\dfrac{1}{3}\\), \\(\\dfrac{1}{3}\\), \\(\\dfrac{1}{3}\\)로 검정하거나, 중간시험에서 A, B, C학점을 받은 비율이 동일한지 \\(\\dfrac{1}{3}\\), \\(\\dfrac{1}{3}\\), \\(\\dfrac{1}{3}\\)로 검정, 또는 \\(20\\%\\), \\(50\\%\\), \\(30\\%\\)인지 검정하는 것을 단일 분류변수에 대한 검정이라고 한다. 범주형 변수의 분포에 대한 적합성 검정은 카이제곱 분포를 이용할 수 있다.\n\n독감접종률 적합도 검정\n\n독감을 접종한 사람들의 거주지역에 따라 독감 접종률의 차이가 있는지 검정해보자.\n\n\n## goodness-of-fit test for one sample\n# area1 area2 area3  \n# H0: p1 = p2 = p3 ==&gt; HO: p1=p2=p3=1/3\n\n# o1 = 19, o2=14, o3 = 13,  n=46\n\no1=19 ; o2=14 ; o3=13\nn = o1 + o2 + o3\nx=c(o1, o2, o3)\n\nchisq.test(x, p=c(1/3, 1/3, 1/3) ) #검정결과 귀무가설 채택, 세 지역의 접종률은 동일함\n\n\n    Chi-squared test for given probabilities\n\ndata:  x\nX-squared = 1.3478, df = 2, p-value = 0.5097\n\nchisq.test(x, p=c(1/4, 2/4, 1/4 )) #귀무가설을 1/4, 2/4, 1/4로 바꿀 수도 있음. 검정결과 귀무가설 기각됨\n\n\n    Chi-squared test for given probabilities\n\ndata:  x\nX-squared = 8.6087, df = 2, p-value = 0.01351\n\n\n\n\n시장점유율 적합도 검정\n\n다음은 제품 4개에 대해 알려진 시장점유율과 실제 500명을 대상으로 사용하고 있는 제품의 사용자수를 나타낸 것이다.  알려진 시장점유율과 관측값이 차이가 나는지 검정하시오.\n\n\n# chisq.test(x=범주별 관측도수, p=범주별 비율, rescale.p, correct)\n# rescale.p: p의 합이 1이 아닐때, 1이 되도록 스케일을 조정할지 결정, F가 기본값\n# correct: 연속성 수정 여부, T가 기본값\nx &lt;- c(110, 195,  47, 148)\np &lt;- c(0.2, 0.4, 0.1, 0.3)\n\nchisq.test(x, p=p)\n\n\n    Chi-squared test for given probabilities\n\ndata:  x\nX-squared = 1.3317, df = 3, p-value = 0.7216\n\n# 검정 통계량이 1.3317이고, p값이 0.7216이다.\n# p값이 0.05(5%) 이상이므로 귀무가설을 기각할 수 없고,\n# 관측된 데이터는 기대되는 빈도와 통계적으로 유의미한 차이가 없다.\n\n\n\n주사위- 균등분포 적합도 검정\n\n주사위의 각 면이 나올 확률이 같은지 알아보기 위해 1200번을 던졌더니 210, 180, 185, 220, 195, 210이 나왔다. 이에 대해 이 주사위의 각 면이 나올 확률이 동일한지 검정하시오.\n\n\np &lt;- rep(1/6, 6)\nx &lt;- c(210, 180, 185, 220, 195, 210)\n\nchisq.test(x, p=p)\n\n\n    Chi-squared test for given probabilities\n\ndata:  x\nX-squared = 6.25, df = 5, p-value = 0.2826\n\n# 검정 통계량이 6.25이고, p값이 0.2826이다.\n# p값이 0.05(5%) 이상이므로 귀무가설을 기각할 수 없고,\n# 주사위의 각 면이 나올 확률이 다르지 않으며, 각 면이 200번으로 나오지 않은 것은 확률적인 우연 변동이다.\n\n\n\nAS접수건수- 포아송분포 적합도 검정\n\n다음은 하루에 발생한 의료영상기기 AS접수건수이다. 이 자료로부터 하루에 발생하는 AS요청건수가 포아송분포를 따른다고 가정하는 것이 적합한지 적합도 검정을 하시오. \n\n\nobserved &lt;- c(94, 67, 33, 3, 3)  # 관측된 빈도\ntotal &lt;- sum(observed)  # 총 건수\nlambda &lt;- sum(0:4 * observed) / total  # 포아송 분포의 평균 발생빈도 계산(하루 평균)\nexpected &lt;- dpois(0:4, lambda) * total  # 기대 빈도 계산\nexpected[5] &lt;- total - sum(expected[1:4])  # 마지막 카테고리는 4 이상의 모든 값 포함\nchisq_test &lt;- chisq.test(x = observed, p = expected / total)  # 카이제곱 적합도 검정 수행\n\nWarning in chisq.test(x = observed, p = expected/total): Chi-squared\napproximation may be incorrect\n\nprint(chisq_test)  # 검정 결과 출력\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 4.9623, df = 4, p-value = 0.2912\n\n# 각 범주의 기대빈도가 5보다 작은 경우에는 잘 맞지 않기에 경고메시지가 뜬다.\n\n기대빈도가 낮은 주위의 범주들을 하나의 범주로 묶어서 붕괴된(collapsed) 빈도표를 만들어 카이제곱 적합도 검정을 수행한다. \n\nx &lt;- 0:4\nObs &lt;- c(94, 67, 33, 3, 3)\n(n &lt;- sum(Obs))\n\n[1] 200\n\n(sample.mean &lt;- drop(x%*%Obs/sum(Obs)))  # %*% := inner product, drop := as scalar\n\n[1] 0.77\n\nprob &lt;- dpois(x, lambda=sample.mean)\nprob &lt;- c(prob, 1-sum(prob))\nExp &lt;- n*prob\nx &lt;- c(x, 5)\nObs &lt;- c(Obs, 0)\ncbind(x, Obs, prob, Exp)\n\n     x Obs        prob        Exp\n[1,] 0  94 0.463013068 92.6026137\n[2,] 1  67 0.356520063 71.3040125\n[3,] 2  33 0.137260224 27.4520448\n[4,] 3   3 0.035230124  7.0460248\n[5,] 4   3 0.006781799  1.3563598\n[6,] 5   0 0.001194722  0.2389444\n\n(Exp &gt;= 5)  # 기대빈도가 5 이상인지 확인\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\nObs[4] &lt;- sum(Obs[4:6])\nprob[4] &lt;- sum(prob[4:6])\nExp[4] &lt;- sum(Exp[4:6])\n\nx &lt;- x[1:4]\nObs &lt;- Obs[1:4]\nprob &lt;- prob[1:4]\nExp &lt;- Exp[1:4]\n\nchisq &lt;- (Obs-Exp)^2/Exp\ncbind(x, Obs, prob, Exp, chisq)\n\n     x Obs       prob       Exp      chisq\n[1,] 0  94 0.46301307 92.602614 0.02108675\n[2,] 1  67 0.35652006 71.304013 0.25979637\n[3,] 2  33 0.13726022 27.452045 1.12122091\n[4,] 3   6 0.04320664  8.641329 0.80735485\n\nsum.chisq &lt;- sum(chisq)\n(p.value &lt;-1 -pchisq(sum.chisq, df=4-1-1))\n\n[1] 0.3313005\n\n# 하루에 발생하는 AS요청건수의 분포가 포아송분포를 따른다는 귀무가설을 기각할 수 없다.\n\n## vcd package --&gt; does not collapes!!!\nObs &lt;- c(94, 67, 33, 3, 3)\nx.poi &lt;- c(rep(0,94), rep(1,67), rep(2, 33), rep(3, 3), rep(4, 3))\nlibrary(vcd)\n\nLoading required package: grid\n\ngf &lt;- goodfit(x.poi, type= \"poisson\", method=\"MinChisq\")\nsummary(gf)\n\nWarning in summary.goodfit(gf): Chi-squared approximation may be incorrect\n\n\n\n     Goodness-of-fit test for poisson distribution\n\n             X^2 df  P(&gt; X^2)\nPearson 4.760495  3 0.1901988\n\nplot(gf, main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\n\n\n머리둘레- 정규분포 적합도 검정\n\n어느 모자 제작업체가 야구모자를 제작하여 야구팬들을 대상으로 판매하려는 계획을 세웠다. 어떤 크기의 모자를 얼마나 많이 만들어야 할지 결정하기 전에 야구팬들의 머리둘레길이가 어떤 분포를 이루고 있는지 알아야 하였다. 이를 조사하기 위해 야구장을 찾은 야구팬 중 임의로 50명을 선정하고 머리둘레를 재어 다음과 같은 크기 순으로 정렬된 자료를 얻었다.\n\n51.92, 52.10, 52.25, 52.41, 52.48, 52.64, 53.02, 53.39, 53.40, 53.77, 54.00, 54.04, 54.30, 54.42, 54.77, 54.78, 54.85, 55.31, 55.33, 55.38, 55.49, 55.51, 55.68, 55.69, 55.80, 55.85, 56.03, 56.05, 56.10, 56.18, 56.22, 56.26, 56.29, 56.32, 56.35, 56.45, 56.57, 56.64, 56.71, 56.84, 56.91, 57.03, 57.14, 57.18, 58.00, 58.07, 58.32, 58.66, 59.26, 59.37\n이 자료로부터 야구팬들의 머리둘레길이가 정규분포를 따른다고 가정하는 것이 적절한지 적합도 검정을 하시오.\n\nx &lt;- c(51.92, 52.10, 52.25, 52.41, 52.48, 52.64, 53.02, 53.39, 53.40, 53.77, 54.00, 54.04, 54.30, 54.42, 54.77, 54.78, 54.85, 55.31, 55.33, 55.38, 55.49, 55.51, 55.68, 55.69, 55.80, 55.85, 56.03, 56.05, 56.10, 56.18, 56.22, 56.26, 56.29, 56.32, 56.35, 56.45, 56.57, 56.64, 56.71, 56.84, 56.91, 57.03, 57.14, 57.18, 58.00, 58.07, 58.32, 58.66, 59.26, 59.37)\n(x.mean &lt;- mean(x))\n\n[1] 55.5506\n\n(x.sd &lt;- sd(x))\n\n[1] 1.859135\n\nprob &lt;- rep(0.1, 10)\np &lt;- cumsum(prob)\n(z &lt;- qnorm(p, x.mean, x.sd))\n\n [1] 53.16802 53.98591 54.57567 55.07959 55.55060 56.02161 56.52553 57.11529\n [9] 57.93318      Inf\n\nz\n\n [1] 53.16802 53.98591 54.57567 55.07959 55.55060 56.02161 56.52553 57.11529\n [9] 57.93318      Inf\n\ncount &lt;- as.vector(10)\nfor (i in 1:10){\n  count[i] &lt;- length(x[x&lt;z[i]])\n}\ncount\n\n [1]  7 10 14 17 22 26 36 42 44 50\n\nfor (i in 10:2){\n  count[i] &lt;- count[i] - count[i-1]\n}\ncount\n\n [1]  7  3  4  3  5  4 10  6  2  6\n\nexp &lt;- rep(5, 10)\nchisq_i &lt;- (count - exp)^2 / exp\ncbind(count, prob, exp, chisq_i)\n\n      count prob exp chisq_i\n [1,]     7  0.1   5     0.8\n [2,]     3  0.1   5     0.8\n [3,]     4  0.1   5     0.2\n [4,]     3  0.1   5     0.8\n [5,]     5  0.1   5     0.0\n [6,]     4  0.1   5     0.2\n [7,]    10  0.1   5     5.0\n [8,]     6  0.1   5     0.2\n [9,]     2  0.1   5     1.8\n[10,]     6  0.1   5     0.2\n\n(chisq &lt;- sum(chisq_i))\n\n[1] 10\n\n1-pchisq(chisq, 7)  # p-value\n\n[1] 0.1885735\n\n# 야구팬들의 머리둘레길이가 정규분포를 따른다는 귀무가설을 기각할 수 없다."
  },
  {
    "objectID": "posts/12/index.html#카이제곱검정을-활용한-독립성-검정",
    "href": "posts/12/index.html#카이제곱검정을-활용한-독립성-검정",
    "title": "12. 카이제곱검정",
    "section": "카이제곱검정을 활용한 독립성 검정",
    "text": "카이제곱검정을 활용한 독립성 검정\n분류변수에 대한 검정 중 가장 빈번하게 사용되며, 한 모집단에서 두 개의 분류변수가 서로 독립적으로 움직이는지 확인하기 위해 사용하는 분석법이다. 즉 혈액형과 성별, 학년과 학점 등 두 개의 범주형 변수에 대해서 먼저 두 변수 간의 관련성이 주된 관심이 되며, 두 변수가 독립인지 검정하게 되는데, 이를 독립성 검정이라고 한다.\n\n두 분류변수의 표 요약: 교차표, 분할표, 이원분류표\n\n2개의 분류변수 혹은 번주변수가 주어진 경우, 관련성을 알아보기 위해 표로 요약하여 두 범주형 변수가 서로 독립인지 아닌지 파악하게 된다. 각 범주변수의 범주값을 가로와 세로에 나열해 교차되는 곳의 관측 도수를 세어서 요약한 표를 교차표, 분할표, 이원분류표라 한다. 이렇게 표로 먼저 요약한 후에 카이제곱 검정을 실시한다.\ntable(), xtabs() 함수를 이용한다.\n\ntable(x1, x2): x1은 행변수, x2는 열변수\ntable(x1, x2, x3): x1은 행변수, x2는 열변수, x3는 층변수\nxtabs(~x1+x2+x3, data=dataframe): ‘~’과 함께 각 분류변수를’+’로 연결\n\n동질성 검정은 한 모집단에서 두 분류변수 관계를 관측하는 것이고, 독립성 검정은 여러 모집단에서 한 분류변수를 측정하는 것이다. 교차표로 요악하는 방식은 동일하지만, 동질성 검정에서는 반드시 한 분류변수가 모집단을 나타낸다.\n귀무가설(\\(H_0\\)): \\(p_{ij} = p_{i}\\ p_{j}\\)\n\n\n\n\nFisher의 Exact결정\n\n독립성 검정에서의 카이제곱 통계량 이용 조건\n\n관측도수(셀)의 기대도수가 1 이상\n기대도수가 5 이하인 셀이 20% 이하\n\n기대도수가 5 이하인 셀이 20%를 넘었을 때, Fisher의 Exact 검정 이용\n\n기대도수가 5 이하인 셀이 20%를 넘었을 때, 다음 메시지 발생\n“Warning in chisq.test(tcy): Chi-squared approximation may be incorrect”\n\nfisher.test() 함수 이용\n\n\n\n성별과 흡연여부의 독립성 검정\n\n성별과 흡연여부는 서로 독립인지, 관련이 있는지 독립성을 검정하시오.\n\n\nlibrary(MASS)\nsurvey$Sex\n\n  [1] Female Male   Male   Male   Male   Female Male   Female Male   Male  \n [11] Female Male   Female Female Male   Female Female Male   Male   Male  \n [21] Male   Male   Male   Male   Female Male   Male   Male   Male   Male  \n [31] Female Male   Female Male   Male   Male   Female Male   Male   Male  \n [41] Female Female Male   Female Female Male   Male   Male   Female Female\n [51] Male   Male   Male   Male   Male   Male   Female Male   Male   Male  \n [61] Male   Female Female Female Female Male   Female Female Male   Male  \n [71] Female Male   Female Female Female Female Female Female Female Male  \n [81] Male   Male   Female Female Male   Female Female Female Male   Female\n [91] Male   Female Female Female Male   Female Male   Female Male   Female\n[101] Male   Male   Female Female Female Female Female Female Male   Male  \n[111] Female Male   Female Male   Female Female Female Male   Female Male  \n[121] Male   Male   Female Male   Male   Male   Female Male   Female Female\n[131] Male   Male   Female Female Male   Male   &lt;NA&gt;   Male   Male   Female\n[141] Female Female Female Male   Female Male   Male   Male   Female Female\n[151] Male   Female Female Male   Male   Male   Male   Female Male   Male  \n[161] Female Male   Male   Female Male   Female Female Female Male   Male  \n[171] Female Male   Female Female Female Female Male   Female Female Female\n[181] Male   Female Female Male   Male   Male   Female Male   Male   Male  \n[191] Male   Male   Male   Female Female Female Female Male   Female Female\n[201] Female Male   Female Female Male   Female Female Male   Male   Female\n[211] Female Female Male   Male   Female Male   Female Male   Female Male  \n[221] Male   Female Female Female Female Female Female Male   Female Male  \n[231] Female Male   Female Female Female Male   Female\nLevels: Female Male\n\nsurvey$Smoke\n\n  [1] Never Regul Occas Never Never Never Never Never Never Never Never Never\n [13] Never Never Never Never Never Never Never Never Never Never Never Never\n [25] Never Never Never Never Never Never Occas Never Heavy Never Regul Occas\n [37] Never Never Never Occas Never Never Never Never Never Never Regul Never\n [49] Occas Never Never Never Never Never Regul Never Never Never Never Occas\n [61] Never Never Never Never Never Never Never Never Never &lt;NA&gt;  Never Heavy\n [73] Never Never Never Heavy Never Heavy Never Never Never Never Never Never\n [85] Never Never Never Never Never Never Never Never Never Never Heavy Never\n [97] Regul Never Never Never Never Never Never Never Heavy Never Occas Never\n[109] Never Regul Never Never Never Never Never Never Never Heavy Never Heavy\n[121] Occas Occas Never Never Never Never Never Regul Never Never Never Never\n[133] Never Occas Never Regul Never Never Never Never Never Never Never Never\n[145] Never Never Regul Never Never Never Never Never Regul Never Never Never\n[157] Heavy Never Never Regul Never Never Never Occas Never Never Never Regul\n[169] Never Never Never Occas Regul Never Never Never Regul Never Regul Never\n[181] Occas Occas Never Never Never Never Never Never Never Never Never Never\n[193] Occas Never Never Never Never Never Occas Never Occas Never Never Never\n[205] Regul Occas Never Never Never Never Never Never Never Never Never Never\n[217] Never Never Never Never Heavy Never Never Never Never Never Heavy Never\n[229] Regul Occas Never Never Never Never Never Never Never\nLevels: Heavy Never Occas Regul\n\n## 1) cross table(교차표)\n####(1) table()\ntle_data = table(survey$Sex, survey$Smoke)\ntle_data\n\n        \n         Heavy Never Occas Regul\n  Female     5    99     9     5\n  Male       6    89    10    12\n\nmarginSums(tle_data, margin = 1)  # row sum\n\n\nFemale   Male \n   118    117 \n\nmarginSums(tle_data, margin = 2)  # column sum\n\n\nHeavy Never Occas Regul \n   11   188    19    17 \n\naddmargins(tle_data)  # 표에 행과 열의 합계 추가\n\n        \n         Heavy Never Occas Regul Sum\n  Female     5    99     9     5 118\n  Male       6    89    10    12 117\n  Sum       11   188    19    17 235\n\n####(2) xtabs()\ntbl_data = xtabs(~Sex+Smoke, data=survey)\ntbl_data\n\n        Smoke\nSex      Heavy Never Occas Regul\n  Female     5    99     9     5\n  Male       6    89    10    12\n\naddmargins(tbl_data)\n\n        Smoke\nSex      Heavy Never Occas Regul Sum\n  Female     5    99     9     5 118\n  Male       6    89    10    12 117\n  Sum       11   188    19    17 235\n\n## 2) independence test, alpha=0.05\n####(1) raw data\nchisq.test(survey$Sex, survey$Smoke)  # 원데이터를 그대로 이용하기\n\n\n    Pearson's Chi-squared test\n\ndata:  survey$Sex and survey$Smoke\nX-squared = 3.5536, df = 3, p-value = 0.3139\n\n####(2) crosstable\nchisq.test(tbl_data)  # 교차표를 만들어서 검정하기, 귀무가설 채택(성별과 흡연여부는 독립임)\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl_data\nX-squared = 3.5536, df = 3, p-value = 0.3139\n\n####(3) CrossTable() of gmodels package\n#install.packages('gmodels')  # gmodels 패키지가 없으면 주석 지우고 실행하기\nlibrary(gmodels)\n\nWarning: package 'gmodels' was built under R version 4.3.3\n\nCrossTable(survey$Sex, survey$Smoke, chisq = T)  # 교차표와 카이제곱검정을 한꺼번에 실행함\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  235 \n\n \n             | survey$Smoke \n  survey$Sex |     Heavy |     Never |     Occas |     Regul | Row Total | \n-------------|-----------|-----------|-----------|-----------|-----------|\n      Female |         5 |        99 |         9 |         5 |       118 | \n             |     0.050 |     0.224 |     0.031 |     1.465 |           | \n             |     0.042 |     0.839 |     0.076 |     0.042 |     0.502 | \n             |     0.455 |     0.527 |     0.474 |     0.294 |           | \n             |     0.021 |     0.421 |     0.038 |     0.021 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|\n        Male |         6 |        89 |        10 |        12 |       117 | \n             |     0.050 |     0.226 |     0.031 |     1.477 |           | \n             |     0.051 |     0.761 |     0.085 |     0.103 |     0.498 | \n             |     0.545 |     0.473 |     0.526 |     0.706 |           | \n             |     0.026 |     0.379 |     0.043 |     0.051 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|\nColumn Total |        11 |       188 |        19 |        17 |       235 | \n             |     0.047 |     0.800 |     0.081 |     0.072 |           | \n-------------|-----------|-----------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  3.553618     d.f. =  3     p =  0.3138744 \n\n\n \n\n\n\n성별과 대장암 유무에 대해 조사한 결과표가 다음과 같다. 성별과 대장암 유무가 관련이 있는지 독립성을 검정하시오. \n\n\n## less 5 frequency over 20% in crosstable(5이하의 도수가 20% 이상시 독립성 검정)\n## fisher's exact test\n\nsex = c(\"female\", \"female\", \"male\", \"male\")\ncancer = c(\"1.Yes\", \"2.No\", \"1.Yes\", \"2.No\")\ncount = c(2,23,4,21)\n\ndata = data.frame(sex,cancer, count)\n\ntab = xtabs(count~sex+cancer, data=data)\ntab\n\n        cancer\nsex      1.Yes 2.No\n  female     2   23\n  male       4   21\n\nchisq.test(tab)  # 분석결과는 나오지만 오류메시지가 뜸\n\nWarning in chisq.test(tab): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tab\nX-squared = 0.18939, df = 1, p-value = 0.6634\n\n## fisher's exact test\nfisher.test(tab)  # 검정결과, 성별과 대장암 발병여부는 독립임.\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tab\np-value = 0.6671\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.03819639 3.62439874\nsample estimates:\nodds ratio \n 0.4635832 \n\n\n\n\n전공과 학점 분할표\n\n130명의 대학원생을 대상으로 학부학점과 대학원전공을 조사한 결과가 다음 표와 같다. 대학원전공과 학부학점 간의 관련이 있는지 검정하시오. \n\n\ny &lt;- matrix(c(30, 13, 16,\n               8, 16,  7,\n              12, 11, 17), nrow=3, ncol=3, byrow=T)\ndimnames(y) &lt;- list(c(\"3.5-4.5\",\"2.5-3.5\",\"1.5-2.5\"), c(\"사회과학\",\"자연과학\",\"공학\"))\ny\n\n        사회과학 자연과학 공학\n3.5-4.5       30       13   16\n2.5-3.5        8       16    7\n1.5-2.5       12       11   17\n\nchisq.test(y, correct=F)\n\n\n    Pearson's Chi-squared test\n\ndata:  y\nX-squared = 13.088, df = 4, p-value = 0.01085\n\n# 학부성적과 대학원전공이 독립이라는 가정을 기각한다.\n\nx &lt;- chisq.test(y, correct=F)  # 독립된 가정에서의 기댓값 출력\nx$expected\n\n        사회과학  자연과학      공학\n3.5-4.5 22.69231 18.153846 18.153846\n2.5-3.5 11.92308  9.538462  9.538462\n1.5-2.5 15.38462 12.307692 12.307692\n\nx$residuals^2  # 독립된 가정에서의 카이제곱값\n\n         사회과학  자연과학      공학\n3.5-4.5 2.3533246 1.4631682 0.2555411\n2.5-3.5 1.2908189 4.3771712 0.6755583\n1.5-2.5 0.7446154 0.1389423 1.7889423"
  },
  {
    "objectID": "posts/12/index.html#카이제곱검정을-활용한-동질성-검정",
    "href": "posts/12/index.html#카이제곱검정을-활용한-동질성-검정",
    "title": "12. 카이제곱검정",
    "section": "카이제곱검정을 활용한 동질성 검정",
    "text": "카이제곱검정을 활용한 동질성 검정\n두 모집단에 대해서 단일변수의 분포가 동일한지 알아보는 것을 동질성 검정(homogeneity test)이라고 한다. 표본을 뽑을 때 모집단이 2개이기 때문에 서로 다른 모집단에서 각각 동일한 분류변수를 관측하는 것이 (적합도 검정과) 차이가 있다. 성인 남녀에서 공공기관 합격여부가 성공 또는 실패 비율이 남녀별 동일한지, 중간시험에서 남녀에 따라 학점의 분포가 동일한지, 도시농촌간 독감예방현황의 분포가 동일한지 등을 알아볼때 사용된다. 다음에 나오는 독립성 검정에서는 한 변수는 모집단을 나타내고, 다른 변수는 단일 분류변수를 나타낸다. 두 모집단이나 세 모집단 또는 여러 모집단에서 범주형 변수의 분포에 대한 동질성 검정도 chisq.test() 함수에 의해 검정할 수 있다.\n\n세 지역의 독감 접종률이 다른지 검정하기\n\n세 지역의 독감 접종 여부를 요약한 표가 다음과 같다. 세 지역의 독감 접종률이 다른지 검정하시오. \n\n\narea = c(\"area1\", \"area1\", \"area2\", \"area2\", \"area3\", \"area3\") #각각에 대하여 접종 여부\nflu_shot = c(\"1.Yes\", \"2.No\", \"1.Yes\", \"2.No\", \"1.Yes\", \"2.No\") #독감예방접종 여부\ncount = c(19, 21, 14, 21, 13, 22)\n\ndata = data.frame(area, flu_shot, count)\ntab = xtabs(count ~ area + flu_shot, data= data)\ntab  #교차표 만들기\n\n       flu_shot\narea    1.Yes 2.No\n  area1    19   21\n  area2    14   21\n  area3    13   22\n\nchisq.test(tab)  #교차표 사용하기, 검정결과 귀무가설 채택, 지역에 따른 예방접종률의 분포는 동일함\n\n\n    Pearson's Chi-squared test\n\ndata:  tab\nX-squared = 0.89274, df = 2, p-value = 0.6399\n\n\n\n\nCars93 데이터셋에서 에어백 비율 검정\n\nCars93 데이터셋에서 에어백 여부의 비율에 대한 검정\n\n\nlibrary(MASS)\nprop.table(table(Cars93$AirBags))  # HO: 1/3 1/3 1/3\n\n\nDriver & Passenger        Driver only               None \n         0.1720430          0.4623656          0.3655914 \n\nchisq.test(x=table(Cars93$AirBags), p=c(1/3, 1/3, 1/3))  #검정결과 귀무가설 기각, 비율이 다름\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(Cars93$AirBags)\nX-squared = 12.194, df = 2, p-value = 0.00225\n\nchisq.test(x=table(Cars93$AirBags), p=c(0.15, 0.70, 0.15))  #귀무가설의 비율을 바꿔서 검정결과 귀무가설 기각, 즉 비율이 하나 이상 차이가 있음\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(Cars93$AirBags)\nX-squared = 36.621, df = 2, p-value = 1.116e-08\n\n\n\n\nHomogeneity Test 동질성 검정\n\n남녀별 대학생의 전공자수를 조사하였다. 남학생은 45명 중 독문학, 불문학을 각각 30, 15명이 전공하고 있었고, 125명의 여학생은 각각 50, 75명이 전공하고 있었다. 남녀별 전공의 분포가 같은지 알아보고자 한다. 즉 남학생 집단에서의 전공에 대한 이항분포의 두 범주에 대한 확률과 여학생 집단에서의 전공비율이 각각 같은지 검정하시오.\n\n\nhomo &lt;- matrix(c(30, 15, 50, 75), nrow=2, ncol=2, byrow=T)\ndimnames(homo) &lt;- list(c(\"남자\",\"여자\"), c(\"독문학\",\"불문학\"))\nn.marginal &lt;- rowSums(homo)\np.obs &lt;- homo / n.marginal\np.exp &lt;- colSums(homo)/350\nhomo.exp &lt;- n.marginal%o%p.exp\nhomo.exp\n\n       독문학   불문학\n남자 10.28571 11.57143\n여자 28.57143 32.14286\n\nhomo.chisq &lt;- (homo - homo.exp)^2 / homo.exp\nhomo.chisq\n\n       독문학    불문학\n남자 37.78571  1.015873\n여자 16.07143 57.142857\n\nrowSums(homo.chisq)\n\n    남자     여자 \n38.80159 73.21429 \n\n(xx &lt;- chisq.test(homo))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  homo\nX-squared = 8.4044, df = 1, p-value = 0.003743\n\nxx$expected\n\n       독문학   불문학\n남자 21.17647 23.82353\n여자 58.82353 66.17647\n\nxx$residuals^2  # 독립이라는 가정 하에 기댓값\n\n       독문학   불문학\n남자 3.676471 3.267974\n여자 1.323529 1.176471\n\nsum(xx$residuals^2)  # 독립이라는 가정 하에 카이제곱값\n\n[1] 9.444444\n\n# p값이 유의 수준(0.05)보다 낮으므로, 두 집단의 분포가 같다는 가정을 기각한다.\n\n\nR에서 이러한 검정을 수행하기 위해 prop.test 함수를 사용할 수 있고, 이 함수는 두 또는 그 이상의 비율을 비교하는 데 사용된다.\n\n\n# 남학생과 여학생의 독문학과 불문학 전공자 수\nmale_students &lt;- c(30, 15)   # 남학생 독문학 30명, 불문학 15명\nfemale_students &lt;- c(50, 75) # 여학생 독문학 50명, 불문학 75명\n\n# prop.test를 사용하여 두 집단 간의 비율 차이 검정\nprop.test(x = c(sum(male_students), sum(female_students)), \n          n = c(sum(male_students) + sum(female_students), sum(female_students) + sum(male_students)), \n          alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(sum(male_students), sum(female_students)) out of c(sum(male_students) + sum(female_students), sum(female_students) + sum(male_students))\nX-squared = 73.424, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.5702594 -0.3709171\nsample estimates:\n   prop 1    prop 2 \n0.2647059 0.7352941 \n\n# p값이 유의 수준(0.05)보다 낮으므로, 두 집단의 분포가 같다는 가정을 기각한다."
  },
  {
    "objectID": "posts/08/index.html#일표본-위치평균-중심에-대한-검증",
    "href": "posts/08/index.html#일표본-위치평균-중심에-대한-검증",
    "title": "08. 모집단과 표본, 모평균의 추정",
    "section": "일표본 위치(평균, 중심)에 대한 검증",
    "text": "일표본 위치(평균, 중심)에 대한 검증\n\n모수적 검정: 데이터가 정규성을 만족하는 경우\n비모수적 검정: 데이터가 정규성을 만족하지 않는 경우\n\n\n\n* 그림 출처: 통계교육원, R을 활용한 통계분석"
  },
  {
    "objectID": "posts/03/index.html#정규분포의-공식-유도",
    "href": "posts/03/index.html#정규분포의-공식-유도",
    "title": "03. 연속확률변수, 정규분포",
    "section": "",
    "text": "정규분포의 공식 유도\n정규분포 확률밀도함수의 유도, 증명, 성질\n나무위키: 가우스 적분"
  },
  {
    "objectID": "posts/10/index.html#두-독립-표본-위치-차이-검정",
    "href": "posts/10/index.html#두-독립-표본-위치-차이-검정",
    "title": "10. 하나의 모집단에 대한 가설검정(평균)",
    "section": "두 독립 표본 위치 차이 검정",
    "text": "두 독립 표본 위치 차이 검정\n\n두 독립 표본 위치 차이 검정이란\n통계적 가설검정에서 두 독립 모집단에 대한 평균을 비교하는 검증으로,\n\n남녀 간의 성의식 차이\n농촌과 도시 간의 주거만족도\n수도권과 비수도권의 삶의 만족도\n\n와 같은 것들을, 보통 \\(t\\)검정을 통해 두 독립 표본 평균의 차이를 비교한다.\n\n\n두 독립 모집단의 평균을 비교할 때의 조건\n\n두 독립 모집단의 분포는 각각 정규분포여야 함\n두 독립 모집단이 모두 정규분포이고, 분산이 알려져 있을때 \\(\\rightarrow\\) \\(Z\\)검정을 통해 평균차이를 검정함\n\n\\(H_0 : ~ \\mu_1 = \\mu_2 ~\\Rightarrow~ \\mu_1 - \\mu_2 =0\\)\n검정통계량: \\(X-Y~ \\sim ~N \\left(\\mu_1 - \\mu_2 ,~ \\dfrac{\\sigma^2 _1}{n_1}+ \\dfrac{\\sigma^2 _2}{n_2} \\right)\\)\n표준화: \\(\\dfrac{\\bar{X}-\\bar{Y}-(\\mu_1 -\\mu_2)}{\\sqrt{\\dfrac{\\sigma^2 _1}{n_1}+ \\dfrac{\\sigma^2 _2}{n_2}}}~\\sim ~N(0,~1)\\)\n\n두 독립 모집단이 모두 정규분포이고, 분산이 알려져 있지 않을때 \\(\\rightarrow\\) 등분산인지 아닌지 검정 후, \\(t\\)검정을 통해 평균차이를 검정\n\n등분산을 가정할때:  \n분산이 같지 않다고 가정할때: \n\n\n\n\n두 독립 모집단의 분산 차이에 대한 등분산성 검정\n\n분산이 동일한지 동일하지 않은지에 따라 분산추정량이 달라지기에, 두 집단의 평균을 비교하기 전에 분산의 동일 여부를 검증하며, 이것을 등분산성 검증이라 한다. 아래 귀무가설에서 \\(\\sigma^2 _1 - \\sigma^2 _2 =0\\)의 분포 유도는 불가능하기에 귀무가설은 \\(\\dfrac{\\sigma^2 _1 }{\\sigma^2 _2}=1\\)로 쓸 수 있고, 분산비 \\(\\dfrac{\\sigma^2 _1 }{\\sigma^2 _2}=1\\)에 대한 추정량은 \\(\\dfrac{S^2 _1 }{S^2 _2}=1\\)이며 \\(F\\)분포를 따른다.\n\n두 독립표본의 모분산에 대한 검정\n\n\n\n\n\n\n\n\n\n\n\n귀무가설\n\n대립가설\n\n\n\n\n\\(H_0 : ~\\sigma^2 _1 = \\sigma^2 _2\\)\nVS\n\\(H_1 : ~\\sigma^2 _1 \\neq \\sigma^2 _2\\)(양측검정)\n\n\n\\(H_0 : ~\\sigma^2 _1 = \\sigma^2 _2\\)\nVS\n\\(H_1 : ~\\sigma^2 _1 &gt; \\sigma^2 _2\\) (단측검정)\n\n\n\\(H_0 : ~\\sigma^2 _1 = \\sigma^2 _2\\)\nVS\n\\(H_1 : ~\\sigma^2 _1 &lt; \\sigma^2 _2\\) (단측검정)\n\n\n\n\n\n검정통계량을 통해 분산의 동일 유무 검증은 Levene Test로 한다.\n\n\n\nCars93 데이터셋으로 실습하기\n\n‘Cars93’ 데이터셋에서 미국회사 유무(Origin)에 따른 자동차의 가격(Price)에 대한 독립표본 \\(t\\)검정하기\n\n\n# two sample t-test\nlibrary(MASS)\ntable(Cars93$Origin)  # 데이터가 30개가 넘으므로 중심극한정리에 의해 정규성을 만족한다고 가정\n\n\n    USA non-USA \n     48      45 \n\n# equal variance test: leven test,  alpha = 0.05 &gt; p ==&gt; H0 reject\n# install.packages(\"lawstat\")  # lawstat 패키지가 없으면 주석을 지우고 설치하기\nlibrary(lawstat)  # levene.test()를 하기 위해 패키지 불러오기\n\nWarning: package 'lawstat' was built under R version 4.3.3\n\nlevene.test(Cars93$Price, Cars93$Origin, location = 'mean')  # H1 채택, 즉 두 집단의 분산이 다름\n\n\n    Classical Levene's test based on the absolute deviations from the mean\n    ( none not applied because the location is not set to median )\n\ndata:  Cars93$Price\nTest Statistic = 4.2341, p-value = 0.04248\n\n# two sample t-test\nprice0 = Cars93$Price[Cars93$Origin == 'USA']\nprice1 = Cars93$Price[Cars93$Origin != 'USA'] # Cars93$Origin을 치면, non-USA값이지만 왼쪽과 같이 써도 됨\n\nt.test(price0, price1, alternative = 'two.sided', var.equal = FALSE)  # p=0.3428이므로 귀무가설 채택\n\n\n    Welch Two Sample t-test\n\ndata:  price0 and price1\nt = -0.95449, df = 77.667, p-value = 0.3428\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.974255  2.102311\nsample estimates:\nmean of x mean of y \n 18.57292  20.50889 \n\nt.test(price0, price1, alternative = 'less', var.equal = FALSE)  # p=0.1714이므로 귀무가설 채택\n\n\n    Welch Two Sample t-test\n\ndata:  price0 and price1\nt = -0.95449, df = 77.667, p-value = 0.1714\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 1.440539\nsample estimates:\nmean of x mean of y \n 18.57292  20.50889 \n\nt.test(price0, price1, alternative = 'greater', var.equal = FALSE)  # p=0.8286이므로 귀무가설 채택\n\n\n    Welch Two Sample t-test\n\ndata:  price0 and price1\nt = -0.95449, df = 77.667, p-value = 0.8286\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -5.312484       Inf\nsample estimates:\nmean of x mean of y \n 18.57292  20.50889 \n\n\n\n‘Cars93’ 데이터셋에서 수동변속기의 여부에 따른 자동차의 가격(Price)에 대한 독립표본 \\(t\\)검정하기\n\n\ntable(Cars93$Man.trans.avail) # 데이터가 30개가 넘으므로 중심극한정리에 의해 정규성을 만족한다고 가정\n\n\n No Yes \n 32  61 \n\n# equal variance test\nprice_0 = Cars93$Price[Cars93$Man.trans.avail == 'Yes']\nprice_1 = Cars93$Price[Cars93$Man.trans.avail != 'Yes']\n\nlevene.test(Cars93$Price, Cars93$Man.trans.avail, location = 'mean')  # p=0.4405이므로 H0 채택, 즉 두 집단은 등분산임\n\n\n    Classical Levene's test based on the absolute deviations from the mean\n    ( none not applied because the location is not set to median )\n\ndata:  Cars93$Price\nTest Statistic = 0.60016, p-value = 0.4405\n\nt.test(price_0, price_1, alternative = 'two.sided', var.equal = TRUE)  # p=0.001403이므로 H1 채택, 즉 수동변속기 유무에 따른 두 집단의 가격이 다르다 할 수 있음 \n\n\n    Two Sample t-test\n\ndata:  price_0 and price_1\nt = -3.2952, df = 91, p-value = 0.001403\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -10.583164  -2.622676\nsample estimates:\nmean of x mean of y \n 17.23770  23.84062 \n\nt.test(price_0, price_1, alternative = 'less', var.equal = TRUE)  # p=0.0007013으로 양측검정때보다 더 작아짐\n\n\n    Two Sample t-test\n\ndata:  price_0 and price_1\nt = -3.2952, df = 91, p-value = 0.0007013\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf -3.273112\nsample estimates:\nmean of x mean of y \n 17.23770  23.84062"
  },
  {
    "objectID": "posts/10/index.html#두-독립-모집단의-정규성을-보장하지-못할-때-평균-차이에-대한-비모수적-검정",
    "href": "posts/10/index.html#두-독립-모집단의-정규성을-보장하지-못할-때-평균-차이에-대한-비모수적-검정",
    "title": "10. 하나의 모집단에 대한 가설검정(평균)",
    "section": "두 독립 모집단의 정규성을 보장하지 못할 때, 평균 차이에 대한 비모수적 검정",
    "text": "두 독립 모집단의 정규성을 보장하지 못할 때, 평균 차이에 대한 비모수적 검정\n\n윌콕슨 순위합 검정이란\n\n\n\n\n예제로 실습하기\n\n남학생과 여학생의 중간시험 성적이 각각 다음과 같을때, 두 집단의 중간시험 성적의 평균이 같은지를 검정하고자 한다. 남학생: 74, 43, 91, 58, 60, 여학생: 50, 23, 69, 45\n\n\n# 13 wilcoxon rank sum test for two sample \n\nmale = c(74, 43, 91, 58, 60)\nfemale = c(50, 23, 69, 45)\nshapiro.test(male) # normality test\n\n\n    Shapiro-Wilk normality test\n\ndata:  male\nW = 0.97284, p-value = 0.8932\n\nshapiro.test(female) # 정규분포를 따른다고 p값이 나오지만 표본수가 적어서 정규분포를 따른다는 가정을 못하고, 윌콕슨 검정하기\n\n\n    Shapiro-Wilk normality test\n\ndata:  female\nW = 0.98134, p-value = 0.9098\n\ndata = c(male, female) # 데이터 합치기\nrank_data = rank(data) # 데이터의 순위를 매기기\nrank_data\n\n[1] 8 2 9 5 6 4 1 7 3\n\nfemale_seq = seq(length(male)+1, length(data)) # 여성 자료의 위치\nfemale_seq\n\n[1] 6 7 8 9\n\nfemale_rank = rank_data[female_seq]\nfemale_rank\n\n[1] 4 1 7 3\n\nW = sum(female_rank)  # 더 적은 집단의 합을 W로 함\nW  # 수작업으로 W구하기\n\n[1] 15\n\nwilcox.test(male, female, alternative = 'two.sided') # 바로 W를 구하고, p값 구함, (alpha = 0.05 &gt; p ==&gt; H0 reject) H0를 채택함, 즉 여학생과 남학생의 성적 평균의 차이가 없음\n\n\n    Wilcoxon rank sum exact test\n\ndata:  male and female\nW = 15, p-value = 0.2857\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\nCars93 데이터셋으로 실습하기\n\n‘Cars93’ 데이터셋에서 미국회사 유무에 따른 자동차의 가격(Price)에 대한 독립표본 윌콕슨 순위합 검정\n\n\nlibrary(MASS)\ntable(Cars93$Origin)\n\n\n    USA non-USA \n     48      45 \n\n# wilcoxon rank sum test , alpha = 0.05 &gt; p ==&gt; Ho rejcet\n\nprice_0 = Cars93$Price[Cars93$Origin == 'USA']\nprice_1 = Cars93$Price[Cars93$Origin != 'USA']\nwilcox.test(price_0, price_1, alternative = 'two.sided') # p값 ==&gt; H0 채택, 즉 가격 차이가 없음\n\nWarning in wilcox.test.default(price_0, price_1, alternative = \"two.sided\"):\ncannot compute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  price_0 and price_1\nW = 1024.5, p-value = 0.6724\nalternative hypothesis: true location shift is not equal to 0\n\nwilcox.test(price_0, price_1, alternative = 'less') # p값 ==&gt; H0 채택\n\nWarning in wilcox.test.default(price_0, price_1, alternative = \"less\"): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  price_0 and price_1\nW = 1024.5, p-value = 0.3362\nalternative hypothesis: true location shift is less than 0\n\nwilcox.test(price_0, price_1, alternative = 'greater') # p값 ==&gt; H0 채택\n\nWarning in wilcox.test.default(price_0, price_1, alternative = \"greater\"):\ncannot compute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  price_0 and price_1\nW = 1024.5, p-value = 0.6666\nalternative hypothesis: true location shift is greater than 0\n\n\n\n‘Cars93’ 데이터셋에서 수동변속기 유무에 따른 자동차의 가격(Price)에 대한 독립표본 윌콕슨 순위합 검정\n\n\nprice0 = Cars93$Price[Cars93$Man.trans.avail == 'Yes']\nprice1 = Cars93$Price[Cars93$Man.trans.avail != 'Yes']\n\nwilcox.test(price0, price1, alternative = 'two.sided') # p값 ==&gt; H0 기각, 즉 가격 차이가 있음\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  price0 and price1\nW = 530.5, p-value = 0.0003196\nalternative hypothesis: true location shift is not equal to 0\n\nwilcox.test(price0, price1, alternative = 'less') # p값 ==&gt; H0 기각\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  price0 and price1\nW = 530.5, p-value = 0.0001598\nalternative hypothesis: true location shift is less than 0\n\nwilcox.test(price0, price1, alternative = 'greater') # 'less'가 가장 적당하다는 것을 알 수 있음\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  price0 and price1\nW = 530.5, p-value = 0.9998\nalternative hypothesis: true location shift is greater than 0"
  },
  {
    "objectID": "posts/10/index.html#대응표본-평균-차이-검정",
    "href": "posts/10/index.html#대응표본-평균-차이-검정",
    "title": "10. 하나의 모집단에 대한 가설검정(평균)",
    "section": "대응표본 평균 차이 검정",
    "text": "대응표본 평균 차이 검정\n하나의 자료에 두 개의 대응되는 변수가 있을때, 두 변수의 평균의 차이가 있는지 검정하는 것\n\n주변 이산화탄소 농도에 따른 나무의 이산화탄소 흡수율 차이 검정\n데이터의 모양은 다음과 같다. \n\n## paired t-test\n## uptake of CO2\nstr(CO2)  # 데이터 구조 확인\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  84 obs. of  5 variables:\n $ Plant    : Ord.factor w/ 12 levels \"Qn1\"&lt;\"Qn2\"&lt;\"Qn3\"&lt;..: 1 1 1 1 1 1 1 2 2 2 ...\n $ Type     : Factor w/ 2 levels \"Quebec\",\"Mississippi\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Treatment: Factor w/ 2 levels \"nonchilled\",\"chilled\": 1 1 1 1 1 1 1 1 1 1 ...\n $ conc     : num  95 175 250 350 500 675 1000 95 175 250 ...\n $ uptake   : num  16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ...\n - attr(*, \"formula\")=Class 'formula'  language uptake ~ conc | Plant\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"outer\")=Class 'formula'  language ~Treatment * Type\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Ambient carbon dioxide concentration\"\n  ..$ y: chr \"CO2 uptake rate\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(uL/L)\"\n  ..$ y: chr \"(umol/m^2 s)\"\n\nuptake_95   = CO2[CO2$conc == 95, ]$uptake  # 주변 이산화탄소의 농도가 95일때 흡수율\nuptake_1000 = CO2[CO2$conc == 1000, ]$uptake  # 주변 이산화탄소의 농도가 1000일때 흡수율\nuptake_95\n\n [1] 16.0 13.6 16.2 14.2  9.3 15.1 10.6 12.0 11.3 10.5  7.7 10.6\n\nuptake_1000\n\n [1] 39.7 44.3 45.5 38.7 42.4 41.4 35.5 31.5 27.8 21.9 14.4 19.9\n\nboxplot(data.frame(uptake_95, uptake_1000),  # 상자 그림을 그려서 데이터 분포 확인\n        ylab='uptake')\n\n\n\n\n\n\n\n\n\n# paired t-test : HO : mu_d = 0, mu_d is no 0\n## 1. compare two variance \nvar.test(uptake_95, uptake_1000) # 한 모집단 내 두 변수의 등분산성 검정, H0: equal variance ==&gt; not equal variance\n\n\n    F test to compare two variances\n\ndata:  uptake_95 and uptake_1000\nF = 0.069007, num df = 11, denom df = 11, p-value = 0.0001066\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.01986555 0.23970934\nsample estimates:\nratio of variances \n        0.06900694 \n\n## 2. t-test, 두 대응표본의 이산화탄소 흡수율(uptake)에 대해 대응표본 t검정\nt.test(uptake_95, uptake_1000, paired = TRUE, # paired 파라미터를 TRUE로 설정하면 대응표본 t검정 실시\n       var.equal = FALSE, alternative = 'two.sided') ## reject H0, H1 채택, 주변이산화탄소 농도에 따른 이산화탄소 흡수율에 차이가 있다 할 수 있음\n\n\n    Paired t-test\n\ndata:  uptake_95 and uptake_1000\nt = -8.5197, df = 11, p-value = 3.573e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -26.83414 -15.81586\nsample estimates:\nmean difference \n        -21.325 \n\n\n\n\n중간고사와 기말고사의 평균 비교(5명이니 소표본임)\n\n# 수치형 변수이고, 표본이 작을 때 두 변수의 평균을 비교하는 비모수적 방법: 윌콕슨 부호 순위 검정\n## non-parametric paired test : wilcoxon signed rank test\n\nmidterm = c(80, 70, 62, 92, 87)\nfinal =   c(82, 72, 60, 91, 75)\n\nwilcox.test(midterm, final, alternative = 'two.sided',\n            paired = TRUE)  # H0 채택, 즉 차이가 없다.\n\nWarning in wilcox.test.default(midterm, final, alternative = \"two.sided\", :\ncannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  midterm and final\nV = 9, p-value = 0.7835\nalternative hypothesis: true location shift is not equal to 0\n\n## tie(동점)가 있는 경우 ==&gt; wilcoxon test ==&gt; wilcox.exact() function\n# install.packages('exactRankTests')  # 라이브러리가 없으면 주석 지우고 설치하기\nlibrary(exactRankTests)\n\nWarning: package 'exactRankTests' was built under R version 4.3.3\n\n\n Package 'exactRankTests' is no longer under development.\n Please consider using package 'coin' instead.\n\nwilcox.exact(midterm, final, alternative = 'two.sided',\n             paired = TRUE)  # H0 채택, 즉 차이가 없다.\n\n\n    Exact Wilcoxon signed rank test\n\ndata:  midterm and final\nV = 9, p-value = 0.8125\nalternative hypothesis: true mu is not equal to 0"
  },
  {
    "objectID": "posts/05/index.html#포아송분포와-체비세프-부등식",
    "href": "posts/05/index.html#포아송분포와-체비세프-부등식",
    "title": "05. 포아송분포, 균등분포",
    "section": "포아송분포와 체비세프 부등식",
    "text": "포아송분포와 체비세프 부등식\n\n체비세프 부등식\n\n\\(P(|X-\\mu| \\ge k) \\leq \\dfrac{\\sigma^2}{k^2}\\)\n\\(P(|X-\\mu| \\leq k ) \\geq 1- \\dfrac{\\sigma^2}{k^2}\\)\n\\(P(|X-\\mu| \\leq k \\sigma ) \\geq 1- \\dfrac{1}{k^2}\\)\n포아송분포에서 부등식 \\(P(|X-\\mu| \\leq 1.5 \\sigma ) \\geq 0.55\\)가 성립하는지 확률을 계산해보자.\n\n\n# 시간당 평균 4건이 사건의 발생하는 포아송분포 상황에서, X가 평균으로부터 1.5 sigma를 빼고 더한 1~7사이에 발생할 확률이 0.55보다 큰지 확인해보자.\nppois(7, 4)-ppois(0, 4)\n\n[1] 0.9305507"
  },
  {
    "objectID": "posts/03/index.html#이항분포의-정규분포로의-근사",
    "href": "posts/03/index.html#이항분포의-정규분포로의-근사",
    "title": "03. 연속확률변수, 정규분포",
    "section": "이항분포의 정규분포로의 근사",
    "text": "이항분포의 정규분포로의 근사\n\n확률변수 \\(X\\)가 이항분포 \\(B(400, 0.2)\\)를 따르고, 확률변수 \\(Y\\)가 정규분포 \\(B(80, 8^2)\\)을 따를 때, \\[\\left| P(X=k)-P(k-0.5 \\le Y \\le k+0.5) \\right|\\]의 최댓값을 구하시오.(단, \\(k\\)는 \\(0\\le k \\le 400\\)인 정수)\n\n\nk &lt;- 0\nprob &lt;- 0\nnewprob &lt;- 0\n\nwhile(k&lt;=400){\n  a &lt;- dbinom(k, 400, 0.2)\n  b &lt;- pnorm(k+0.5, 80, 8)-pnorm(k-0.5, 80, 8)\n  if(a&gt;=b){newprob &lt;- a-b}\n  else{newprob &lt;- b-a}\n  if(prob&lt;=newprob){prob &lt;- newprob; k &lt;- k+1}\n  else{ k &lt;- k+1 }\n}\nprob\n\n[1] 0.0008873549"
  },
  {
    "objectID": "posts/02/index.html#큰수의-법칙",
    "href": "posts/02/index.html#큰수의-법칙",
    "title": "02. 이산확률변수, 이항분포",
    "section": "큰수의 법칙",
    "text": "큰수의 법칙\n\n한 개의 주사위를 \\(n\\)회 던지는 시행에서 \\(1\\)의 눈이 나오는 횟수를 확률변수 \\(X\\)라고 하자. \\(n\\)의 값이 \\(10\\), \\(30\\), \\(50\\)일 때, 상대도수 \\(\\dfrac{X}{n}\\)와 한 개의 주사위를 \\(1\\)회 던질 때, \\(1\\)의 눈이 나올 수학적 확률 \\(\\dfrac{1}{6}\\)의 차가 \\(0.1\\)보다 작을 확률 \\(P \\left(\\left|\\dfrac{X}{n}-\\dfrac{1}{6} \\right| &lt;0.1 \\right)\\)을 이용하여 각각 구하시오.\n\n\nsum(dbinom(1:2, 10, 1/6))\n\n[1] 0.6137212\n\nsum(dbinom(3:7, 30, 1/6))\n\n[1] 0.7835228\n\nsum(dbinom(4:13, 50, 1/6))\n\n[1] 0.9454534\n\n\n\n한 개의 동전을 \\(n\\)번 던지는 시행에서 앞면이 나오는 횟수를 확률변수 \\(X\\)라 하자. \\(n\\)은 \\(5\\)의 배수이고, \\(P \\left(\\left|\\dfrac{X}{n}-\\dfrac{1}{2} \\right| \\le 0.1 \\right)\\ge0.95\\)를 만족시키는 자연수 \\(n\\)의 최솟값을 구하시오.\n\n\nn &lt;- 5\nwhile(sum(dbinom((0.4*n):(06*n),n,0.5))&lt;0.95){\n  n &lt;- n+5\n}\nn\n\n[1] 60"
  },
  {
    "objectID": "posts/13/index.html#모수적-일원배치-분산분석",
    "href": "posts/13/index.html#모수적-일원배치-분산분석",
    "title": "13. 분산분석",
    "section": "모수적 일원배치 분산분석",
    "text": "모수적 일원배치 분산분석\n\n일원배치 분산분석(One-way analysis of variance(ANOVA))이란?\n\n일반적으로 3개 이상의 모집단에서 모평균을 비교하는 문제에서 사용되며, 이때 사용되는 변수는 두 가지로 분류된다. 한 변수는 범주형 변수(3가지 이상의 범주값)이고, 평균을 비교하고자 하는 변수는 수치형 변수(평균 비교에 사용되는 변수)이다.\n평균을 비교하기 전에 세 집단의 분포가 정규분포인지, 집단 간 분산이 동일한지에 따라 그 결과가 다르다. 따라서 정규성 검정과 등분산성 검정을 먼저 실시하고 그 결과에 따라 평균을 비교하는 방법으로 진행한다.\n일원배치 분산분석은 평균에 차이가 있다, 없다만 판단 가능하며 어느 집단끼리 차이가 있는지에 대해서는 알 수 없다. 3개 이상의 평균 차이가 있다면 사후 분석이라는 다중비교를 할 수 있다.\n여러가지 다중비교 방법 중 Tukey HSD 검정이 있다.\n세 가지 이상 집단의 평균비교임에도 평균비교라고 하지 않고 분산분석이라고 하는 이유는 분산비를 이용하여 평균 차이를 검정하기 때문이다.\n분산분석 실험의 목적은 독립변수가 종속변수에 어떤 영향을 미치는지 알아보는 것이다.\n\n\n\n\n\nCars93데이터셋에서 에어백 유무에 따른 가격에 대한 일원배치 분산분석\n\n에어백 유무의 3가지 범주: 에어백 없음, 운전석 에어백, 운전석과 조수석 에어백\n\n\n\n# one-way ANalysis Of VAriance (ANOVA)\nlibrary(MASS)\nCars93$AirBags  # group variable = independent variable\n\n [1] None               Driver & Passenger Driver only        Driver & Passenger\n [5] Driver only        Driver only        Driver only        Driver only       \n [9] Driver only        Driver only        Driver & Passenger None              \n[13] Driver only        Driver & Passenger None               None              \n[17] None               Driver only        Driver only        Driver & Passenger\n[21] Driver & Passenger Driver only        None               Driver only       \n[25] Driver only        Driver only        Driver only        Driver only       \n[29] None               Driver & Passenger None               None              \n[33] None               Driver only        Driver only        Driver only       \n[37] Driver only        Driver only        None               Driver only       \n[41] Driver & Passenger Driver only        Driver & Passenger None              \n[45] None               None               None               Driver only       \n[49] Driver only        Driver & Passenger Driver & Passenger Driver & Passenger\n[53] None               None               Driver only        None              \n[57] Driver only        Driver only        Driver & Passenger Driver only       \n[61] None               None               Driver only        Driver only       \n[65] Driver only        None               Driver only        None              \n[69] Driver only        None               Driver only        None              \n[73] None               None               Driver & Passenger None              \n[77] Driver & Passenger Driver only        Driver only        None              \n[81] None               Driver only        None               Driver only       \n[85] Driver only        Driver only        Driver only        None              \n[89] None               None               None               Driver only       \n[93] Driver & Passenger\nLevels: Driver & Passenger Driver only None\n\nCars93$Price    # dependent variable\n\n [1] 15.9 33.9 29.1 37.7 30.0 15.7 20.8 23.7 26.3 34.7 40.1 13.4 11.4 15.1 15.9\n[16] 16.3 16.6 18.8 38.0 18.4 15.8 29.5  9.2 11.3 13.3 19.0 15.6 25.8 12.2 19.3\n[31]  7.4 10.1 11.3 15.9 14.0 19.9 20.2 20.9  8.4 12.5 19.8 12.1 17.5  8.0 10.0\n[46] 10.0 13.9 47.9 28.0 35.2 34.3 36.1  8.3 11.6 16.5 19.1 32.5 31.9 61.9 14.1\n[61] 14.9 10.3 26.1 11.8 15.7 19.1 21.5 13.5 16.3 19.5 20.7 14.4  9.0 11.1 17.7\n[76] 18.5 24.4 28.7 11.1  8.4 10.9 19.5  8.6  9.8 18.4 18.2 22.7  9.1 19.7 20.0\n[91] 23.3 22.7 26.7\n\n# dist~ of Airbags\ntable(Cars93$AirBags)\n\n\nDriver & Passenger        Driver only               None \n                16                 43                 34 \n\n# Price normality test for AirBags: 세 가지 모두 검정결과 정규분포 따르지 않으나 표본수가 많으므로 중심극한정리에 의해 정규성을 가정해도 됨.\nshapiro.test(Cars93$Price[Cars93$AirBags =='None'])\n\n\n    Shapiro-Wilk normality test\n\ndata:  Cars93$Price[Cars93$AirBags == \"None\"]\nW = 0.92329, p-value = 0.02004\n\nshapiro.test(Cars93$Price[Cars93$AirBags =='Driver only'])\n\n\n    Shapiro-Wilk normality test\n\ndata:  Cars93$Price[Cars93$AirBags == \"Driver only\"]\nW = 0.93184, p-value = 0.01345\n\nshapiro.test(Cars93$Price[Cars93$AirBags =='Driver & Passenger'])\n\n\n    Shapiro-Wilk normality test\n\ndata:  Cars93$Price[Cars93$AirBags == \"Driver & Passenger\"]\nW = 0.8631, p-value = 0.02136\n\n# Price equal variance test for AirBags : levene.test(), Bartlett()\nlibrary(lawstat)\n\nWarning: package 'lawstat' was built under R version 4.3.3\n\nlevene.test(Cars93$Price, Cars93$AirBags) #검정결과 등분산 아님\n\n\n    Modified robust Brown-Forsythe Levene-type test based on the absolute\n    deviations from the median\n\ndata:  Cars93$Price\nTest Statistic = 8.1213, p-value = 0.0005721\n\nbartlett.test(Cars93$Price ~ Cars93$AirBags) #검정결과 등분산 아님\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  Cars93$Price by Cars93$AirBags\nBartlett's K-squared = 24.899, df = 2, p-value = 3.92e-06\n\n# normality, equal variance(평소 많이 가정하는 것)\n## one-way ANOVA(집단간 평균비교)\naggregate(Price ~ AirBags, Cars93, base::mean) #에어백 여부에 따른 가격의 평균 계산\n\n             AirBags    Price\n1 Driver & Passenger 28.36875\n2        Driver only 21.22326\n3               None 13.17353\n\n## if equal variance : aov()   oneway.test()\nresult = aov(Price ~ AirBags, data=Cars93) #검정결과 각 집단간에 적어도 하나의 그룹은 평균이 다르다 할 수 있음\nsummary(result)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nAirBags      2   2747  1373.5   21.18 2.9e-08 ***\nResiduals   90   5837    64.9                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\noneway.test(Price ~ AirBags, data=Cars93, var.equal = T) #위와 같은 결과임\n\n\n    One-way analysis of means\n\ndata:  Price and AirBags\nF = 21.178, num df = 2, denom df = 90, p-value = 2.901e-08\n\n## if no equal vaiance : oneway.test()\noneway.test(Price ~ AirBags, data=Cars93, var.equal = F) #검정결과 각 집단간에 적어도 하나의 그룹은 평균이 다르다 할 수 있음\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  Price and AirBags\nF = 23.082, num df = 2.000, denom df = 34.479, p-value = 4.35e-07\n\n## TukeyHSD() : post Hoc if reject HO of one-way ANOVA [사후검정]\n## equal variance 가정 ==&gt; aov함수 사용하기\nresult = aov(Price ~ AirBags, data=Cars93)\nTukeyHSD(result) #검정결과 세 그룹 모두 가격 평균이 다르다 할 수 있음\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Price ~ AirBags, data = Cars93)\n\n$AirBags\n                                     diff       lwr       upr     p adj\nDriver only-Driver & Passenger  -7.145494 -12.76566 -1.525327 0.0088790\nNone-Driver & Passenger        -15.195221 -21.01361 -9.376828 0.0000000\nNone-Driver only                -8.049726 -12.45415 -3.645302 0.0001033\n\nplot(TukeyHSD(result, \"AirBags\"))"
  },
  {
    "objectID": "posts/13/index.html#비모수적-일원배치-분산분석",
    "href": "posts/13/index.html#비모수적-일원배치-분산분석",
    "title": "13. 분산분석",
    "section": "비모수적 일원배치 분산분석",
    "text": "비모수적 일원배치 분산분석\n\n비모수적 일원배치 분산분석이란?\n\n비모수적 검정(크루스칼-왈리스 검정, Kruskal-Wallis test)은 세 집단 이상의 평균을 비교할 때, 각 집단에서 정규성 검정을 만족하지 못하거나 소표본인 경우 실행하며 순위평균의 차이 여부를 가지고 모집단의 평균 차이 여부를 알아본다.\n\n\n\n\nCars93데이터셋에서 에어백 유무에 따른 가격에 대한 일원배치 분산분석\n\n에어백 유무의 3가지 범주: 에어백 없음, 운전석 에어백, 운전석과 조수석 에어백\n\n\n\n## Kruskal-wallis test : non-parametric test\nlibrary(MASS)\ntable(Cars93$AirBags)\n\n\nDriver & Passenger        Driver only               None \n                16                 43                 34 \n\naggregate(Price~AirBags, Cars93, base::mean)\n\n             AirBags    Price\n1 Driver & Passenger 28.36875\n2        Driver only 21.22326\n3               None 13.17353\n\n# normality test : alpha = 0.05 ==&gt; 0.01 ??(유의수준을 0.01로 변경하면 모든 그룹이 정규성을 따름)\ntapply(Cars93$Price, Cars93$AirBags, shapiro.test)\n\n$`Driver & Passenger`\n\n    Shapiro-Wilk normality test\n\ndata:  X[[i]]\nW = 0.8631, p-value = 0.02136\n\n\n$`Driver only`\n\n    Shapiro-Wilk normality test\n\ndata:  X[[i]]\nW = 0.93184, p-value = 0.01345\n\n\n$None\n\n    Shapiro-Wilk normality test\n\ndata:  X[[i]]\nW = 0.92329, p-value = 0.02004\n\n# if HO reject ==&gt; Kruskal-wallis test(정규성을 만족하지 않으면 비모수적 검정)\n## Kruskal Wallis test : HO: means for all groups equal\n\nkruskal.test(Price ~ AirBags, data=Cars93) # 검정결과 적어도 하나의 그룹은 다른 그룹과 차이가 있음\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Price by AirBags\nKruskal-Wallis chi-squared = 32.639, df = 2, p-value = 8.175e-08\n\n## mutiple comparison or Post Hoc test [사후검정]\n### parametric test : Tukey, Scheffe, Games-Howell, Tukey's Honest Significant Difference(HSD)\n### non-parametric test : 1) density plot for grop   2) dunn.test   3) mctp()\n\n### 1) density plot: moonBook package\n# install.packages(\"moonBook\") # moomBook패키지가 없으면 주석 지우고 설치하기\nlibrary(moonBook)\n\nWarning: package 'moonBook' was built under R version 4.3.3\n\ndensityplot(Price~AirBags, data=Cars93) #그래프를 보는 방법으로 주관적임\n\n\n ' Price ' is an invalid column name: Instead ' Min.Price ' is used\n\n### 2) Ddunn's multiple comparison test: dunn.test package\n# install.packages('dunn.test') # dunn.test패키지가 없으면 주석 지우고 설치하기\nlibrary(dunn.test)\ndunn.test(Cars93$Price, Cars93$AirBags, method='bonferroni')\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 32.6393, df = 2, p-value = 0\n\n                           Comparison of x by group                            \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |   Driver &   Driver o\n---------+----------------------\nDriver o |   1.717373\n         |     0.1289\n         |\n    None |   5.078861   4.517930\n         |    0.0000*    0.0000*\n\nalpha = 0.05\nReject Ho if p &lt;= alpha/2\n\n## 3) mctp(): nparcomp package, 정규분포이고 등분산 아닐때 사용\n# install.packages('nparcomp') # nparcomp패키지가 없으면 주석 지우고 설치하기\nlibrary(nparcomp)\n\nWarning: package 'nparcomp' was built under R version 4.3.3\n\n\nLoading required package: multcomp\n\n\nWarning: package 'multcomp' was built under R version 4.3.3\n\n\nLoading required package: mvtnorm\n\n\nWarning: package 'mvtnorm' was built under R version 4.3.3\n\n\nLoading required package: survival\n\n\nLoading required package: TH.data\n\n\nWarning: package 'TH.data' was built under R version 4.3.3\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\n\n\n\n\n\n\n\nresult = mctp(Price ~ AirBags, data=Cars93)  # tukey default\n\n\n #----------------Nonparametric Multiple Comparisons for relative effects---------------# \n \n - Alternative Hypothesis:  True differences of relative effects are not equal to 0 \n - Estimation Method:  Global Pseudo Ranks \n - Type of Contrast : Tukey \n - Confidence Level: 95 % \n - Method = Fisher with 28 DF \n \n #--------------------------------------------------------------------------------------# \n \n\nsummary(result)\n\n\n #----------------Nonparametric Multiple Comparisons for relative effects---------------# \n \n - Alternative Hypothesis:  True differences of relative effects are not equal to 0 \n - Estimation Method: Global Pseudo ranks \n - Type of Contrast : Tukey \n - Confidence Level: 95 % \n - Method = Fisher with 28 DF \n \n #--------------------------------------------------------------------------------------# \n \n #----Data Info-------------------------------------------------------------------------# \n              Sample Size    Effect     Lower     Upper\n1 Driver & Passenger   16 0.6950239 0.6310752 0.7522398\n2        Driver only   43 0.5464974 0.4880819 0.6036599\n3               None   34 0.2584787 0.2201847 0.3008632\n\n #----Contrast--------------------------------------------------------------------------# \n       1  2 3\n2 - 1 -1  1 0\n3 - 1 -1  0 1\n3 - 2  0 -1 1\n\n #----Analysis--------------------------------------------------------------------------# \n      Estimator  Lower  Upper Statistic      p.Value\n2 - 1    -0.149 -0.310  0.021    -2.153 9.326285e-02\n3 - 1    -0.437 -0.555 -0.301    -7.298 4.041749e-08\n3 - 2    -0.288 -0.402 -0.166    -5.632 1.217733e-05\n\n #----Overall---------------------------------------------------------------------------# \n  Quantile      p.Value\n1 2.454117 4.041749e-08\n\n #--------------------------------------------------------------------------------------#"
  },
  {
    "objectID": "posts/13/index.html#간단하게-살펴보는-일원배치분산분석-예제",
    "href": "posts/13/index.html#간단하게-살펴보는-일원배치분산분석-예제",
    "title": "13. 분산분석",
    "section": "간단하게 살펴보는 일원배치분산분석 예제",
    "text": "간단하게 살펴보는 일원배치분산분석 예제\n\n자동차 충돌실험 결과 여러 등급의 자동차에서 인형의 파손 정도\n\n자동차 충돌실험의 결과 인형의 파손 정도가 다음과 같다. 이 데이터를 사용하여 인형의 파손 정도는 자동차의 크기에 관계없이 동일하다는 귀무가설을 검정하시오.\n\n\n\n\n\n\n소형자동차\n준중형자동차\n중형자동차\n대형자동차\n\n\n\n\n\n681\n643\n469\n384\n\n\n\n728\n655\n727\n656\n\n\n\n917\n742\n525\n602\n\n\n\n898\n514\n454\n687\n\n\n\n620\n525\n459\n360\n\n\n평균\n768.8\n615.8\n526.8\n537.8\n\n\n표준편차\n132.4\n95.9\n115.5\n154.6\n\n\n\n\n\ny &lt;- c(681, 728, 917, 898, 620,\n       643, 655, 742, 514, 525,\n       469, 727, 525, 454, 459,\n       384, 656, 602, 687, 360) \n \nx &lt;- rep(c(\"소형\",\"준중형\",\"중형\",\"대형\"), c(5,5,5,5))\nx &lt;-factor(x)\n\naov.result=aov(y ~ x)\nsummary(aov.result)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nx            3 186825   62275   3.893  0.029 *\nResiduals   16 255923   15995                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# F통계량값인 3.8934가 기각역에 있으므로 귀무가설을 기각한다.\n# 또는 p값 0.029가 유의수준 0.05보다 작으므로 귀무가설을 기각한다.\n# 자동차의 크기에 따른 등급은 인형모형의 파손 정도에 통계적으로 유의한 차이를 보인다."
  },
  {
    "objectID": "posts/13/index.html#이원배치분산분석",
    "href": "posts/13/index.html#이원배치분산분석",
    "title": "13. 분산분석",
    "section": "이원배치분산분석",
    "text": "이원배치분산분석\n\n이원배치분산분석(Two-way analysis of variance(ANOVA))이란\n\n\n\n\n\nCars93데이터셋에서 에어백 유무와 미국회사 유무에 따른 가격에 대한 이원배치 분산분석\n\n균형자료라고 가정하고 검정하기\n\n\n# two-way ANOVA \n\nlibrary(MASS)  # Cars93\n## factor1 : AirBags\n## factor2 : Origin \n## dependent variable : Price\n\nreplications(Price ~ AirBags * Origin, Cars93) #균형설계자료인지 확인(각 범주별로 자료수가 동일한지)\n\n$AirBags\nAirBags\nDriver & Passenger        Driver only               None \n                16                 38                 28 \n\n$Origin\nOrigin\n    USA non-USA \n     42      40 \n\n$`AirBags:Origin`\n                    Origin\nAirBags              USA non-USA\n  Driver & Passenger   9       7\n  Driver only         20      18\n  None                13      15\n\naggregate(Price~AirBags, Cars93, base::mean) #평균 비교해보기\n\n             AirBags    Price\n1 Driver & Passenger 28.36875\n2        Driver only 21.22326\n3               None 13.17353\n\naggregate(Price~Origin, Cars93, base::mean) #평균 비교해보기\n\n   Origin    Price\n1     USA 18.57292\n2 non-USA 20.50889\n\n## two-way  anova :  parametric method , balanced data(지금 자료가 불균형이지만 균형자료라고 가정하고 검정)\n### 1) for factor1 HO: All means are equal\n### 2) for facotr2 HO: all means are equal\n### 3) for interaction factor1*factor2 : HO: all means are equal\n## method1(모수적 방법1)\nresult = aov(Price ~ AirBags * Origin, data= Cars93) #정규성,등분산성을 만족한다고 가정하고 분산분석 실시\nsummary(result)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nAirBags         2   2747  1373.5  21.925 1.95e-08 ***\nOrigin          1    170   170.3   2.719    0.103    \nAirBags:Origin  2    217   108.4   1.730    0.183    \nResiduals      87   5450    62.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## method2 (모수적 방법2)\nanova(lm(Price ~ AirBags * Origin, data=Cars93))\n\nAnalysis of Variance Table\n\nResponse: Price\n               Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nAirBags         2 2747.0 1373.49 21.9254 1.947e-08 ***\nOrigin          1  170.3  170.31  2.7186    0.1028    \nAirBags:Origin  2  216.7  108.35  1.7296    0.1834    \nResiduals      87 5450.0   62.64                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n위 검정 결과, 에어벡 여부에 따른 각 집단간의 평균은 적어도 하나는 다르고, 미국회사 유무에 따른 각 집단의 평균은 모두 같다. 또한 에어백 여부와 미국회사 유무의 상호작용에 따른 각 집단의 평균은 모두 같다.\n위 자료가 불균형자료인데, 균형자료라고 가정하고 검정했다. 균형자료일 경우에는 아래 그림에서 aov, anova 함수의 기본옵션인 type2로 계산한다. 불균형자료는 type3로 계산해야 한다.\n\n\n\n## two-way ANOVA for  unbalanced data  : Type3 SS\n### Anova()  ==&gt; car package \n# install.packages('car')  # car 라이브러리가 없으면 주석 지우고 설치하기\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.3.3\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.3.3\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:lawstat':\n\n    levene.test\n\nresult = aov(Price ~ AirBags * Origin, data= Cars93)\nAnova(result, type=3)  # 검정결과 미국회사유무에 따라서도 가격 차이가 있음\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\n\nAnova Table (Type III tests)\n\nResponse: Price\n               Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)    5436.6  1 86.7857 1.013e-14 ***\nAirBags         802.8  2  6.4076  0.002535 ** \nOrigin          295.6  1  4.7194  0.032544 *  \nAirBags:Origin  216.7  2  1.7296  0.183391    \nResiduals      5450.0 87                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## plot for interaction effect(상호작용 효과가 있을때 그려보기)\ninteraction.plot(Cars93$AirBags, Cars93$Origin, Cars93$Price)\n\n\n\n\n\n\n\n\n\n\n비모수적 방법(프리드만 검정, freidman test)\n\n주조시간에 따라 강도에 차이가 있는지 검정하자.(소표본)\n\n\n\n\n## non-parametric : two-way ANOVA  ==&gt; freidman test\n## factor1: independent variable\n## factor2: block(실험에서 차이가 있어도 되고 없어도 될때 block이라 함)\n## dependent variable \n\ndata = read.csv(\"strength.csv\", header = T)  # 첫번째행에 변수명 있음\nstr(data)  # 자료 구조 살피기\n\n'data.frame':   9 obs. of  3 variables:\n $ A      : chr  \"A1\" \"A1\" \"A1\" \"A2\" ...\n $ B      : chr  \"B1\" \"B2\" \"B3\" \"B1\" ...\n $ strenth: int  90 95 100 100 94 100 104 110 113\n\nfriedman.test(data$strenth, data$A, data$B)\n\n\n    Friedman rank sum test\n\ndata:  data$strenth, data$A and data$B\nFriedman chi-squared = 4.9091, df = 2, p-value = 0.0859"
  },
  {
    "objectID": "posts/13/index.html#반복측정-분산분석",
    "href": "posts/13/index.html#반복측정-분산분석",
    "title": "13. 분산분석",
    "section": "반복측정 분산분석",
    "text": "반복측정 분산분석\n\n반복측정 분산분석이란\n\n\n\n\n\n실습예제\n\n주변 이산화탄소의 여러 농도에 따른 나무의 이산화탄소 흡수율 비교\n\n\n## repeated measure ANOVA\n\n## 1. one - repeated factor, dependent variable(사전에 정규성, 등분산성 검증은 사전에 실시해야 함)\n## 'CO2' \n\nstr(CO2)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  84 obs. of  5 variables:\n $ Plant    : Ord.factor w/ 12 levels \"Qn1\"&lt;\"Qn2\"&lt;\"Qn3\"&lt;..: 1 1 1 1 1 1 1 2 2 2 ...\n $ Type     : Factor w/ 2 levels \"Quebec\",\"Mississippi\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Treatment: Factor w/ 2 levels \"nonchilled\",\"chilled\": 1 1 1 1 1 1 1 1 1 1 ...\n $ conc     : num  95 175 250 350 500 675 1000 95 175 250 ...\n $ uptake   : num  16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ...\n - attr(*, \"formula\")=Class 'formula'  language uptake ~ conc | Plant\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"outer\")=Class 'formula'  language ~Treatment * Type\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Ambient carbon dioxide concentration\"\n  ..$ y: chr \"CO2 uptake rate\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(uL/L)\"\n  ..$ y: chr \"(umol/m^2 s)\"\n\n# dependent variable : uptake(이산화탄소 흡수율)\n# repeated factor : conc(이산화탄소 농도)\n\nhead(CO2)\n\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n4   Qn1 Quebec nonchilled  350   37.2\n5   Qn1 Quebec nonchilled  500   35.3\n6   Qn1 Quebec nonchilled  675   39.2\n\nboxplot(formula = uptake~conc, data=CO2) # 이산화탄소 농도에 따른 이산화탄소 흡수율의 상자그림\n\n\n\n\n\n\n\nmeans = aggregate(uptake~conc, CO2, base::mean)\nmeans\n\n  conc   uptake\n1   95 12.25833\n2  175 22.28333\n3  250 28.87500\n4  350 30.66667\n5  500 30.87500\n6  675 31.95000\n7 1000 33.58333\n\n## repeated ANOVA \n## H0: means of uptake for conc are same\n\nresult = aov(uptake ~ conc + Error(Plant/conc), data=CO2) # conc factor는 반복측정되므로 오차항에 Error(Plant/conc)로 넣어줘야 함\nsummary(result) # H0기각, 주변 이산화탄소 농도에 따른 이산화탄소 흡수율에는 차이가 있다고 할 수 있음\n\n\nError: Plant\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 11   4862     442               \n\nError: Plant:conc\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nconc       1 2285.0  2285.0   70.39 4.14e-06 ***\nResiduals 11  357.1    32.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 60   2203   36.71               \n\n## cf one-way ANOVA\nresult1 = aov(uptake ~ conc, data=CO2)\nsummary(result1) # 반복이 없는 분산분석의 MSE는 90.5, 반복측정분산분석의 MSE는 32.5로 반복측정 시, F값의 분모인 MSE가 더 작으므로 검정력이 높음\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nconc         1   2285  2285.0   25.25 2.91e-06 ***\nResiduals   82   7422    90.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Post-Hoc test\n\nwith(CO2, pairwise.t.test(uptake, conc, paired=T, p.adjust.method='bonferroni'))\n\n\n    Pairwise comparisons using paired t tests \n\ndata:  uptake and conc \n\n     95      175     250     350     500     675    \n175  0.00025 -       -       -       -       -      \n250  0.00012 0.00736 -       -       -       -      \n350  0.00013 0.00098 0.53879 -       -       -      \n500  0.00012 0.00268 0.01458 1.00000 -       -      \n675  4.5e-05 0.00052 0.00210 0.83520 0.76607 -      \n1000 7.5e-05 0.00059 0.00253 0.01468 0.01741 0.10482\n\nP value adjustment method: bonferroni \n\n\n- 저온처리 유무가 추가되었을때, 주변 이산화탄소의 여러 농도에 따른 나무의 이산화탄소 흡수율 비교\n\n## one factor, one repeated factor ==&gt; two-way repeated ANOVA\n## Treatment(저온처리 유무), conc(주변 이산화탄소 농도) \n\nhead(CO2)\n\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n4   Qn1 Quebec nonchilled  350   37.2\n5   Qn1 Quebec nonchilled  500   35.3\n6   Qn1 Quebec nonchilled  675   39.2\n\naggregate(uptake~conc, CO2, base::mean)\n\n  conc   uptake\n1   95 12.25833\n2  175 22.28333\n3  250 28.87500\n4  350 30.66667\n5  500 30.87500\n6  675 31.95000\n7 1000 33.58333\n\naggregate(uptake~Treatment, CO2, base::mean)\n\n   Treatment   uptake\n1 nonchilled 30.64286\n2    chilled 23.78333\n\nresult3 = aov(uptake~Treatment*conc + Error(Plant/conc), data=CO2)\nsummary(result3)\n\n\nError: Plant\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nTreatment  1    988   988.1   2.551  0.141\nResiduals 10   3874   387.4               \n\nError: Plant:conc\n               Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nconc            1 2285.0  2285.0   70.27 7.8e-06 ***\nTreatment:conc  1   31.9    31.9    0.98   0.346    \nResiduals      10  325.2    32.5                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 60   2203   36.71               \n\n## cf  two-way ANOVA\nresult4 = aov(uptake~Treatment*conc, data=CO2)\nsummary(result4)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment       1    988   988.1  12.348  0.00073 ***\nconc            1   2285  2285.0  28.554 8.38e-07 ***\nTreatment:conc  1     32    31.9   0.398  0.52979    \nResiduals      80   6402    80.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/10/index.html#두-모집단에-대한-비율-차이-검정-예제",
    "href": "posts/10/index.html#두-모집단에-대한-비율-차이-검정-예제",
    "title": "10. 하나의 모집단에 대한 가설검정(평균)",
    "section": "두 모집단에 대한 비율 차이 검정 예제",
    "text": "두 모집단에 대한 비율 차이 검정 예제\n여러 독립 모집단의 모비율 차이에 대한 검정도 두 독립모집단의 모비율 차이와 동일하게 검토할 수 있다. 독감예방주사여부에 따른 감기증세 비율 예제를 통해 이항분포를 활용하여 검정하는 방법과 정규근사를 활용하여 검정하는 두 가지 방법을 배워보자. 그리고 Cars93 데이터셋에서 prop.test()으로 검정하는 것도 배워보자.\n\n독감예방백신주사를 맞은 집단과 맞지 않은 집단에서의 감기증세를 보이는 비율 차이 검정\n\n\n# significant test for two-sample proportion \n\n# 1. binomial distribution\n## binom.test(x, n, p0, alternative='two.sided'): 하나의 모비율에 대한 검정\n## binom.test(x-vector, n-vector, ~): 두 모비율의 차이에 대한 검정\n\n## H0: p1 = p2   p1: shot   p2: not-shot\n## H1: p1 is not p2\n# shot    : n1 = 201    number of catch cold x1=11  \n# not-shot: n2 = 200    number of catch cold x2=33\n# alpha = 0.05\n\nn1=201  #독감예방접종을 한 사람\nn2=200  #독감예방접종을 안한 사람\nx1=11  #n1에서 감기증세 있는 사람 수\nx2=33  #n2에서 감기증세 있는 사람 수\n\nn = c(n1, n2)\nx = c(x1, x2)\nbinom.test(x, n, alternative = 'two.sided') #검정결과 두 집단에서 감기증세를 보이는 사람의 비율은 같지 않음\n\n\n    Exact binomial test\n\ndata:  x\nnumber of successes = 11, number of trials = 44, p-value = 0.00126\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1319274 0.4033804\nsample estimates:\nprobability of success \n                  0.25 \n\n## H1: p1 &lt; p2\nbinom.test(x, n, alternative = 'less') #검정결과 독감주사를 맞은 집단이 감기 걸릴 확률이 더 낮음\n\n\n    Exact binomial test\n\ndata:  x\nnumber of successes = 11, number of trials = 44, p-value = 0.00063\nalternative hypothesis: true probability of success is less than 0.5\n95 percent confidence interval:\n 0.0000000 0.3797055\nsample estimates:\nprobability of success \n                  0.25 \n\n## normal distribution(n이 크므로 정규분포의 근사를 활용하여 검정)\nprop.test(x, n, alternative = 'two.sided', correct = T) #검정결과 귀무가설 기각으로 두 모비율이 차이가 있음.(함수 내부적으로 카이제곱검정 사용)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  x out of n\nX-squared = 11.376, df = 1, p-value = 0.000744\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.17555184 -0.04499543\nsample estimates:\n    prop 1     prop 2 \n0.05472637 0.16500000 \n\nprop.test(x, n, alternative = 'less', correct = T) #단측검정을 하면 p값이 더 작아지고, 더 잘 기각하게 됨. 즉 독감예방주사를 맞은 집단이 모비율이 더 작음\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  x out of n\nX-squared = 11.376, df = 1, p-value = 0.000372\nalternative hypothesis: less\n95 percent confidence interval:\n -1.00000000 -0.05468857\nsample estimates:\n    prop 1     prop 2 \n0.05472637 0.16500000 \n\n\n\nCars93 데이터셋에서 미국회사 여부(Origin)에 따른 두 집단의 수동변속기 사용 가능 비율에 대한 모비율 검정\n\n\nlibrary(MASS)\n## Origin: USA, non-USA\n## Man.trans.avail: 수동변속기 사용 여부\n\ntable(Cars93[ , c('Man.trans.avail', 'Origin')])\n\n               Origin\nMan.trans.avail USA non-USA\n            No   26       6\n            Yes  22      39\n\n## H0: p1 of USA = p2 of non-USA  Yes prop of Man.trans.avail \n## H1: p1 is not p2\n\nn = c(48, 45) #미국회사 유무에 따른 차량수\nx = c(22, 39) #수동변속기 사용 가능한 차량수\nprop.test(x, n, alternative = 'two.sided') #모비율을 검정하는 내장함수, 검정결과 귀무가설 기각, 즉 비율이 다름\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  x out of n\nX-squared = 15.397, df = 1, p-value = 8.712e-05\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.6022941 -0.2143725\nsample estimates:\n   prop 1    prop 2 \n0.4583333 0.8666667 \n\nprop.test(x, n, alternative = 'less') #정보를 더 알면 단측검정이 유리함\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  x out of n\nX-squared = 15.397, df = 1, p-value = 4.356e-05\nalternative hypothesis: less\n95 percent confidence interval:\n -1.0000000 -0.2420952\nsample estimates:\n   prop 1    prop 2 \n0.4583333 0.8666667 \n\nprop.test(x, n, alternative = 'greater') #이 경우는 할 필요가 없다는 것을 알 수 있음\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  x out of n\nX-squared = 15.397, df = 1, p-value = 1\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.5745715  1.0000000\nsample estimates:\n   prop 1    prop 2 \n0.4583333 0.8666667"
  },
  {
    "objectID": "posts/16/index.html#학생들의-대학-합격-여부-예측",
    "href": "posts/16/index.html#학생들의-대학-합격-여부-예측",
    "title": "16. 상관분석",
    "section": "학생들의 대학 합격 여부 예측",
    "text": "학생들의 대학 합격 여부 예측\n\n학생들의 대학 합격 여부를 예측하는 로지스틱 회귀분석을 가정해보자. 여기서 종속 변수는 합격 여부(합격=1, 불합격=0)이고, 독립 변수로는 시험 점수(GRE), 학점(GPA), 그리고 추천서의 수(Recommendations)를 사용한다.\n(아래 코드에서 생성한 가상데이터가 통계적으로 유의미한 결과를 가져오지 않을 수 있음.)\n\n\n# 데이터 준비\ndata &lt;- data.frame(\n  admit = c(0, 1, 1, 0, 1, 1), # 합격 여부\n  gre = c(600, 700, 700, 500, 660, 680), # GRE 점수\n  gpa = c(3.3, 3.7, 3.9, 2.5, 3.3, 3.8), # GPA\n  recommendations = c(1, 2, 2, 1, 1, 2) # 추천서의 수\n)\n\n# 로지스틱 회귀 모델 적합\nmodel &lt;- glm(admit ~ gre + gpa + recommendations, family = binomial(link = \"logit\"), data = data)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# 결과 해석\nsummary(model)\n\n\nCall:\nglm(formula = admit ~ gre + gpa + recommendations, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     -2.086e+02  5.506e+05       0        1\ngre              7.937e-01  2.117e+03       0        1\ngpa             -9.818e+01  3.886e+05       0        1\nrecommendations  3.257e+01  1.612e+05       0        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7.6382e+00  on 5  degrees of freedom\nResidual deviance: 3.9359e-10  on 2  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 23"
  },
  {
    "objectID": "posts/16/index.html#은행에서-대출-승인-여부-예측",
    "href": "posts/16/index.html#은행에서-대출-승인-여부-예측",
    "title": "16. 상관분석",
    "section": "은행에서 대출 승인 여부 예측",
    "text": "은행에서 대출 승인 여부 예측\n\n은행에서 고객의 신용 데이터를 기반으로 개인 대출 승인 여부를 예측하는 경우를 들 수 있다. 이 예제에서는 고객의 신용 점수(credit score), 연 소득(annual income), 대출 요청 금액(loan amount)을 독립 변수로 사용하여 대출 승인 여부(loan approval)를 종속 변수로 예측하는 모델을 구축한다.\n(아래 코드에서 생성한 가상데이터가 통계적으로 유의미한 결과를 가져오지 않을 수 있음.)\n\n\n# 가상 데이터 생성\nset.seed(123) # 결과 재현성을 위한 시드 설정\nloan_approval &lt;- sample(0:1, 100, replace = TRUE)\ncredit_score &lt;- rnorm(100, mean = 650, sd = 100)\nannual_income &lt;- rnorm(100, mean = 50000, sd = 20000)\nloan_amount &lt;- rnorm(100, mean = 15000, sd = 5000)\ndata &lt;- data.frame(loan_approval, credit_score, annual_income, loan_amount)\n\n# 로지스틱 회귀 모델 적합\nmodel &lt;- glm(loan_approval ~ credit_score + annual_income + loan_amount, \n             family = binomial(link = \"logit\"), data = data)\n\n# 모델 요약 결과 출력\nsummary(model)\n\n\nCall:\nglm(formula = loan_approval ~ credit_score + annual_income + \n    loan_amount, family = binomial(link = \"logit\"), data = data)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    1.879e-02  1.657e+00   0.011    0.991\ncredit_score  -1.656e-04  2.119e-03  -0.078    0.938\nannual_income -1.180e-05  1.129e-05  -1.046    0.296\nloan_amount    2.527e-05  4.156e-05   0.608    0.543\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 136.66  on 99  degrees of freedom\nResidual deviance: 135.24  on 96  degrees of freedom\nAIC: 143.24\n\nNumber of Fisher Scoring iterations: 4\n\n# 새로운 데이터에 대한 예측\nnew_data &lt;- data.frame(credit_score = c(700), annual_income = c(60000), loan_amount = c(10000))\npredict(model, newdata = new_data, type = \"response\")\n\n        1 \n0.3652591"
  },
  {
    "objectID": "posts/17/index.html",
    "href": "posts/17/index.html",
    "title": "17. 정원 가꾸기",
    "section": "",
    "text": "학교 담장에 클레마티스 심기\n클레마티스 심기(Clematis) 2020. 5. 18.\n\n\n\n\n학교 정원에 장미정원 만들기\n장미정원 만들기 1(Rose Garden) 2020. 10. 19.\n\n\n장미정원 만들기 2(Rose Garden) 2020. 10. 20.\n\n\n학교 장미정원(Rose Garden) 2024. 1. 13.\n\n\n덩굴장미의 위력(Rose Garden) 2024. 1. 14.\n\n\n\n\n학교 정원을 아름답게 꾸미기\n연못 꾸미기 2020. 4. 20.\n\n\n튤립 심기 2020. 4. 20.\n\n\n백합 심기 2020. 6. 28."
  }
]